"""
ç‰¹å¾µåˆ†æžèˆ‡é¸æ“‡è…³æœ¬
ç ”ç©¶ç‰¹å¾µæ•¸é‡å°æ¨¡åž‹æº–ç¢ºåº¦çš„å½±éŸ¿
"""
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, r2_score
import json
import os
import sys

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from feature_engineering import create_comprehensive_features, get_feature_columns

def load_data():
    """åŠ è¼‰æ•¸æ“š"""
    csv_path = '../NDH_AED_Clean.csv'
    if os.path.exists(csv_path):
        df = pd.read_csv(csv_path)
        if 'date' in df.columns:
            df['Date'] = df['date']
        if 'patient_count' in df.columns:
            df['Attendance'] = df['patient_count']
        return df[['Date', 'Attendance']]
    return None

def analyze_feature_importance(n_top_features_list=[10, 20, 30, 50, 80, 100, 130, 161]):
    """
    åˆ†æžä¸åŒç‰¹å¾µæ•¸é‡å°æº–ç¢ºåº¦çš„å½±éŸ¿
    
    é€™å€‹å¯¦é©—æ¸¬è©¦ã€Œç¶­åº¦ç½é›£ã€å‡è¨­ï¼š
    - å¤ªå°‘ç‰¹å¾µï¼šæ¨¡åž‹æ¬ æ“¬åˆ
    - å¤ªå¤šç‰¹å¾µï¼šæ¨¡åž‹éŽæ“¬åˆæˆ–æœ‰å™ªéŸ³ç‰¹å¾µ
    - æœ€ä½³é»žï¼šæº–ç¢ºåº¦å’Œæ³›åŒ–èƒ½åŠ›çš„å¹³è¡¡
    """
    print("=" * 70)
    print("ðŸ”¬ ç‰¹å¾µæ•¸é‡ vs æº–ç¢ºåº¦ ç ”ç©¶")
    print("=" * 70)
    print("\nðŸ“š ç†è«–èƒŒæ™¯ï¼š")
    print("   1. ç¶­åº¦ç½é›£ (Curse of Dimensionality)ï¼š")
    print("      - ç‰¹å¾µå¢žåŠ æ™‚ï¼Œæ•¸æ“šè®Šå¾—ç¨€ç–")
    print("      - éœ€è¦æŒ‡æ•¸ç´šæ•¸æ“šé‡ä¾†ç¶­æŒçµ±è¨ˆé¡¯è‘—æ€§")
    print("   2. éŽæ“¬åˆé¢¨éšªï¼š")
    print("      - éŽå¤šç‰¹å¾µ â†’ æ¨¡åž‹è¨˜ä½è¨“ç·´æ•¸æ“š")
    print("      - è¡¨ç¾ï¼šè¨“ç·´é›†å¥½ï¼Œæ¸¬è©¦é›†å·®")
    print("   3. å™ªéŸ³ç‰¹å¾µï¼š")
    print("      - ç„¡é—œç‰¹å¾µæœƒå¼•å…¥å™ªéŸ³")
    print("      - é™ä½Žæ¨¡åž‹æ³›åŒ–èƒ½åŠ›")
    print()
    
    # åŠ è¼‰æ•¸æ“š
    df = load_data()
    if df is None:
        print("âŒ ç„¡æ³•åŠ è¼‰æ•¸æ“š")
        return
    
    print(f"ðŸ“Š æ•¸æ“šé‡: {len(df)} ç­†")
    print(f"ðŸ“… æ—¥æœŸç¯„åœ: {df['Date'].min()} â†’ {df['Date'].max()}")
    
    # å‰µå»ºç‰¹å¾µ
    print("\nâ³ å‰µå»ºç‰¹å¾µä¸­...")
    df = create_comprehensive_features(df)
    df = df.dropna(subset=['Attendance'])
    
    # ç²å–æ‰€æœ‰ç‰¹å¾µåˆ—
    all_feature_cols = get_feature_columns()
    all_feature_cols = [col for col in all_feature_cols if col in df.columns]
    print(f"âœ… å¯ç”¨ç‰¹å¾µæ•¸: {len(all_feature_cols)}")
    
    # æ™‚é–“åºåˆ—åˆ†å‰²
    split_idx = int(len(df) * 0.8)
    train_data = df[:split_idx].copy()
    test_data = df[split_idx:].copy()
    
    print(f"\nðŸ“Š æ•¸æ“šåˆ†å‰²:")
    print(f"   è¨“ç·´é›†: {len(train_data)} ç­†")
    print(f"   æ¸¬è©¦é›†: {len(test_data)} ç­†")
    
    # é¦–å…ˆè¨“ç·´å®Œæ•´æ¨¡åž‹ç²å–ç‰¹å¾µé‡è¦æ€§
    print("\nðŸ”„ è¨“ç·´å®Œæ•´æ¨¡åž‹ä»¥ç²å–ç‰¹å¾µé‡è¦æ€§...")
    X_train_full = train_data[all_feature_cols]
    y_train = train_data['Attendance']
    X_test_full = test_data[all_feature_cols]
    y_test = test_data['Attendance']
    
    full_model = xgb.XGBRegressor(
        n_estimators=300,
        max_depth=6,
        learning_rate=0.05,
        random_state=42,
        n_jobs=-1
    )
    full_model.fit(X_train_full, y_train, verbose=False)
    
    # ç²å–ç‰¹å¾µé‡è¦æ€§æŽ’å
    importance = full_model.feature_importances_
    feature_importance = list(zip(all_feature_cols, importance))
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    
    print("\nðŸ“Š Top 20 æœ€é‡è¦ç‰¹å¾µ:")
    for i, (feat, imp) in enumerate(feature_importance[:20]):
        bar = "â–ˆ" * int(imp / max(importance) * 30)
        print(f"   {i+1:2}. {feat:30} {bar} {imp:.4f}")
    
    # æ¸¬è©¦ä¸åŒç‰¹å¾µæ•¸é‡
    print("\n" + "=" * 70)
    print("ðŸ§ª å¯¦é©—ï¼šä¸åŒç‰¹å¾µæ•¸é‡çš„æº–ç¢ºåº¦")
    print("=" * 70)
    print(f"\n{'ç‰¹å¾µæ•¸':>8} | {'MAE':>8} | {'MAPE':>8} | {'RÂ²':>8} | {'èª¿æ•´RÂ²':>8} | è©•ä¼°")
    print("-" * 70)
    
    results = []
    
    for n_features in n_top_features_list:
        if n_features > len(all_feature_cols):
            n_features = len(all_feature_cols)
        
        # é¸æ“‡ top N ç‰¹å¾µ
        top_features = [f[0] for f in feature_importance[:n_features]]
        
        X_train = train_data[top_features]
        X_test = test_data[top_features]
        
        # è¨“ç·´æ¨¡åž‹
        model = xgb.XGBRegressor(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.05,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train, y_train, verbose=False)
        
        # è©•ä¼°
        y_pred = model.predict(X_test)
        mae = mean_absolute_error(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        r2 = r2_score(y_test, y_pred)
        
        # èª¿æ•´ RÂ²
        n = len(y_test)
        p = n_features
        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) if n > p + 1 else r2
        
        # è©•ä¼°
        if mae < 4:
            rating = "ðŸ† å„ªç§€"
        elif mae < 5:
            rating = "âœ… è‰¯å¥½"
        elif mae < 6:
            rating = "âš ï¸ ä¸€èˆ¬"
        else:
            rating = "âŒ éœ€æ”¹é€²"
        
        results.append({
            'n_features': n_features,
            'mae': mae,
            'mape': mape,
            'r2': r2,
            'adj_r2': adj_r2,
            'features': top_features
        })
        
        print(f"{n_features:>8} | {mae:>8.2f} | {mape:>7.2f}% | {r2*100:>7.1f}% | {adj_r2*100:>7.1f}% | {rating}")
    
    # æ‰¾å‡ºæœ€ä½³ç‰¹å¾µæ•¸é‡
    print("\n" + "=" * 70)
    print("ðŸ“Š åˆ†æžçµè«–")
    print("=" * 70)
    
    # ä»¥ MAE ç‚ºä¸»è¦æŒ‡æ¨™
    best_by_mae = min(results, key=lambda x: x['mae'])
    # ä»¥èª¿æ•´ RÂ² ç‚ºæŒ‡æ¨™
    best_by_adj_r2 = max(results, key=lambda x: x['adj_r2'])
    
    print(f"\nðŸ† æœ€ä½³ MAE: {best_by_mae['mae']:.2f} (ä½¿ç”¨ {best_by_mae['n_features']} å€‹ç‰¹å¾µ)")
    print(f"ðŸ† æœ€ä½³èª¿æ•´ RÂ²: {best_by_adj_r2['adj_r2']*100:.1f}% (ä½¿ç”¨ {best_by_adj_r2['n_features']} å€‹ç‰¹å¾µ)")
    
    # å»ºè­°
    print("\nðŸ’¡ å»ºè­°ï¼š")
    
    # æ¯”è¼ƒ 161 ç‰¹å¾µå’Œæœ€ä½³é»ž
    full_result = [r for r in results if r['n_features'] >= 130][-1] if any(r['n_features'] >= 130 for r in results) else results[-1]
    
    if best_by_mae['n_features'] < full_result['n_features'] * 0.7:
        print(f"   âš ï¸ ä½¿ç”¨è¼ƒå°‘ç‰¹å¾µ ({best_by_mae['n_features']}) åè€Œæº–ç¢ºåº¦æ›´é«˜")
        print(f"   âš ï¸ é€™è¡¨æ˜Žæœ‰éŽå¤šå™ªéŸ³ç‰¹å¾µæˆ–éŽæ“¬åˆ")
        print(f"   âœ… å»ºè­°ä½¿ç”¨ {best_by_mae['n_features']} å€‹ç‰¹å¾µ")
    else:
        print(f"   âœ… ç•¶å‰ç‰¹å¾µæ•¸é‡åˆç†")
    
    # ä¿å­˜æœ€ä½³ç‰¹å¾µåˆ—è¡¨
    print(f"\nðŸ“ ä¿å­˜æœ€ä½³ç‰¹å¾µé…ç½®...")
    best_config = {
        'optimal_n_features': best_by_mae['n_features'],
        'optimal_features': best_by_mae['features'],
        'metrics': {
            'mae': best_by_mae['mae'],
            'mape': best_by_mae['mape'],
            'r2': best_by_mae['r2'],
            'adj_r2': best_by_mae['adj_r2']
        },
        'comparison': [
            {'n_features': r['n_features'], 'mae': r['mae'], 'r2': r['r2']} 
            for r in results
        ]
    }
    
    with open('models/optimal_features.json', 'w') as f:
        json.dump(best_config, f, indent=2)
    
    print(f"âœ… å·²ä¿å­˜åˆ° models/optimal_features.json")
    
    return results

if __name__ == '__main__':
    analyze_feature_importance()
