"""
Stacking Ensemble æ¨¡å‹
ä½¿ç”¨å¤šå€‹åŸºæ¨¡å‹çš„é æ¸¬ä½œç‚ºå…ƒç‰¹å¾µï¼Œè¨“ç·´å…ƒå­¸ç¿’å™¨

é æœŸæ”¹å–„: MAE 15.77 â†’ 14.5 (ç´„ 8% æ”¹å–„)
"""
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import json
import os
from datetime import datetime

try:
    from lightgbm import LGBMRegressor
    LIGHTGBM_AVAILABLE = True
except:
    LIGHTGBM_AVAILABLE = False

try:
    import catboost
    from catboost import CatBoostRegressor
    CATBOOST_AVAILABLE = True
except:
    CATBOOST_AVAILABLE = False


class StackingEnsemble:
    """
    Stacking Ensemble é æ¸¬å™¨

    ç¬¬ä¸€å±¤ (Base Models):
    - XGBoost
    - LightGBM (å¦‚æœå¯ç”¨)
    - Random Forest
    - Gradient Boosting
    - CatBoost (å¦‚æœå¯ç”¨)

    ç¬¬äºŒå±¤ (Meta Learner):
    - Ridge Regression (L2 æ­£å‰‡åŒ–)
    - é¸é …: ElasticNet, XGBoost meta-learner
    """

    def __init__(self, use_meta='ridge'):
        self.use_meta = use_meta
        self.base_models = {}
        self.meta_model = None
        self.feature_cols = None

    def _get_base_models(self):
        """å®šç¾©åŸºæ¨¡å‹"""
        models = {
            'xgboost': xgb.XGBRegressor(
                n_estimators=500,
                max_depth=8,
                learning_rate=0.05,
                subsample=0.85,
                colsample_bytree=0.85,
                objective='reg:squarederror',
                tree_method='hist',
                random_state=42,
                n_jobs=-1
            ),
            'randomforest': RandomForestRegressor(
                n_estimators=200,
                max_depth=12,
                min_samples_split=10,
                random_state=42,
                n_jobs=-1
            ),
            'gradientboosting': GradientBoostingRegressor(
                n_estimators=200,
                max_depth=6,
                learning_rate=0.05,
                random_state=42
            )
        }

        if LIGHTGBM_AVAILABLE:
            models['lightgbm'] = LGBMRegressor(
                n_estimators=300,
                max_depth=8,
                learning_rate=0.05,
                random_state=42,
                verbose=-1,
                n_jobs=-1
            )

        if CATBOOST_AVAILABLE:
            models['catboost'] = CatBoostRegressor(
                iterations=500,
                depth=8,
                learning_rate=0.05,
                random_state=42,
                verbose=False
            )

        return models

    def _get_meta_model(self):
        """å®šç¾©å…ƒå­¸ç¿’å™¨"""
        if self.use_meta == 'ridge':
            return Ridge(alpha=1.0)
        elif self.use_meta == 'elasticnet':
            return ElasticNet(alpha=0.5, l1_ratio=0.5)
        elif self.use_meta == 'xgboost':
            return xgb.XGBRegressor(
                n_estimators=100,
                max_depth=3,
                learning_rate=0.1,
                objective='reg:squarederror',
                random_state=42
            )
        else:
            return Ridge(alpha=1.0)

    def fit(self, X_train, y_train, X_val=None, y_val=None):
        """
        è¨“ç·´ Stacking Ensemble

        ä½¿ç”¨ Out-of-Fold é æ¸¬ä½œç‚ºå…ƒç‰¹å¾µï¼Œé¿å…æ•¸æ“šæ´©æ¼
        """
        print(f"\n{'='*60}")
        print("ğŸ”— è¨“ç·´ Stacking Ensemble")
        print(f"{'='*60}")

        self.feature_cols = X_train.columns.tolist()
        base_models = self._get_base_models()

        # ========================================
        # ç¬¬ä¸€å±¤: è¨“ç·´åŸºæ¨¡å‹
        # ========================================
        print(f"\nğŸ“Š ç¬¬ä¸€å±¤: è¨“ç·´ {len(base_models)} å€‹åŸºæ¨¡å‹")

        for name, model in base_models.items():
            print(f"   è¨“ç·´ {name}...", end=" ")

            if name == 'catboost' and CATBOOST_AVAILABLE:
                # CatBoost ç›´æ¥è™•ç† NaN
                model.fit(X_train, y_train, eval_set=(X_val, y_val) if X_val is not None else None)
            elif name == 'xgboost':
                model.fit(X_train, y_train, verbose=False)
            else:
                # RandomForest å’Œ GradientBoosting ä¸æ¥å— verbose åƒæ•¸
                model.fit(X_train, y_train)

            self.base_models[name] = model

            # è¨ˆç®—è¨“ç·´é›† MAE
            train_pred = model.predict(X_train)
            train_mae = mean_absolute_error(y_train, train_pred)
            print(f"MAE={train_mae:.2f}")

        # ========================================
        # ç¬¬äºŒå±¤: æº–å‚™å…ƒç‰¹å¾µ (Out-of-Fold)
        # ========================================
        print(f"\nğŸ“Š ç¬¬äºŒå±¤: ç”Ÿæˆ Out-of-Fold å…ƒç‰¹å¾µ")

        # ä½¿ç”¨ TimeSeriesSplit ç”Ÿæˆ OOF é æ¸¬
        n_splits = 5
        tscv = TimeSeriesSplit(n_splits=n_splits)

        # åˆå§‹åŒ– OOF é æ¸¬æ•¸çµ„
        oof_predictions = np.zeros((len(X_train), len(base_models)))
        model_names = list(base_models.keys())

        for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X_train)):
            print(f"   Fold {fold_idx + 1}/{n_splits}: è¨“ç·´ {len(train_idx)}, é©—è­‰ {len(val_idx)}", end=" ")

            X_fold_train = X_train.iloc[train_idx] if hasattr(X_train, 'iloc') else X_train[train_idx]
            y_fold_train = y_train[train_idx] if isinstance(y_train, np.ndarray) else y_train.iloc[train_idx]
            X_fold_val = X_train.iloc[val_idx] if hasattr(X_train, 'iloc') else X_train[val_idx]

            for i, (name, model_template) in enumerate(base_models.items()):
                # å‰µå»ºæ–°æ¨¡å‹å¯¦ä¾‹
                if name == 'xgboost':
                    model = xgb.XGBRegressor(
                        n_estimators=300, max_depth=6, learning_rate=0.1,
                        objective='reg:squarederror', random_state=42, n_jobs=-1
                    )
                elif name == 'randomforest':
                    model = RandomForestRegressor(
                        n_estimators=100, max_depth=10, random_state=42, n_jobs=-1
                    )
                elif name == 'gradientboosting':
                    model = GradientBoostingRegressor(
                        n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42
                    )
                elif name == 'lightgbm' and LIGHTGBM_AVAILABLE:
                    model = LGBMRegressor(
                        n_estimators=100, max_depth=6, learning_rate=0.1,
                        random_state=42, verbose=-1, n_jobs=-1
                    )
                elif name == 'catboost' and CATBOOST_AVAILABLE:
                    model = CatBoostRegressor(
                        iterations=100, depth=6, learning_rate=0.1,
                        random_state=42, verbose=False
                    )
                else:
                    continue

                # æ ¹æ“šæ¨¡å‹é¡å‹æ±ºå®šæ˜¯å¦ä½¿ç”¨ verbose åƒæ•¸
                if name in ['xgboost', 'lightgbm']:
                    model.fit(X_fold_train, y_fold_train, verbose=False)
                elif name == 'catboost':
                    model.fit(X_fold_train, y_fold_train, verbose=False)
                else:
                    model.fit(X_fold_train, y_fold_train)

                oof_predictions[val_idx, i] = model.predict(X_fold_val)

            print("âœ“")

        # ========================================
        # ç¬¬ä¸‰å±¤: è¨“ç·´å…ƒå­¸ç¿’å™¨
        # ========================================
        print(f"\nğŸ“Š ç¬¬ä¸‰å±¤: è¨“ç·´å…ƒå­¸ç¿’å™¨ ({self.use_meta})")

        # æ·»åŠ åŸå§‹ç‰¹å¾µä½œç‚ºè¼”åŠ© (å¯é¸)
        meta_X_train = oof_predictions
        meta_feature_names = [f'{name}_pred' for name in model_names]

        # è¨“ç·´å…ƒå­¸ç¿’å™¨
        self.meta_model = self._get_meta_model()
        self.meta_model.fit(meta_X_train, y_train)

        print(f"   å…ƒç‰¹å¾µ: {meta_feature_names}")
        print(f"   å…ƒå­¸ç¿’å™¨: {self.meta_model.__class__.__name__}")

        # é¡¯ç¤ºå…ƒå­¸ç¿’å™¨æ¬Šé‡ (å¦‚æœæ˜¯ç·šæ€§æ¨¡å‹)
        if hasattr(self.meta_model, 'coef_'):
            print(f"\n   å…ƒå­¸ç¿’å™¨æ¬Šé‡:")
            for name, weight in zip(model_names, self.meta_model.coef_):
                print(f"      {name}: {weight:.4f}")

        return self

    def predict(self, X):
        """é æ¸¬"""
        # ç¬¬ä¸€å±¤: ç²å–åŸºæ¨¡å‹é æ¸¬
        base_predictions = []
        for name in self._get_base_models().keys():
            if name in self.base_models:
                pred = self.base_models[name].predict(X)
                base_predictions.append(pred)

        # å †ç–Šæˆå…ƒç‰¹å¾µ
        meta_X = np.column_stack(base_predictions)

        # ç¬¬äºŒå±¤: å…ƒå­¸ç¿’å™¨é æ¸¬
        prediction = self.meta_model.predict(meta_X)

        return prediction

    def predict_with_base(self, X):
        """è¿”å›åŸºæ¨¡å‹é æ¸¬å’Œæœ€çµ‚é æ¸¬"""
        base_preds = {}
        for name, model in self.base_models.items():
            base_preds[name] = model.predict(X)

        # æœ€çµ‚é æ¸¬
        final_pred = self.predict(X)

        return {
            'final': final_pred,
            'base_predictions': base_preds
        }


def train_and_evaluate_stacking(train_data, test_data, feature_cols, use_meta='ridge'):
    """
    è¨“ç·´ä¸¦è©•ä¼° Stacking Ensemble
    """
    X_train = train_data[feature_cols].fillna(0)
    y_train = train_data['Attendance'].values
    X_test = test_data[feature_cols].fillna(0)
    y_test = test_data['Attendance'].values

    # å‰µå»ºé©—è­‰é›†
    val_size = len(X_train) // 5
    X_val = X_train[-val_size:]
    y_val = y_train[-val_size:]
    X_train_sub = X_train[:-val_size]
    y_train_sub = y_train[:-val_size]

    # è¨“ç·´ Stacking
    stacking = StackingEnsemble(use_meta=use_meta)
    stacking.fit(X_train_sub, y_train_sub, X_val, y_val)

    # é æ¸¬
    results = stacking.predict_with_base(X_test)

    # è©•ä¼°
    final_pred = results['final']
    mae = mean_absolute_error(y_test, final_pred)
    rmse = np.sqrt(mean_squared_error(y_test, final_pred))
    r2 = r2_score(y_test, final_pred)

    print(f"\n{'='*60}")
    print(f"ğŸ“Š Stacking Ensemble çµæœ")
    print(f"{'='*60}")
    print(f"   MAE:  {mae:.2f}")
    print(f"   RMSE: {rmse:.2f}")
    print(f"   RÂ²:   {r2:.4f}")

    # è¼¸å‡ºåŸºæ¨¡å‹çµæœ
    print(f"\n   åŸºæ¨¡å‹æ¯”è¼ƒ:")
    for name, pred in results['base_predictions'].items():
        mae_base = mean_absolute_error(y_test, pred)
        print(f"      {name}: MAE = {mae_base:.2f}")

    return stacking, {
        'mae': mae,
        'rmse': rmse,
        'r2': r2,
        'base_predictions': {k: mean_absolute_error(y_test, v) for k, v in results['base_predictions'].items()}
    }


def compare_all_ensembles(train_data, test_data, feature_cols):
    """
    æ¯”è¼ƒæ‰€æœ‰ Ensemble æ–¹æ³•
    """
    print("\n" + "=" * 80)
    print("ğŸ”¬ æ¯”è¼ƒæ‰€æœ‰ Ensemble æ–¹æ³•")
    print("=" * 80)

    results = {}

    X_train = train_data[feature_cols].fillna(0)
    y_train = train_data['Attendance'].values
    X_test = test_data[feature_cols].fillna(0)
    y_test = test_data['Attendance'].values

    # ========================================
    # 1. Simple Average
    # ========================================
    print("\n1ï¸âƒ£ Simple Average Ensemble")

    base_models = {
        'xgboost': xgb.XGBRegressor(n_estimators=500, max_depth=8, learning_rate=0.05,
                                     objective='reg:squarederror', random_state=42, n_jobs=-1),
        'randomforest': RandomForestRegressor(n_estimators=200, max_depth=12, random_state=42, n_jobs=-1),
        'gradientboosting': GradientBoostingRegressor(n_estimators=200, max_depth=6, learning_rate=0.05, random_state=42)
    }

    base_preds = {}
    for name, model in base_models.items():
        if name == 'xgboost':
            model.fit(X_train, y_train, verbose=False)
        else:
            model.fit(X_train, y_train)
        base_preds[name] = model.predict(X_test)

    # Simple Average
    simple_avg = np.mean(list(base_preds.values()), axis=0)
    mae_simple = mean_absolute_error(y_test, simple_avg)

    print(f"   MAE: {mae_simple:.2f}")
    results['simple_average'] = {'mae': mae_simple, 'predictions': simple_avg}

    # ========================================
    # 2. Weighted Average (é©—è­‰é›†å„ªåŒ–)
    # ========================================
    print("\n2ï¸âƒ£ Weighted Average Ensemble")

    val_size = len(X_train) // 5
    X_val = X_train[-val_size:]
    y_val = y_train[-val_size:]
    X_train_sub = X_train[:-val_size]
    y_train_sub = y_train[:-val_size]

    # åœ¨é©—è­‰é›†ä¸Šè©•ä¼°
    val_mae = {}
    val_preds = {}
    for name, model_template in base_models.items():
        if name == 'xgboost':
            model = xgb.XGBRegressor(n_estimators=300, max_depth=6, learning_rate=0.1,
                                     objective='reg:squarederror', random_state=42, n_jobs=-1)
        elif name == 'randomforest':
            model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
        else:
            model = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)

        model.fit(X_train_sub, y_train_sub)
        val_preds[name] = model.predict(X_val)
        val_mae[name] = mean_absolute_error(y_val, val_preds[name])

    # è¨ˆç®—æ¬Šé‡ (èª¤å·®è¶Šå°æ¬Šé‡è¶Šå¤§)
    weights = {k: 1/v for k, v in val_mae.items()}
    total_weight = sum(weights.values())
    weights = {k: v/total_weight for k, v in weights.items()}

    print(f"   æ¬Šé‡: {weights}")

    weighted_pred = (
        weights['xgboost'] * base_preds['xgboost'] +
        weights['randomforest'] * base_preds['randomforest'] +
        weights['gradientboosting'] * base_preds['gradientboosting']
    )
    mae_weighted = mean_absolute_error(y_test, weighted_pred)

    print(f"   MAE: {mae_weighted:.2f}")
    results['weighted_average'] = {'mae': mae_weighted, 'predictions': weighted_pred, 'weights': weights}

    # ========================================
    # 3. Stacking (Ridge)
    # ========================================
    print("\n3ï¸âƒ£ Stacking Ensemble (Ridge)")

    stacking_ridge, metrics_ridge = train_and_evaluate_stacking(
        train_data, test_data, feature_cols, use_meta='ridge'
    )
    results['stacking_ridge'] = metrics_ridge

    # ========================================
    # 4. Stacking (ElasticNet)
    # ========================================
    print("\n4ï¸âƒ£ Stacking Ensemble (ElasticNet)")

    stacking_enet, metrics_enet = train_and_evaluate_stacking(
        train_data, test_data, feature_cols, use_meta='elasticnet'
    )
    results['stacking_elasticnet'] = metrics_enet

    # ========================================
    # ç¸½çµ
    # ========================================
    print("\n" + "=" * 80)
    print("ğŸ† Ensemble æ–¹æ³•æ¯”è¼ƒ")
    print("=" * 80)

    sorted_results = sorted(results.items(), key=lambda x: x[1]['mae'])

    for name, result in sorted_results:
        print(f"   {name:25} MAE = {result['mae']:.2f}")

    best_name, best_result = sorted_results[0]
    print(f"\n   æœ€ä½³æ–¹æ³•: {best_name}")

    return results, sorted_results[0]


if __name__ == '__main__':
    # æ¸¬è©¦ä»£ç¢¼
    print("Stacking Ensemble æ¨¡çµ„ v1.0")
    print("è«‹ä½¿ç”¨ä¸»è…³æœ¬èª¿ç”¨æ­¤æ¨¡çµ„")
