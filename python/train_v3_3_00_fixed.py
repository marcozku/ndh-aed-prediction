# -*- coding: utf-8 -*-
"""
XGBoost æ¨¡å‹è¨“ç·´è…³æœ¬ v3.3.00 - ä¿®å¾©æ•¸æ“šæ´©æ¼ + å…¬çœ¾å‡æœŸç‰¹å¾µ
ä¿®å¾©å…§å®¹:
1. EWMA/Change ç‰¹å¾µä½¿ç”¨ shift(1) é¿å…æ•¸æ“šæ´©æ¼
2. æ·»åŠ é¦™æ¸¯å…¬çœ¾å‡æœŸç‰¹å¾µ
3. å¯¦æ–½æ™‚é–“åºåˆ—äº¤å‰é©—è­‰
4. å‹•æ…‹ç‰¹å¾µé‡è¦æ€§ç›£æ§

é æœŸæ€§èƒ½:
- è¨“ç·´ MAE: 6-8 äºº (æ›´çœŸå¯¦)
- ç”Ÿç”¢ MAE: 8-12 äºº (æ”¹å–„ 45-54%)
"""
import sys
import io

if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score as sklearn_r2_score
from sklearn.model_selection import TimeSeriesSplit
import json
import os
from datetime import datetime, timedelta

# æ“´å±•ç‰¹å¾µåˆ—è¡¨ (10 + 5 å‡æœŸç‰¹å¾µ)
OPTIMAL_FEATURES = [
    'Attendance_EWMA7', 'Daily_Change', 'Attendance_EWMA14',
    'Weekly_Change', 'Day_of_Week', 'Attendance_Lag7',
    'Attendance_Lag1', 'Is_Weekend', 'DayOfWeek_sin', 'DayOfWeek_cos',
    # æ–°å¢å‡æœŸç‰¹å¾µ
    'Is_Public_Holiday', 'Days_To_Holiday', 'Days_After_Holiday',
    'Is_Holiday_Eve', 'Is_Holiday_After'
]

# COVID æœŸé–“
COVID_PERIODS = [
    ('2020-01-23', '2020-04-08'),
    ('2020-07-16', '2020-09-30'),
    ('2020-11-23', '2021-01-05'),
    ('2022-02-05', '2022-04-30'),
    ('2022-11-10', '2022-12-27'),
]

# Optuna å„ªåŒ–çš„æœ€ä½³åƒæ•¸ (30 trials)
OPTUNA_BEST_PARAMS = {
    'max_depth': 9,
    'learning_rate': 0.045,
    'min_child_weight': 6,
    'subsample': 0.67,
    'colsample_bytree': 0.92,
    'gamma': 0.84,
    'reg_alpha': 1.35,
    'reg_lambda': 0.79,
    'objective': 'reg:squarederror',
    'tree_method': 'hist',
    'eval_metric': 'mae',
}


def load_holidays():
    """åŠ è¼‰é¦™æ¸¯å…¬çœ¾å‡æœŸæ•¸æ“š"""
    try:
        holidays_path = os.path.join(os.path.dirname(__file__), 'hk_public_holidays.json')
        with open(holidays_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # å±•å¹³æ‰€æœ‰å‡æœŸåˆ°ä¸€å€‹åˆ—è¡¨
        all_holidays = []
        for year, dates in data['holidays'].items():
            all_holidays.extend(dates)

        return set(pd.to_datetime(all_holidays).date)
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•åŠ è¼‰å‡æœŸæ•¸æ“š: {e}", flush=True)
        return set()


def load_data_from_db():
    """å¾æ•¸æ“šåº«åŠ è¼‰æ•¸æ“š"""
    try:
        from sqlalchemy import create_engine
        from urllib.parse import quote_plus
        from dotenv import load_dotenv
        load_dotenv()

        password = os.getenv('PGPASSWORD') or os.getenv('DATABASE_PASSWORD') or 'nIdJPREHqkBdMgUifrazOsVlWbxsmDGq'
        host = os.getenv('PGHOST') or 'tramway.proxy.rlwy.net'
        port = int(os.getenv('PGPORT') or '45703')
        user = os.getenv('PGUSER') or 'postgres'
        database = os.getenv('PGDATABASE') or 'railway'

        print(f"   ğŸ“¡ é€£æ¥è³‡æ–™åº«: {host}:{port}/{database}", flush=True)
        sys.stdout.flush()

        connection_string = f"postgresql://{user}:{quote_plus(password)}@{host}:{port}/{database}?sslmode=require"
        engine = create_engine(connection_string)

        print(f"   ğŸ“¥ æ­£åœ¨åŠ è¼‰æ•¸æ“š...", flush=True)
        sys.stdout.flush()

        query = "SELECT date as Date, patient_count as Attendance FROM actual_data ORDER BY date ASC"
        df = pd.read_sql_query(query, engine)

        # è™•ç†åˆ—å
        df.columns = [col if col in ['Date', 'Attendance'] else
                     ('Date' if col.lower() == 'date' else
                      'Attendance' if col.lower() in ['attendance', 'patient_count'] else col)
                     for col in df.columns]

        return df[['Date', 'Attendance']]
    except Exception as e:
        print(f"ç„¡æ³•å¾æ•¸æ“šåº«åŠ è¼‰æ•¸æ“š: {e}", flush=True)
        return None


def add_holiday_features(df, holidays):
    """æ·»åŠ å…¬çœ¾å‡æœŸç‰¹å¾µ"""
    print(f"   ğŸ† æ·»åŠ å…¬çœ¾å‡æœŸç‰¹å¾µ...", flush=True)

    df = df.copy()
    df['Date_only'] = pd.to_datetime(df['Date']).dt.date

    # 1. æ˜¯å¦ç‚ºå…¬çœ¾å‡æœŸ
    df['Is_Public_Holiday'] = df['Date_only'].isin(holidays).astype(int)

    # 2. è·é›¢ä¸‹ä¸€å€‹å‡æœŸçš„å¤©æ•¸
    def days_to_next_holiday(date):
        future_holidays = [h for h in holidays if h > date]
        if future_holidays:
            return (min(future_holidays) - date).days
        return 365  # å¦‚æœæ²’æœ‰æœªä¾†å‡æœŸï¼Œè¿”å›å¤§å€¼

    df['Days_To_Holiday'] = df['Date_only'].apply(days_to_next_holiday)

    # 3. è·é›¢ä¸Šä¸€å€‹å‡æœŸçš„å¤©æ•¸
    def days_after_last_holiday(date):
        past_holidays = [h for h in holidays if h < date]
        if past_holidays:
            return (date - max(past_holidays)).days
        return 365

    df['Days_After_Holiday'] = df['Date_only'].apply(days_after_last_holiday)

    # 4. æ˜¯å¦ç‚ºå‡æœŸå‰ä¸€å¤©
    df['Is_Holiday_Eve'] = (df['Days_To_Holiday'] == 1).astype(int)

    # 5. æ˜¯å¦ç‚ºå‡æœŸå¾Œä¸€å¤©
    df['Is_Holiday_After'] = (df['Days_After_Holiday'] == 1).astype(int)

    # é™åˆ¶è·é›¢ç‰¹å¾µçš„ç¯„åœ (é¿å…æ¥µç«¯å€¼)
    df['Days_To_Holiday'] = df['Days_To_Holiday'].clip(0, 30)
    df['Days_After_Holiday'] = df['Days_After_Holiday'].clip(0, 30)

    df = df.drop('Date_only', axis=1)

    holiday_count = df['Is_Public_Holiday'].sum()
    print(f"   âœ… è­˜åˆ¥ {holiday_count} å€‹å…¬çœ¾å‡æœŸ", flush=True)

    return df


def prepare_optimal_features(df, holidays):
    """æº–å‚™ç‰¹å¾µ (ä¿®å¾©æ•¸æ“šæ´©æ¼)"""
    print("\nğŸ“Š æº–å‚™ç‰¹å¾µ (v3.3.00 - ä¿®å¾©æ•¸æ“šæ´©æ¼)...", flush=True)
    sys.stdout.flush()

    df = df.copy()
    df['Date'] = pd.to_datetime(df['Date'])
    df = df.sort_values('Date').reset_index(drop=True)

    print(f"   ğŸ”¨ è¨ˆç®—æ™‚é–“ç‰¹å¾µ...", flush=True)
    df['Day_of_Week'] = df['Date'].dt.dayofweek
    df['Is_Weekend'] = (df['Day_of_Week'] >= 5).astype(int)

    print(f"   ğŸ”¨ è¨ˆç®—é€±æœŸç·¨ç¢¼...", flush=True)
    df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7)
    df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7)

    print(f"   ğŸ”¨ è¨ˆç®—æ»¯å¾Œç‰¹å¾µ...", flush=True)
    df['Attendance_Lag1'] = df['Attendance'].shift(1)
    df['Attendance_Lag7'] = df['Attendance'].shift(7)

    print(f"   ğŸ”¨ è¨ˆç®— EWMA (âœ… ä¿®å¾©æ•¸æ“šæ´©æ¼)...", flush=True)
    # âœ… ä¿®å¾©: ä½¿ç”¨ shift(1) é¿å…æ•¸æ“šæ´©æ¼ï¼Œä¸åŒ…å«ç•¶å¤©çš„ Attendance
    df['Attendance_EWMA7'] = df['Attendance'].shift(1).ewm(span=7, adjust=False).mean()
    df['Attendance_EWMA14'] = df['Attendance'].shift(1).ewm(span=14, adjust=False).mean()

    print(f"   ğŸ”¨ è¨ˆç®—è®ŠåŒ–ç‰¹å¾µ (âœ… ä¿®å¾©æ•¸æ“šæ´©æ¼)...", flush=True)
    # âœ… ä¿®å¾©: ä½¿ç”¨ shift(1) é¿å…æ•¸æ“šæ´©æ¼
    df['Daily_Change'] = df['Attendance'].shift(1).diff()
    df['Weekly_Change'] = df['Attendance'].shift(1).diff(7)

    # æ·»åŠ å…¬çœ¾å‡æœŸç‰¹å¾µ
    df = add_holiday_features(df, holidays)

    print(f"   ğŸ”¨ è™•ç†ç¼ºå¤±å€¼...", flush=True)
    df['Attendance_Lag1'] = df['Attendance_Lag1'].fillna(df['Attendance'].mean())
    df['Attendance_Lag7'] = df['Attendance_Lag7'].fillna(df['Attendance'].mean())
    df['Attendance_EWMA7'] = df['Attendance_EWMA7'].bfill()
    df['Attendance_EWMA14'] = df['Attendance_EWMA14'].bfill()
    df['Daily_Change'] = df['Daily_Change'].fillna(0)
    df['Weekly_Change'] = df['Weekly_Change'].fillna(0)

    df = df.dropna()

    print(f"   âœ… æº–å‚™å®Œæˆ: {len(df)} ç­†", flush=True)
    return df


def exclude_covid_periods(df):
    """æ’é™¤ COVID æœŸé–“"""
    print("\nğŸ¦  æ’é™¤ COVID æœŸé–“...", flush=True)
    sys.stdout.flush()
    original_count = len(df)

    for start, end in COVID_PERIODS:
        start_date = pd.to_datetime(start)
        end_date = pd.to_datetime(end)
        mask = (df['Date'] >= start_date) & (df['Date'] <= end_date)
        removed = mask.sum()
        df = df[~mask].copy()
        if removed > 0:
            print(f"   ç§»é™¤ {start} åˆ° {end}: -{removed} ç­†", flush=True)

    print(f"   ğŸ“Š éæ¿¾å¾Œ: {len(df)} ç­† (ç§»é™¤ {original_count - len(df)} ç­†)", flush=True)
    return df


def train_with_time_series_cv(X, y, n_splits=5):
    """ä½¿ç”¨æ™‚é–“åºåˆ—äº¤å‰é©—è­‰è¨“ç·´"""
    print(f"\nğŸ”„ æ™‚é–“åºåˆ—äº¤å‰é©—è­‰ ({n_splits} folds)...", flush=True)
    sys.stdout.flush()

    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=180)  # æ¯æ¬¡æ¸¬è©¦ 6 å€‹æœˆ

    fold_results = []

    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
        print(f"\n   ğŸ“ Fold {fold}/{n_splits}", flush=True)

        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        print(f"      è¨“ç·´: {len(X_train)} ç­†, æ¸¬è©¦: {len(X_test)} ç­†", flush=True)

        # è¨“ç·´æ¨¡å‹
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dtest = xgb.DMatrix(X_test, label=y_test)

        model = xgb.train(
            OPTUNA_BEST_PARAMS,
            dtrain,
            num_boost_round=500,
            evals=[(dtest, 'test')],
            early_stopping_rounds=50,
            verbose_eval=False
        )

        # è©•ä¼°
        y_pred = model.predict(dtest)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        r2 = sklearn_r2_score(y_test, y_pred)

        fold_results.append({
            'fold': fold,
            'mae': mae,
            'rmse': rmse,
            'mape': mape,
            'r2': r2
        })

        print(f"      MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%, RÂ²: {r2:.4f}", flush=True)

    # è¨ˆç®—å¹³å‡æ€§èƒ½
    avg_mae = np.mean([r['mae'] for r in fold_results])
    avg_rmse = np.mean([r['rmse'] for r in fold_results])
    avg_mape = np.mean([r['mape'] for r in fold_results])
    avg_r2 = np.mean([r['r2'] for r in fold_results])

    std_mae = np.std([r['mae'] for r in fold_results])

    print(f"\n   ğŸ“Š äº¤å‰é©—è­‰å¹³å‡æ€§èƒ½:", flush=True)
    print(f"      MAE:  {avg_mae:.2f} Â± {std_mae:.2f}", flush=True)
    print(f"      RMSE: {avg_rmse:.2f}", flush=True)
    print(f"      MAPE: {avg_mape:.2f}%", flush=True)
    print(f"      RÂ²:   {avg_r2:.4f}", flush=True)

    return fold_results, avg_mae


def train_final_model(X_train, y_train, X_test, y_test):
    """è¨“ç·´æœ€çµ‚æ¨¡å‹"""
    print(f"\nğŸš€ è¨“ç·´æœ€çµ‚æ¨¡å‹...", flush=True)
    print(f"   è¨“ç·´é›†: {len(X_train)} ç­†", flush=True)
    print(f"   æ¸¬è©¦é›†: {len(X_test)} ç­†", flush=True)
    print(f"   ç‰¹å¾µæ•¸: {len(X_train.columns)} å€‹", flush=True)
    sys.stdout.flush()

    val_idx = int(len(X_train) * 0.85)
    X_train_sub = X_train.iloc[:val_idx]
    y_train_sub = y_train.iloc[:val_idx]
    X_val = X_train.iloc[val_idx:]
    y_val = y_train.iloc[val_idx:]

    print(f"\nğŸ”¨ å»ºç«‹è¨“ç·´çŸ©é™£...", flush=True)
    sys.stdout.flush()

    dtrain = xgb.DMatrix(X_train_sub, label=y_train_sub)
    dval = xgb.DMatrix(X_val, label=y_val)

    print(f"ğŸ‹ï¸ é–‹å§‹è¨“ç·´ (max 500 rounds, early stopping 50)...", flush=True)
    sys.stdout.flush()

    model = xgb.train(
        OPTUNA_BEST_PARAMS,
        dtrain,
        num_boost_round=500,
        evals=[(dval, 'validation')],
        early_stopping_rounds=50,
        verbose_eval=False
    )

    best_iteration = model.best_iteration if hasattr(model, 'best_iteration') else 0
    print(f"\n   âœ… è¨“ç·´å®Œæˆï¼Œæœ€ä½³è¿­ä»£: {best_iteration}", flush=True)

    # è©•ä¼°
    print(f"\nğŸ“Š è©•ä¼°æ¨¡å‹æ€§èƒ½...", flush=True)
    sys.stdout.flush()

    dtest = xgb.DMatrix(X_test)
    y_pred = model.predict(dtest)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r2 = sklearn_r2_score(y_test, y_pred)

    print(f"\nğŸ“Š æœ€çµ‚æ¨¡å‹æ€§èƒ½:", flush=True)
    print(f"   MAE:  {mae:.2f}", flush=True)
    print(f"   RMSE: {rmse:.2f}", flush=True)
    print(f"   MAPE: {mape:.2f}%", flush=True)
    print(f"   RÂ²:   {r2:.4f}", flush=True)

    # ç‰¹å¾µé‡è¦æ€§
    importance_scores = model.get_score(importance_type='weight')
    feature_importance = {}
    for i, feat in enumerate(X_train.columns):
        key = f'f{i}'
        feature_importance[feat] = float(importance_scores.get(key, 0.0))

    # æ’åºä¸¦é¡¯ç¤º
    sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
    print(f"\nğŸ“Š ç‰¹å¾µé‡è¦æ€§ (Top 10):", flush=True)
    for i, (feat, imp) in enumerate(sorted_importance[:10], 1):
        print(f"   {i:2d}. {feat:25s}: {imp:.1f}", flush=True)

    return model, {'mae': mae, 'rmse': rmse, 'mape': mape, 'r2': r2}, feature_importance


def main():
    print("=" * 80, flush=True)
    print("ğŸ¯ XGBoost æ¨¡å‹è¨“ç·´ v3.3.00 - ä¿®å¾©æ•¸æ“šæ´©æ¼ + å…¬çœ¾å‡æœŸ", flush=True)
    print("=" * 80, flush=True)
    print(f"æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n", flush=True)
    sys.stdout.flush()

    # 1. åŠ è¼‰å‡æœŸæ•¸æ“š
    print("ğŸ† åŠ è¼‰é¦™æ¸¯å…¬çœ¾å‡æœŸæ•¸æ“š...", flush=True)
    holidays = load_holidays()
    print(f"   âœ… åŠ è¼‰ {len(holidays)} å€‹å‡æœŸ", flush=True)

    # 2. åŠ è¼‰æ•¸æ“š
    print("\nğŸ“¥ åŠ è¼‰æ•¸æ“š...", flush=True)
    sys.stdout.flush()

    df = load_data_from_db()

    if df is None:
        print("âŒ ç„¡æ³•åŠ è¼‰æ•¸æ“š", flush=True)
        return

    print(f"   âœ… åŠ è¼‰ {len(df)} ç­†è¨˜éŒ„", flush=True)
    print(f"   ğŸ“… ç¯„åœ: {df['Date'].min()} â†’ {df['Date'].max()}", flush=True)
    sys.stdout.flush()

    # 3. æº–å‚™ç‰¹å¾µ
    df = prepare_optimal_features(df, holidays)

    # 4. æ’é™¤ COVID
    df = exclude_covid_periods(df)

    # 5. åˆ†å‰²æ•¸æ“š
    print("\nâœ‚ï¸ åˆ†å‰²æ•¸æ“š (80/20)...", flush=True)
    split_idx = int(len(df) * 0.8)
    train_df = df[:split_idx]
    test_df = df[split_idx:]

    print(f"   è¨“ç·´é›†: {len(train_df)} ç­†", flush=True)
    print(f"   æ¸¬è©¦é›†: {len(test_df)} ç­†", flush=True)
    sys.stdout.flush()

    X_train = train_df[OPTIMAL_FEATURES]
    y_train = train_df['Attendance']
    X_test = test_df[OPTIMAL_FEATURES]
    y_test = test_df['Attendance']

    # 6. æ™‚é–“åºåˆ—äº¤å‰é©—è­‰
    fold_results, avg_cv_mae = train_with_time_series_cv(
        pd.concat([X_train, X_test]),
        pd.concat([y_train, y_test]),
        n_splits=5
    )

    # 7. è¨“ç·´æœ€çµ‚æ¨¡å‹
    model, metrics, feature_importance = train_final_model(X_train, y_train, X_test, y_test)

    # 8. å°æ¯” v3.2.01
    print("\n" + "=" * 80, flush=True)
    print("ğŸ“Š æ€§èƒ½å°æ¯” (v3.2.01 vs v3.3.00)", flush=True)
    print("=" * 80, flush=True)

    print(f"\n{'æŒ‡æ¨™':<15} {'v3.2.01 (æ´©æ¼)':<20} {'v3.3.00 (ä¿®å¾©)':<20} {'è®ŠåŒ–':<15}", flush=True)
    print("-" * 70, flush=True)

    old_mae = 2.85
    old_rmse = 4.54
    old_mape = 1.17
    old_r2 = 0.9718

    mae_change = ((metrics['mae'] - old_mae) / old_mae) * 100
    rmse_change = ((metrics['rmse'] - old_rmse) / old_rmse) * 100

    print(f"{'MAE':<15} {old_mae:<20.2f} {metrics['mae']:<20.2f} {mae_change:+.1f}%", flush=True)
    print(f"{'RMSE':<15} {old_rmse:<20.2f} {metrics['rmse']:<20.2f} {rmse_change:+.1f}%", flush=True)
    print(f"{'MAPE':<15} {old_mape:<20.2f}% {metrics['mape']:<19.2f}% -", flush=True)
    print(f"{'RÂ²':<15} {old_r2:<20.4f} {metrics['r2']:<20.4f} -", flush=True)
    print(f"{'CV MAE':<15} {'N/A':<20} {avg_cv_mae:<20.2f} -", flush=True)

    print(f"\nğŸ’¡ åˆ†æ:", flush=True)
    print(f"   - è¨“ç·´ MAE ä¸Šå‡ {mae_change:.1f}% æ˜¯æ­£å¸¸çš„ï¼ˆä¿®å¾©æ•¸æ“šæ´©æ¼å¾Œï¼‰", flush=True)
    print(f"   - é€™å€‹ MAE æ›´æ¥è¿‘çœŸå¯¦ç”Ÿç”¢ç’°å¢ƒæ€§èƒ½", flush=True)
    print(f"   - é æœŸç”Ÿç”¢ MAE å¾ 21.93 é™åˆ° 8-12 äººï¼ˆæ”¹å–„ 45-54%ï¼‰", flush=True)

    # 9. ä¿å­˜æ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹...", flush=True)
    sys.stdout.flush()

    models_dir = os.path.join(os.path.dirname(__file__), 'models')
    os.makedirs(models_dir, exist_ok=True)

    model_path = os.path.join(models_dir, 'xgboost_v3_3_00_fixed.json')
    model.save_model(model_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}", flush=True)

    # ä¿å­˜ç‰¹å¾µåˆ—è¡¨
    features_path = os.path.join(models_dir, 'xgboost_v3_3_00_features.json')
    with open(features_path, 'w', encoding='utf-8') as f:
        json.dump(OPTIMAL_FEATURES, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ ç‰¹å¾µåˆ—è¡¨å·²ä¿å­˜: {features_path}", flush=True)

    # ä¿å­˜æŒ‡æ¨™
    metrics_data = {
        'version': '3.3.00',
        'model_name': 'xgboost_v3_3_00_fixed',
        'features': OPTIMAL_FEATURES,
        'n_features': len(OPTIMAL_FEATURES),
        'mae': metrics['mae'],
        'rmse': metrics['rmse'],
        'mape': metrics['mape'],
        'r2': metrics['r2'],
        'cv_mae': avg_cv_mae,
        'cv_results': fold_results,
        'improvements': {
            'data_leakage_fixed': True,
            'holiday_features_added': True,
            'time_series_cv': True
        },
        'comparison_v3_2_01': {
            'old_mae': old_mae,
            'new_mae': metrics['mae'],
            'mae_change_pct': mae_change,
            'note': 'è¨“ç·´ MAE ä¸Šå‡æ˜¯æ­£å¸¸çš„ï¼ˆä¿®å¾©æ•¸æ“šæ´©æ¼å¾Œï¼‰ï¼Œæ›´æ¥è¿‘çœŸå¯¦æ€§èƒ½'
        },
        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'train_size': len(train_df),
        'test_size': len(test_df),
        'feature_importance': feature_importance,
        'optuna_params': OPTUNA_BEST_PARAMS
    }

    metrics_path = os.path.join(models_dir, 'xgboost_v3_3_00_metrics.json')
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ æŒ‡æ¨™å·²ä¿å­˜: {metrics_path}", flush=True)

    print("\n" + "=" * 80, flush=True)
    print("âœ… è¨“ç·´å®Œæˆï¼", flush=True)
    print("=" * 80, flush=True)


if __name__ == '__main__':
    main()
