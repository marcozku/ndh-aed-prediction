"""
XGBoost å„ªåŒ–ç‰ˆè¨“ç·´è…³æœ¬ v2.9.51
ä½¿ç”¨ç‰¹å¾µé¸æ“‡å„ªåŒ–ï¼Œåªä½¿ç”¨æœ€é‡è¦çš„ 20-30 å€‹ç‰¹å¾µ
"""
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import json
import os
import sys
import datetime
import time

try:
    from zoneinfo import ZoneInfo
except ImportError:
    from backports.zoneinfo import ZoneInfo

from feature_engineering import create_comprehensive_features

HKT = ZoneInfo('Asia/Hong_Kong')

# æœ€ä½³ç‰¹å¾µé›†ï¼ˆåŸºæ–¼ç‰¹å¾µé‡è¦æ€§åˆ†æï¼‰
OPTIMAL_FEATURES = [
    # Top 20 ç‰¹å¾µï¼ˆä½”ç¸½é‡è¦æ€§ 95%+ï¼‰
    "Attendance_EWMA7",        # 87.87% - çµ•å°æ ¸å¿ƒï¼
    "Monthly_Change",           # 2.59%
    "Daily_Change",             # 2.58%
    "Attendance_Lag1",          # 1.18%
    "Weekly_Change",            # 0.83%
    "Attendance_Rolling7",      # 0.48%
    "Attendance_Position7",     # 0.47%
    "Attendance_Lag30",         # 0.41%
    "Attendance_Lag7",          # 0.34%
    "Day_of_Week",              # 0.32%
    "Lag1_Diff",                # 0.30%
    "DayOfWeek_sin",            # 0.21%
    "Attendance_Rolling14",     # 0.17%
    "Attendance_Position14",    # 0.16%
    "Attendance_Position30",    # 0.13%
    "Attendance_Rolling3",      # 0.12%
    "Attendance_Min7",          # 0.11%
    "Attendance_Median14",      # 0.10%
    "DayOfWeek_Target_Mean",    # 0.09%
    "Attendance_Median3",       # 0.08%
    # å¯é¸ï¼šæ·»åŠ æ›´å¤šå¦‚æœéœ€è¦
    "Attendance_EWMA14",
    "Attendance_EWMA30",
    "Is_Winter_Flu_Season",
    "Is_Weekend",
    "Holiday_Factor",
]

def load_data_from_csv(csv_path):
    """å¾ CSV æ–‡ä»¶åŠ è¼‰æ•¸æ“š"""
    try:
        df = pd.read_csv(csv_path)
        if 'Date' not in df.columns and 'date' in df.columns:
            df['Date'] = df['date']
        if 'Attendance' not in df.columns and 'patient_count' in df.columns:
            df['Attendance'] = df['patient_count']
        return df[['Date', 'Attendance']]
    except Exception as e:
        print(f"ç„¡æ³•å¾ CSV åŠ è¼‰æ•¸æ“š: {e}")
        return None

def main():
    print(f"\n{'='*60}")
    print("ğŸ¥ NDH AED XGBoost å„ªåŒ–ç‰ˆè¨“ç·´ v2.9.51")
    print(f"{'='*60}")
    print(f"â° é–‹å§‹æ™‚é–“: {datetime.datetime.now(HKT).strftime('%Y-%m-%d %H:%M:%S')} HKT")
    print(f"ğŸ“Š ä½¿ç”¨å„ªåŒ–ç‰¹å¾µé›†: {len(OPTIMAL_FEATURES)} å€‹ç‰¹å¾µ")
    
    # åŠ è¼‰æ•¸æ“š
    df = load_data_from_csv('../NDH_AED_Clean.csv')
    if df is None:
        print("âŒ ç„¡æ³•åŠ è¼‰æ•¸æ“š")
        return
    
    print(f"\nğŸ“Š æ•¸æ“šé‡: {len(df)} ç­†")
    print(f"ğŸ“… æ—¥æœŸç¯„åœ: {df['Date'].min()} â†’ {df['Date'].max()}")
    
    # å‰µå»ºç‰¹å¾µ
    print("\nâ³ å‰µå»ºç‰¹å¾µä¸­...")
    df = create_comprehensive_features(df)
    df = df.dropna(subset=['Attendance'])
    
    # é¸æ“‡å„ªåŒ–ç‰¹å¾µ
    available_features = [f for f in OPTIMAL_FEATURES if f in df.columns]
    print(f"âœ… ä½¿ç”¨ {len(available_features)} å€‹ç‰¹å¾µ")
    
    # æ™‚é–“åºåˆ—åˆ†å‰²
    split_idx = int(len(df) * 0.8)
    train_data = df[:split_idx].copy()
    test_data = df[split_idx:].copy()
    
    X_train = train_data[available_features]
    y_train = train_data['Attendance']
    X_test = test_data[available_features]
    y_test = test_data['Attendance']
    
    print(f"\nğŸ“Š æ•¸æ“šåˆ†å‰²:")
    print(f"   è¨“ç·´é›†: {len(train_data)} ç­†")
    print(f"   æ¸¬è©¦é›†: {len(test_data)} ç­†")
    
    # è¨“ç·´æ¨¡å‹
    print(f"\nğŸ”¥ é–‹å§‹è¨“ç·´...")
    model = xgb.XGBRegressor(
        n_estimators=300,
        max_depth=6,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )
    
    start_time = time.time()
    model.fit(X_train, y_train, verbose=False)
    train_time = time.time() - start_time
    
    # è©•ä¼°
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    r2 = r2_score(y_test, y_pred)
    
    n = len(y_test)
    p = len(available_features)
    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    
    print(f"\nâœ… è¨“ç·´å®Œæˆï¼è€—æ™‚: {train_time:.2f} ç§’")
    print(f"\n{'='*60}")
    print("ğŸ“Š æ¨¡å‹æ€§èƒ½")
    print(f"{'='*60}")
    print(f"   MAE: {mae:.2f} äºº")
    print(f"   MAPE: {mape:.2f}%")
    print(f"   RÂ²: {r2*100:.1f}%")
    print(f"   èª¿æ•´ RÂ²: {adj_r2*100:.1f}%")
    
    # ä¿å­˜æ¨¡å‹ï¼ˆä½¿ç”¨ booster ç›´æ¥ä¿å­˜ä»¥é¿å… sklearn å•é¡Œï¼‰
    model.get_booster().save_model('models/xgboost_optimized.json')
    
    # ä¿å­˜æŒ‡æ¨™
    metrics = {
        'mae': mae,
        'rmse': rmse,
        'mape': mape,
        'r2': r2,
        'adj_r2': adj_r2,
        'feature_count': len(available_features),
        'features': available_features,
        'training_date': datetime.datetime.now(HKT).strftime('%Y-%m-%d %H:%M:%S HKT'),
        'version': '2.9.51-optimized'
    }
    
    with open('models/xgboost_optimized_metrics.json', 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ° models/xgboost_optimized.json")
    
    # èˆ‡å®Œæ•´ç‰ˆæ¯”è¼ƒ
    print(f"\n{'='*60}")
    print("ğŸ“Š èˆ‡å®Œæ•´ç‰ˆ (161ç‰¹å¾µ) æ¯”è¼ƒ")
    print(f"{'='*60}")
    print(f"   å„ªåŒ–ç‰ˆ ({len(available_features)} ç‰¹å¾µ): MAE={mae:.2f}, RÂ²={r2*100:.1f}%")
    print(f"   å®Œæ•´ç‰ˆ (161 ç‰¹å¾µ): MAE=3.44, RÂ²=96.3% (åƒè€ƒ)")
    
    if mae < 3.44:
        print(f"\n   ğŸ† å„ªåŒ–ç‰ˆæ›´å¥½ï¼MAE é™ä½äº† {3.44 - mae:.2f}")
    else:
        print(f"\n   â„¹ï¸ å·®ç•°å¾ˆå°ï¼Œå„ªåŒ–ç‰ˆç‰¹å¾µæ•¸æ¸›å°‘ {161 - len(available_features)} å€‹")

if __name__ == '__main__':
    main()
