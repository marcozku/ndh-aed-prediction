"""
è‡ªå‹•ç‰¹å¾µå„ªåŒ–å™¨ v1.0
æ™ºèƒ½å°‹æ‰¾æœ€ä½³ç‰¹å¾µçµ„åˆï¼Œå¯¦ç¾æœ€é«˜æº–ç¢ºåº¦é æ¸¬

åŠŸèƒ½ï¼š
1. å¤šç¨®ç‰¹å¾µé¸æ“‡ç®—æ³•ï¼ˆé‡è¦æ€§ã€RFEã€ç›¸é—œæ€§éæ¿¾ï¼‰
2. è‡ªå‹•åŒ–æ¸¬è©¦ä¸åŒç‰¹å¾µçµ„åˆ
3. æŒçºŒå­¸ç¿’ä¸¦è¨˜éŒ„æœ€ä½³é…ç½®
4. èˆ‡ä¸»è¨“ç·´è…³æœ¬æ•´åˆ

Usage:
    python auto_feature_optimizer.py              # é‹è¡Œå®Œæ•´å„ªåŒ–
    python auto_feature_optimizer.py --quick      # å¿«é€Ÿå„ªåŒ–ï¼ˆè¼ƒå°‘è©¦é©—ï¼‰
    python auto_feature_optimizer.py --update     # æ ¹æ“šæ­·å²è¨˜éŒ„æ›´æ–°
"""
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.feature_selection import RFE
import json
import os
import sys
import argparse
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from feature_engineering import create_comprehensive_features, get_feature_columns

# é¦™æ¸¯æ™‚å€
os.environ['TZ'] = 'Asia/Hong_Kong'

# å„ªåŒ–æ­·å²è¨˜éŒ„æ–‡ä»¶
OPTIMIZATION_HISTORY_FILE = 'models/feature_optimization_history.json'
OPTIMAL_FEATURES_FILE = 'models/optimal_features.json'


def get_hkt_time():
    """ç²å–é¦™æ¸¯æ™‚é–“"""
    from datetime import timezone, timedelta
    hkt = timezone(timedelta(hours=8))
    return datetime.now(hkt).strftime('%Y-%m-%d %H:%M:%S HKT')


def load_data():
    """åŠ è¼‰æ•¸æ“š"""
    csv_paths = ['../NDH_AED_Clean.csv', 'NDH_AED_Clean.csv', '/workspace/NDH_AED_Clean.csv']
    for csv_path in csv_paths:
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path)
            if 'date' in df.columns:
                df['Date'] = df['date']
            if 'patient_count' in df.columns:
                df['Attendance'] = df['patient_count']
            return df[['Date', 'Attendance']]
    return None


def load_optimization_history():
    """åŠ è¼‰å„ªåŒ–æ­·å²"""
    if os.path.exists(OPTIMIZATION_HISTORY_FILE):
        with open(OPTIMIZATION_HISTORY_FILE, 'r') as f:
            return json.load(f)
    return {'optimizations': [], 'best_ever': None}


def save_optimization_history(history):
    """ä¿å­˜å„ªåŒ–æ­·å²"""
    os.makedirs(os.path.dirname(OPTIMIZATION_HISTORY_FILE), exist_ok=True)
    with open(OPTIMIZATION_HISTORY_FILE, 'w') as f:
        json.dump(history, f, indent=2, ensure_ascii=False)


def calculate_feature_correlations(df, feature_cols, target='Attendance'):
    """è¨ˆç®—ç‰¹å¾µèˆ‡ç›®æ¨™çš„ç›¸é—œæ€§"""
    correlations = {}
    for col in feature_cols:
        if col in df.columns:
            corr = df[col].corr(df[target])
            if not np.isnan(corr):
                correlations[col] = abs(corr)
    return correlations


def remove_highly_correlated_features(df, feature_cols, threshold=0.95):
    """ç§»é™¤é«˜åº¦ç›¸é—œçš„å†—é¤˜ç‰¹å¾µ"""
    X = df[feature_cols].copy()
    
    # è¨ˆç®—ç›¸é—œçŸ©é™£
    corr_matrix = X.corr().abs()
    
    # æ‰¾å‡ºé«˜åº¦ç›¸é—œçš„ç‰¹å¾µå°
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    
    to_drop = set()
    for col in upper.columns:
        high_corr = upper[col][upper[col] > threshold].index.tolist()
        if high_corr:
            # ä¿ç•™ç¬¬ä¸€å€‹ï¼Œç§»é™¤å…¶ä»–
            to_drop.update(high_corr)
    
    remaining = [col for col in feature_cols if col not in to_drop]
    return remaining, list(to_drop)


def feature_importance_selection(X_train, y_train, X_test, y_test, all_features, 
                                  test_sizes=[5, 10, 15, 20, 25, 30, 40, 50, 75, 100]):
    """åŸºæ–¼ç‰¹å¾µé‡è¦æ€§çš„é¸æ“‡"""
    print("\nğŸ“Š æ–¹æ³• 1: ç‰¹å¾µé‡è¦æ€§æ’åºé¸æ“‡")
    print("-" * 50)
    
    # è¨“ç·´å®Œæ•´æ¨¡å‹ç²å–é‡è¦æ€§
    model = xgb.XGBRegressor(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.05,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train[all_features], y_train, verbose=False)
    
    # æ’åºç‰¹å¾µ
    importance = model.feature_importances_
    feature_importance = list(zip(all_features, importance))
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    
    results = []
    best_mae = float('inf')
    best_config = None
    
    for n_features in test_sizes:
        if n_features > len(all_features):
            continue
            
        top_features = [f[0] for f in feature_importance[:n_features]]
        
        # è¨“ç·´æ¸¬è©¦
        m = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            random_state=42,
            n_jobs=-1
        )
        m.fit(X_train[top_features], y_train, verbose=False)
        y_pred = m.predict(X_test[top_features])
        
        mae = mean_absolute_error(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        r2 = r2_score(y_test, y_pred)
        
        results.append({
            'method': 'importance',
            'n_features': n_features,
            'features': top_features,
            'mae': mae,
            'mape': mape,
            'r2': r2
        })
        
        status = "ğŸ†" if mae < best_mae else "  "
        print(f"   {status} {n_features:3}å€‹ç‰¹å¾µ: MAE={mae:.2f}, MAPE={mape:.2f}%, RÂ²={r2*100:.1f}%")
        
        if mae < best_mae:
            best_mae = mae
            best_config = results[-1]
    
    return results, best_config, feature_importance


def rfe_selection(X_train, y_train, X_test, y_test, all_features, target_sizes=[15, 20, 25, 30]):
    """éæ­¸ç‰¹å¾µæ¶ˆé™¤é¸æ“‡"""
    print("\nğŸ“Š æ–¹æ³• 2: éæ­¸ç‰¹å¾µæ¶ˆé™¤ (RFE)")
    print("-" * 50)
    
    results = []
    best_mae = float('inf')
    best_config = None
    
    for n_features in target_sizes:
        if n_features > len(all_features):
            continue
            
        print(f"   â³ æ¸¬è©¦ {n_features} å€‹ç‰¹å¾µ (RFE)...")
        
        # RFE
        base_model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=4,
            learning_rate=0.1,
            random_state=42,
            n_jobs=-1
        )
        
        rfe = RFE(estimator=base_model, n_features_to_select=n_features, step=5)
        rfe.fit(X_train[all_features], y_train)
        
        selected_features = [f for f, s in zip(all_features, rfe.support_) if s]
        
        # è¨“ç·´æ¸¬è©¦
        m = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            random_state=42,
            n_jobs=-1
        )
        m.fit(X_train[selected_features], y_train, verbose=False)
        y_pred = m.predict(X_test[selected_features])
        
        mae = mean_absolute_error(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        r2 = r2_score(y_test, y_pred)
        
        results.append({
            'method': 'rfe',
            'n_features': n_features,
            'features': selected_features,
            'mae': mae,
            'mape': mape,
            'r2': r2
        })
        
        status = "ğŸ†" if mae < best_mae else "  "
        print(f"   {status} {n_features:3}å€‹ç‰¹å¾µ: MAE={mae:.2f}, MAPE={mape:.2f}%, RÂ²={r2*100:.1f}%")
        
        if mae < best_mae:
            best_mae = mae
            best_config = results[-1]
    
    return results, best_config


def correlation_based_selection(X_train, y_train, X_test, y_test, all_features, df_train,
                                 target_counts=[15, 20, 25, 30, 40]):
    """åŸºæ–¼ç›¸é—œæ€§çš„ç‰¹å¾µé¸æ“‡"""
    print("\nğŸ“Š æ–¹æ³• 3: ç›¸é—œæ€§é¸æ“‡ + å»å†—é¤˜")
    print("-" * 50)
    
    # è¨ˆç®—èˆ‡ç›®æ¨™çš„ç›¸é—œæ€§
    correlations = {}
    for col in all_features:
        corr = df_train[col].corr(df_train['Attendance'])
        if not np.isnan(corr):
            correlations[col] = abs(corr)
    
    # æ’åº
    sorted_features = sorted(correlations.items(), key=lambda x: x[1], reverse=True)
    
    results = []
    best_mae = float('inf')
    best_config = None
    
    for n_features in target_counts:
        if n_features > len(sorted_features):
            continue
            
        # é¸æ“‡ top ç›¸é—œç‰¹å¾µ
        selected = [f[0] for f in sorted_features[:n_features]]
        
        # å»é™¤é«˜åº¦ç›¸é—œç‰¹å¾µ
        remaining, dropped = remove_highly_correlated_features(
            df_train, selected, threshold=0.95
        )
        
        if len(remaining) < 5:
            remaining = selected[:max(5, n_features//2)]
        
        # è¨“ç·´æ¸¬è©¦
        m = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            random_state=42,
            n_jobs=-1
        )
        m.fit(X_train[remaining], y_train, verbose=False)
        y_pred = m.predict(X_test[remaining])
        
        mae = mean_absolute_error(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        r2 = r2_score(y_test, y_pred)
        
        results.append({
            'method': 'correlation',
            'n_features': len(remaining),
            'features': remaining,
            'mae': mae,
            'mape': mape,
            'r2': r2
        })
        
        status = "ğŸ†" if mae < best_mae else "  "
        print(f"   {status} {len(remaining):3}å€‹ç‰¹å¾µ: MAE={mae:.2f}, MAPE={mape:.2f}%, RÂ²={r2*100:.1f}%")
        
        if mae < best_mae:
            best_mae = mae
            best_config = results[-1]
    
    return results, best_config


def hybrid_selection(X_train, y_train, X_test, y_test, all_features, df_train, feature_importance):
    """æ··åˆé¸æ“‡ç­–ç•¥ï¼šçµåˆå¤šç¨®æ–¹æ³•çš„å„ªå‹¢"""
    print("\nğŸ“Š æ–¹æ³• 4: æ··åˆæ™ºèƒ½é¸æ“‡")
    print("-" * 50)
    
    # 1. å¾é‡è¦æ€§æ’åºå– top ç‰¹å¾µ
    imp_sorted = [f[0] for f in feature_importance]
    
    # 2. å¾ç›¸é—œæ€§å– top ç‰¹å¾µ
    correlations = {}
    for col in all_features:
        corr = df_train[col].corr(df_train['Attendance'])
        if not np.isnan(corr):
            correlations[col] = abs(corr)
    corr_sorted = sorted(correlations.items(), key=lambda x: x[1], reverse=True)
    corr_sorted = [f[0] for f in corr_sorted]
    
    results = []
    best_mae = float('inf')
    best_config = None
    
    # æ¸¬è©¦ä¸åŒçš„æ··åˆç­–ç•¥
    strategies = [
        ('importance_top20', imp_sorted[:20]),
        ('importance_top25', imp_sorted[:25]),
        ('importance_top30', imp_sorted[:30]),
        ('corr_top20', corr_sorted[:20]),
        ('corr_top25', corr_sorted[:25]),
        ('hybrid_15+10', list(set(imp_sorted[:15] + corr_sorted[:10]))),
        ('hybrid_20+10', list(set(imp_sorted[:20] + corr_sorted[:10]))),
        ('hybrid_15+15', list(set(imp_sorted[:15] + corr_sorted[:15]))),
    ]
    
    for name, selected in strategies:
        # ç¢ºä¿ç‰¹å¾µå­˜åœ¨
        selected = [f for f in selected if f in X_train.columns]
        
        if len(selected) < 5:
            continue
        
        # è¨“ç·´æ¸¬è©¦
        m = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            random_state=42,
            n_jobs=-1
        )
        m.fit(X_train[selected], y_train, verbose=False)
        y_pred = m.predict(X_test[selected])
        
        mae = mean_absolute_error(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        r2 = r2_score(y_test, y_pred)
        
        results.append({
            'method': f'hybrid_{name}',
            'n_features': len(selected),
            'features': selected,
            'mae': mae,
            'mape': mape,
            'r2': r2
        })
        
        status = "ğŸ†" if mae < best_mae else "  "
        print(f"   {status} {name:20}: {len(selected):2}å€‹ç‰¹å¾µ, MAE={mae:.2f}, MAPE={mape:.2f}%, RÂ²={r2*100:.1f}%")
        
        if mae < best_mae:
            best_mae = mae
            best_config = results[-1]
    
    return results, best_config


def run_optimization(quick=False):
    """é‹è¡Œå®Œæ•´å„ªåŒ–æµç¨‹"""
    print("=" * 70)
    print("ğŸ”¬ è‡ªå‹•ç‰¹å¾µå„ªåŒ–å™¨ v1.0")
    print("=" * 70)
    print(f"â° é–‹å§‹æ™‚é–“: {get_hkt_time()}")
    print(f"ğŸ“Š æ¨¡å¼: {'å¿«é€Ÿ' if quick else 'å®Œæ•´'}")
    
    # åŠ è¼‰æ•¸æ“š
    print("\nğŸ“¥ åŠ è¼‰æ•¸æ“š...")
    df = load_data()
    if df is None:
        print("âŒ ç„¡æ³•åŠ è¼‰æ•¸æ“š")
        return None
    
    print(f"   æ•¸æ“šé‡: {len(df)} ç­†")
    
    # å‰µå»ºç‰¹å¾µ
    print("\nğŸ”§ å‰µå»ºç‰¹å¾µ...")
    df = create_comprehensive_features(df)
    df = df.dropna(subset=['Attendance'])
    
    # ç²å–æ‰€æœ‰ç‰¹å¾µåˆ—
    all_features = get_feature_columns()
    all_features = [col for col in all_features if col in df.columns]
    print(f"   å¯ç”¨ç‰¹å¾µ: {len(all_features)} å€‹")
    
    # æ™‚é–“åºåˆ—åˆ†å‰²
    split_idx = int(len(df) * 0.8)
    train_data = df[:split_idx].copy()
    test_data = df[split_idx:].copy()
    
    X_train = train_data[all_features]
    y_train = train_data['Attendance']
    X_test = test_data[all_features]
    y_test = test_data['Attendance']
    
    print(f"   è¨“ç·´é›†: {len(train_data)} ç­†")
    print(f"   æ¸¬è©¦é›†: {len(test_data)} ç­†")
    
    # é‹è¡Œå„ç¨®é¸æ“‡æ–¹æ³•
    all_results = []
    
    # 1. ç‰¹å¾µé‡è¦æ€§é¸æ“‡
    if quick:
        test_sizes = [10, 20, 25, 30, 50]
    else:
        test_sizes = [5, 10, 15, 20, 25, 30, 40, 50, 75, 100, 130, 160]
    
    imp_results, imp_best, feature_importance = feature_importance_selection(
        X_train, y_train, X_test, y_test, all_features, test_sizes
    )
    all_results.extend(imp_results)
    
    # 2. RFE é¸æ“‡
    if quick:
        rfe_sizes = [20, 25]
    else:
        rfe_sizes = [15, 20, 25, 30, 40]
    
    rfe_results, rfe_best = rfe_selection(
        X_train, y_train, X_test, y_test, all_features, rfe_sizes
    )
    all_results.extend(rfe_results)
    
    # 3. ç›¸é—œæ€§é¸æ“‡
    if quick:
        corr_sizes = [20, 25, 30]
    else:
        corr_sizes = [15, 20, 25, 30, 40, 50]
    
    corr_results, corr_best = correlation_based_selection(
        X_train, y_train, X_test, y_test, all_features, train_data, corr_sizes
    )
    all_results.extend(corr_results)
    
    # 4. æ··åˆé¸æ“‡
    hybrid_results, hybrid_best = hybrid_selection(
        X_train, y_train, X_test, y_test, all_features, train_data, feature_importance
    )
    all_results.extend(hybrid_results)
    
    # æ‰¾å‡ºå…¨å±€æœ€ä½³
    print("\n" + "=" * 70)
    print("ğŸ† å„ªåŒ–çµæœç¸½çµ")
    print("=" * 70)
    
    best_overall = min(all_results, key=lambda x: x['mae'])
    
    print(f"\nğŸ¥‡ æœ€ä½³é…ç½®:")
    print(f"   æ–¹æ³•: {best_overall['method']}")
    print(f"   ç‰¹å¾µæ•¸: {best_overall['n_features']}")
    print(f"   MAE: {best_overall['mae']:.2f}")
    print(f"   MAPE: {best_overall['mape']:.2f}%")
    print(f"   RÂ²: {best_overall['r2']*100:.1f}%")
    
    print(f"\nğŸ“‹ æœ€ä½³ç‰¹å¾µåˆ—è¡¨ ({best_overall['n_features']} å€‹):")
    for i, feat in enumerate(best_overall['features'][:10]):
        print(f"   {i+1:2}. {feat}")
    if len(best_overall['features']) > 10:
        print(f"   ... é‚„æœ‰ {len(best_overall['features'])-10} å€‹ç‰¹å¾µ")
    
    # ä¿å­˜çµæœ
    history = load_optimization_history()
    
    optimization_record = {
        'timestamp': get_hkt_time(),
        'mode': 'quick' if quick else 'full',
        'total_features_tested': len(all_features),
        'best_method': best_overall['method'],
        'best_n_features': best_overall['n_features'],
        'best_mae': best_overall['mae'],
        'best_mape': best_overall['mape'],
        'best_r2': best_overall['r2'],
        'best_features': best_overall['features'],
        'all_results_summary': [
            {
                'method': r['method'],
                'n_features': r['n_features'],
                'mae': r['mae'],
                'r2': r['r2']
            }
            for r in sorted(all_results, key=lambda x: x['mae'])[:10]
        ]
    }
    
    history['optimizations'].append(optimization_record)
    
    # æ›´æ–°æ­·å²æœ€ä½³
    if history['best_ever'] is None or best_overall['mae'] < history['best_ever']['mae']:
        history['best_ever'] = {
            'timestamp': get_hkt_time(),
            'mae': best_overall['mae'],
            'mape': best_overall['mape'],
            'r2': best_overall['r2'],
            'n_features': best_overall['n_features'],
            'method': best_overall['method'],
            'features': best_overall['features']
        }
        print(f"\nğŸ‰ æ–°çš„æ­·å²æœ€ä½³è¨˜éŒ„ï¼")
    else:
        print(f"\nğŸ“Š æ­·å²æœ€ä½³: MAE={history['best_ever']['mae']:.2f} ({history['best_ever']['timestamp']})")
    
    save_optimization_history(history)
    
    # ä¿å­˜æœ€ä½³ç‰¹å¾µé…ç½®
    optimal_config = {
        'version': '2.9.52',
        'updated': get_hkt_time(),
        'optimal_n_features': best_overall['n_features'],
        'optimal_features': best_overall['features'],
        'method': best_overall['method'],
        'metrics': {
            'mae': best_overall['mae'],
            'mape': best_overall['mape'],
            'r2': best_overall['r2']
        },
        'feature_importance': [
            {'feature': f, 'importance': float(i)} 
            for f, i in feature_importance[:50]
        ]
    }
    
    with open(OPTIMAL_FEATURES_FILE, 'w') as f:
        json.dump(optimal_config, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… çµæœå·²ä¿å­˜åˆ°:")
    print(f"   - {OPTIMIZATION_HISTORY_FILE}")
    print(f"   - {OPTIMAL_FEATURES_FILE}")
    print(f"\nâ° å®Œæˆæ™‚é–“: {get_hkt_time()}")
    
    return best_overall


def main():
    parser = argparse.ArgumentParser(description='è‡ªå‹•ç‰¹å¾µå„ªåŒ–å™¨')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿå„ªåŒ–æ¨¡å¼')
    parser.add_argument('--update', action='store_true', help='æŸ¥çœ‹æ­·å²ä¸¦æ›´æ–°')
    args = parser.parse_args()
    
    if args.update:
        history = load_optimization_history()
        if history['best_ever']:
            print("ğŸ“Š æ­·å²æœ€ä½³é…ç½®:")
            print(f"   æ™‚é–“: {history['best_ever']['timestamp']}")
            print(f"   MAE: {history['best_ever']['mae']:.2f}")
            print(f"   æ–¹æ³•: {history['best_ever']['method']}")
            print(f"   ç‰¹å¾µæ•¸: {history['best_ever']['n_features']}")
        print(f"\nğŸ“ˆ å„ªåŒ–æ¬¡æ•¸: {len(history['optimizations'])}")
        if history['optimizations']:
            print("\næœ€è¿‘ 5 æ¬¡å„ªåŒ–:")
            for opt in history['optimizations'][-5:]:
                print(f"   {opt['timestamp']}: MAE={opt['best_mae']:.2f} ({opt['best_method']})")
    else:
        run_optimization(quick=args.quick)


if __name__ == '__main__':
    main()
