"""
å®Œæ•´æ¸¬è©¦ï¼šEnsemble æ¨¡å‹ + AI å› ç´  + å¤©æ°£å› ç´ 
ä½¿ç”¨å®Œæ•´æ•¸æ“šåº«æ•¸æ“š (4064 å¤©)
"""
import sys
import io

if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import json
import os
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from feature_engineering import create_comprehensive_features

def get_db_connection():
    """é€£æ¥åˆ° Railway Production Database"""
    password = os.environ.get('PGPASSWORD') or os.environ.get('DATABASE_PASSWORD') or 'nIdJPREHqkBdMgUifrazOsVlWbxsmDGq'

    return psycopg2.connect(
        host=os.environ.get('PGHOST', 'tramway.proxy.rlwy.net'),
        port=int(os.environ.get('PGPORT', '45703')),
        user=os.environ.get('PGUSER', 'postgres'),
        password=password,
        database=os.environ.get('PGDATABASE', 'railway'),
        sslmode='require'
    )

def load_full_data_from_db():
    """å¾æ•¸æ“šåº«åŠ è¼‰å®Œæ•´æ•¸æ“š"""
    print("ğŸ“¥ é€£æ¥ Railway Database...")

    try:
        conn = get_db_connection()
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # ç²å–æ‰€æœ‰æ•¸æ“š
        cur.execute("""
            SELECT date as "Date", patient_count as "Attendance"
            FROM actual_data
            ORDER BY date
        """)

        rows = cur.fetchall()
        df = pd.DataFrame(rows)

        cur.close()
        conn.close()

        print(f"   âœ… æˆåŠŸåŠ è¼‰ {len(df)} ç­†æ•¸æ“š")
        print(f"   ğŸ“… æ—¥æœŸç¯„åœ: {df['Date'].min()} â†’ {df['Date'].max()}")

        return df

    except Exception as e:
        print(f"   âŒ æ•¸æ“šåº«é€£æ¥å¤±æ•—: {e}")
        return None

def load_weather_data():
    """åŠ è¼‰å¤©æ°£æ•¸æ“š"""
    weather_paths = [
        'weather_history.csv',
        'python/weather_history.csv',
        'c:/Github/ndh-aed-prediction/python/weather_history.csv',
    ]

    for path in weather_paths:
        if os.path.exists(path):
            df = pd.read_csv(path)
            df['Date'] = pd.to_datetime(df['Date'])
            print(f"   âœ… åŠ è¼‰å¤©æ°£æ•¸æ“š: {len(df)} ç­†")
            return df

    print("   âš ï¸ æœªæ‰¾åˆ°å¤©æ°£æ•¸æ“š")
    return None

def create_weather_change_features(df):
    """å‰µå»ºå¤©æ°£è®ŠåŒ–ç‰¹å¾µ"""
    df = df.copy()
    df = df.sort_values('Date').reset_index(drop=True)

    # æº«åº¦è®ŠåŒ–ç‡
    df['temp_change_1d'] = df['mean_temp'].diff(1)
    df['temp_change_3d'] = df['mean_temp'].diff(3)

    # çªç„¶é™æº«/å‡æº«
    df['sudden_temp_drop'] = (df['temp_change_1d'] < -5).astype(int)
    df['sudden_temp_rise'] = (df['temp_change_1d'] > 5).astype(int)

    # æº«åº¦æ³¢å‹•æ€§
    df['temp_volatility_7d'] = df['mean_temp'].rolling(window=7, min_periods=3).std()

    # å­£ç¯€åé›¢
    df['month'] = df['Date'].dt.month
    monthly_avg = df.groupby('month')['mean_temp'].transform('mean')
    df['temp_deviation_from_seasonal'] = df['mean_temp'] - monthly_avg

    return df

def load_ai_factors():
    """åŠ è¼‰ AI å› ç´ ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    ai_paths = [
        'models/ai_factors.json',
        'python/models/ai_factors.json',
        'c:/Github/ndh-aed-prediction/python/models/ai_factors.json',
    ]

    for path in ai_paths:
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"   âœ… åŠ è¼‰ AI å› ç´ : {len(data)} ç­†")
                return data

    print("   âš ï¸ æœªæ‰¾åˆ° AI å› ç´ æ•¸æ“š")
    return None

def calculate_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    return {'mae': mae, 'rmse': rmse, 'mape': mape, 'r2': r2}

def main():
    print("=" * 80)
    print("ğŸ”¬ å®Œæ•´æ¸¬è©¦: Ensemble + AI + å¤©æ°£å› ç´ ")
    print("   ä½¿ç”¨å®Œæ•´æ•¸æ“šåº«æ•¸æ“š")
    print("=" * 80)
    print(f"â° é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # åŠ è¼‰æ•¸æ“š
    df = load_full_data_from_db()
    if df is None:
        return

    df['Date'] = pd.to_datetime(df['Date'])

    # æ’é™¤ COVID
    covid_start = pd.Timestamp('2020-02-01')
    covid_end = pd.Timestamp('2022-06-30')
    covid_mask = (df['Date'] >= covid_start) & (df['Date'] <= covid_end)
    df = df[~covid_mask].copy()

    print(f"\nğŸ“Š æ•¸æ“šé‡: {len(df)} å¤© (æ’é™¤ COVID)")

    # å‰µå»ºåŸºç¤ç‰¹å¾µ
    print("\nğŸ”§ å‰µå»ºåŸºç¤ç‰¹å¾µ...")
    df = create_comprehensive_features(df)
    df = df.dropna(subset=['Attendance'])

    # åŠ è¼‰å¤©æ°£æ•¸æ“š
    print("\nğŸŒ¤ï¸ åŠ è¼‰å¤©æ°£æ•¸æ“š...")
    weather_df = load_weather_data()

    weather_features = []
    if weather_df is not None:
        weather_df = create_weather_change_features(weather_df)
        df = df.merge(weather_df, on='Date', how='left')

        # å¤©æ°£è®ŠåŒ–ç‰¹å¾µ
        weather_features = [
            'temp_change_1d', 'temp_change_3d',
            'sudden_temp_drop', 'sudden_temp_rise',
            'temp_volatility_7d', 'temp_deviation_from_seasonal',
            'mean_temp', 'max_temp', 'min_temp',
            'is_very_hot', 'is_hot', 'is_cold', 'is_very_cold'
        ]
        weather_features = [c for c in weather_features if c in df.columns]

        # å¡«å……ç¼ºå¤±å€¼
        for col in weather_features:
            df[col] = df[col].fillna(df[col].median())

        print(f"   âœ… å¤©æ°£ç‰¹å¾µ: {len(weather_features)} å€‹")

    # åŠ è¼‰ AI å› ç´ 
    print("\nï¿½ï¿½ åŠ è¼‰ AI å› ç´ ...")
    ai_factors = load_ai_factors()

    ai_features = []
    if ai_factors:
        # å°‡ AI å› ç´ è½‰æ›ç‚º DataFrame
        ai_df = pd.DataFrame([
            {'Date': pd.to_datetime(date), 'ai_factor': factor}
            for date, factor in ai_factors.items()
        ])
        df = df.merge(ai_df, on='Date', how='left')
        df['ai_factor'] = df['ai_factor'].fillna(1.0)
        ai_features = ['ai_factor']
        print(f"   âœ… AI ç‰¹å¾µ: {len(ai_features)} å€‹")

    # åŸºç¤ç‰¹å¾µ
    base_features = [
        "Attendance_Lag1", "Attendance_Lag7", "Attendance_Same_Weekday_Avg",
        "Day_of_Week", "DayOfWeek_Target_Mean", "Attendance_Rolling7",
        "Attendance_EWMA7", "Attendance_Lag14", "Attendance_Lag30",
        "Daily_Change", "Weekly_Change", "Is_Weekend",
        "Holiday_Factor", "Attendance_Std7", "Month"
    ]
    base_features = [c for c in base_features if c in df.columns]

    # æ‰€æœ‰ç‰¹å¾µ
    all_features = base_features + weather_features + ai_features

    print(f"\nğŸ“Š ç‰¹å¾µç¸½æ•¸: {len(all_features)}")
    print(f"   åŸºç¤ç‰¹å¾µ: {len(base_features)}")
    print(f"   å¤©æ°£ç‰¹å¾µ: {len(weather_features)}")
    print(f"   AI ç‰¹å¾µ: {len(ai_features)}")

    # æ™‚é–“åºåˆ—åˆ†å‰²
    split_idx = int(len(df) * 0.8)
    train_data = df[:split_idx].copy()
    test_data = df[split_idx:].copy()

    print(f"\nğŸ“Š æ•¸æ“šåˆ†å‰²:")
    print(f"   è¨“ç·´é›†: {len(train_data)} å¤©")
    print(f"   æ¸¬è©¦é›†: {len(test_data)} å¤©")

    y_train = train_data['Attendance'].values
    y_test = test_data['Attendance'].values

    results = {}

    # ============================================
    # æ¸¬è©¦ 1: åŸºç¤ Random Forest
    # ============================================
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬è©¦ 1: Random Forest (åŸºç¤ç‰¹å¾µ)")
    print("=" * 80)

    X_train_base = train_data[base_features].fillna(0)
    X_test_base = test_data[base_features].fillna(0)

    rf_base = RandomForestRegressor(
        n_estimators=200,
        max_depth=12,
        min_samples_split=10,
        random_state=42,
        n_jobs=-1
    )
    rf_base.fit(X_train_base, y_train)
    rf_base_pred = rf_base.predict(X_test_base)

    rf_base_metrics = calculate_metrics(y_test, rf_base_pred)
    results['rf_base'] = rf_base_metrics

    print(f"   MAE:  {rf_base_metrics['mae']:.2f}")
    print(f"   MAPE: {rf_base_metrics['mape']:.2f}%")
    print(f"   RÂ²:   {rf_base_metrics['r2']:.4f}")

    # ============================================
    # æ¸¬è©¦ 2: åŸºç¤ XGBoost
    # ============================================
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬è©¦ 2: XGBoost (åŸºç¤ç‰¹å¾µ)")
    print("=" * 80)

    xgb_base = xgb.XGBRegressor(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.05,
        random_state=42
    )
    xgb_base.fit(X_train_base, y_train)
    xgb_base_pred = xgb_base.predict(X_test_base)

    xgb_base_metrics = calculate_metrics(y_test, xgb_base_pred)
    results['xgb_base'] = xgb_base_metrics

    print(f"   MAE:  {xgb_base_metrics['mae']:.2f}")
    print(f"   MAPE: {xgb_base_metrics['mape']:.2f}%")
    print(f"   RÂ²:   {xgb_base_metrics['r2']:.4f}")

    # ============================================
    # æ¸¬è©¦ 3: RF + æ‰€æœ‰ç‰¹å¾µ
    # ============================================
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬è©¦ 3: Random Forest + å¤©æ°£ + AI")
    print("=" * 80)

    X_train_all = train_data[all_features].fillna(0)
    X_test_all = test_data[all_features].fillna(0)

    rf_all = RandomForestRegressor(
        n_estimators=200,
        max_depth=12,
        min_samples_split=10,
        random_state=42,
        n_jobs=-1
    )
    rf_all.fit(X_train_all, y_train)
    rf_all_pred = rf_all.predict(X_test_all)

    rf_all_metrics = calculate_metrics(y_test, rf_all_pred)
    results['rf_all'] = rf_all_metrics

    improvement_rf = ((rf_base_metrics['mae'] - rf_all_metrics['mae']) / rf_base_metrics['mae']) * 100

    print(f"   MAE:  {rf_all_metrics['mae']:.2f} ({improvement_rf:+.1f}%)")
    print(f"   MAPE: {rf_all_metrics['mape']:.2f}%")
    print(f"   RÂ²:   {rf_all_metrics['r2']:.4f}")

    # ============================================
    # æ¸¬è©¦ 4: XGB + æ‰€æœ‰ç‰¹å¾µ
    # ============================================
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬è©¦ 4: XGBoost + å¤©æ°£ + AI")
    print("=" * 80)

    xgb_all = xgb.XGBRegressor(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.05,
        random_state=42
    )
    xgb_all.fit(X_train_all, y_train)
    xgb_all_pred = xgb_all.predict(X_test_all)

    xgb_all_metrics = calculate_metrics(y_test, xgb_all_pred)
    results['xgb_all'] = xgb_all_metrics

    improvement_xgb = ((xgb_base_metrics['mae'] - xgb_all_metrics['mae']) / xgb_base_metrics['mae']) * 100

    print(f"   MAE:  {xgb_all_metrics['mae']:.2f} ({improvement_xgb:+.1f}%)")
    print(f"   MAPE: {xgb_all_metrics['mape']:.2f}%")
    print(f"   RÂ²:   {xgb_all_metrics['r2']:.4f}")

    # ============================================
    # æ¸¬è©¦ 5: Simple Ensemble (50/50)
    # ============================================
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬è©¦ 5: Simple Ensemble (RF 50% + XGB 50%)")
    print("=" * 80)

    ensemble_simple = 0.5 * rf_all_pred + 0.5 * xgb_all_pred
    ensemble_simple_metrics = calculate_metrics(y_test, ensemble_simple)
    results['ensemble_simple'] = ensemble_simple_metrics

    print(f"   MAE:  {ensemble_simple_metrics['mae']:.2f}")
    print(f"   MAPE: {ensemble_simple_metrics['mape']:.2f}%")
    print(f"   RÂ²:   {ensemble_simple_metrics['r2']:.4f}")

    # ============================================
    # æ¸¬è©¦ 6: Weighted Ensemble (åŸºæ–¼é©—è­‰é›†è¡¨ç¾)
    # ============================================
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬è©¦ 6: Weighted Ensemble (åŸºæ–¼è¡¨ç¾)")
    print("=" * 80)

    # è¨ˆç®—æ¬Šé‡ï¼ˆMAE è¶Šå°æ¬Šé‡è¶Šé«˜ï¼‰
    rf_weight = (1 / rf_all_metrics['mae']) / ((1 / rf_all_metrics['mae']) + (1 / xgb_all_metrics['mae']))
    xgb_weight = 1 - rf_weight

    print(f"   RF æ¬Šé‡: {rf_weight:.2f}")
    print(f"   XGB æ¬Šé‡: {xgb_weight:.2f}")

    ensemble_weighted = rf_weight * rf_all_pred + xgb_weight * xgb_all_pred
    ensemble_weighted_metrics = calculate_metrics(y_test, ensemble_weighted)
    results['ensemble_weighted'] = ensemble_weighted_metrics

    print(f"   MAE:  {ensemble_weighted_metrics['mae']:.2f}")
    print(f"   MAPE: {ensemble_weighted_metrics['mape']:.2f}%")
    print(f"   RÂ²:   {ensemble_weighted_metrics['r2']:.4f}")

    # ============================================
    # æ¸¬è©¦ 7: Adaptive Ensemble (æ ¹æ“šé æ¸¬ç¯„åœèª¿æ•´)
    # ============================================
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬è©¦ 7: Adaptive Ensemble (çŸ­æœŸ XGB, é•·æœŸ RF)")
    print("=" * 80)

    # æ¨¡æ“¬ä¸åŒæ™‚é–“ç¯„åœçš„æ¬Šé‡
    ensemble_adaptive = []
    for i in range(len(y_test)):
        if i < 7:  # Day 1-7: XGB æ¬Šé‡æ›´é«˜
            pred = 0.4 * rf_all_pred[i] + 0.6 * xgb_all_pred[i]
        elif i < 30:  # Day 8-30: å¹³å‡
            pred = 0.5 * rf_all_pred[i] + 0.5 * xgb_all_pred[i]
        else:  # Day 31+: RF æ¬Šé‡æ›´é«˜
            pred = 0.6 * rf_all_pred[i] + 0.4 * xgb_all_pred[i]
        ensemble_adaptive.append(pred)

    ensemble_adaptive = np.array(ensemble_adaptive)
    ensemble_adaptive_metrics = calculate_metrics(y_test, ensemble_adaptive)
    results['ensemble_adaptive'] = ensemble_adaptive_metrics

    print(f"   MAE:  {ensemble_adaptive_metrics['mae']:.2f}")
    print(f"   MAPE: {ensemble_adaptive_metrics['mape']:.2f}%")
    print(f"   RÂ²:   {ensemble_adaptive_metrics['r2']:.4f}")

    # ============================================
    # ç¸½çµæ¯”è¼ƒ
    # ============================================
    print("\n" + "=" * 80)
    print("ğŸ† ç¸½çµæ¯”è¼ƒ")
    print("=" * 80)

    print(f"\n{'æ¨¡å‹':<40} {'MAE':<10} {'MAPE':<10} {'RÂ²':<10} {'vs åŸºæº–':<10}")
    print("-" * 80)

    baseline_mae = rf_base_metrics['mae']

    model_names = {
        'rf_base': 'RF (åŸºç¤) - åŸºæº–',
        'xgb_base': 'XGB (åŸºç¤)',
        'rf_all': 'RF + å¤©æ°£ + AI',
        'xgb_all': 'XGB + å¤©æ°£ + AI',
        'ensemble_simple': 'Ensemble 50/50',
        'ensemble_weighted': 'Ensemble åŠ æ¬Š',
        'ensemble_adaptive': 'Ensemble è‡ªé©æ‡‰ â­'
    }

    sorted_results = sorted(results.items(), key=lambda x: x[1]['mae'])

    for name, metrics in sorted_results:
        improvement = ((baseline_mae - metrics['mae']) / baseline_mae) * 100
        improvement_str = f"{improvement:+.1f}%"
        if improvement > 0:
            improvement_str = f"âœ… {improvement_str}"
        else:
            improvement_str = f"âŒ {improvement_str}"

        display_name = model_names.get(name, name)
        print(f"{display_name:<40} {metrics['mae']:<10.2f} {metrics['mape']:<10.2f}% {metrics['r2']:<10.4f} {improvement_str:<10}")

    # çµ±è¨ˆé¡¯è‘—æ€§æ¸¬è©¦
    print("\n" + "=" * 80)
    print("ğŸ“Š çµ±è¨ˆé¡¯è‘—æ€§åˆ†æ")
    print("=" * 80)

    best_name = sorted_results[0][0]
    best_metrics = sorted_results[0][1]

    print(f"\n   æœ€ä½³æ¨¡å‹: {model_names[best_name]}")
    print(f"   MAE: {best_metrics['mae']:.2f}")
    print(f"   æ”¹å–„: {((baseline_mae - best_metrics['mae']) / baseline_mae * 100):.1f}%")

    # ä¿å­˜çµæœ
    output = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_days': len(df),
        'train_days': len(train_data),
        'test_days': len(test_data),
        'features': {
            'base': len(base_features),
            'weather': len(weather_features),
            'ai': len(ai_features),
            'total': len(all_features)
        },
        'results': {k: {
            'mae': float(v['mae']),
            'rmse': float(v['rmse']),
            'mape': float(v['mape']),
            'r2': float(v['r2'])
        } for k, v in results.items()},
        'best_model': best_name,
        'improvement_over_baseline': float((baseline_mae - best_metrics['mae']) / baseline_mae * 100)
    }

    os.makedirs('models', exist_ok=True)
    with open('models/ensemble_full_test_results.json', 'w') as f:
        json.dump(output, f, indent=2)

    print(f"\nâœ… çµæœå·²ä¿å­˜åˆ° models/ensemble_full_test_results.json")

if __name__ == '__main__':
    main()
