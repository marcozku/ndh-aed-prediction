"""
XGBoost æ¨¡å‹è¨“ç·´è…³æœ¬ v3.0.98
æ ¹æ“š AI-AED-Algorithm-Specification.txt Section 6.1
æ–°å¢: Optuna è¶…åƒæ•¸å„ªåŒ–ã€ç‰¹å¾µé¸æ“‡å„ªåŒ–ï¼ˆ25ç‰¹å¾µï¼‰ã€RÂ² æŒ‡æ¨™
v3.0.81: è¨“ç·´å‰è‡ªå‹•æ›´æ–°å‹•æ…‹ factorsï¼ˆå¾ Railway Databaseï¼‰
v3.0.98: COVID æœŸé–“æ’é™¤æ³•å–ä»£ Sliding Windowï¼ˆåŸºæ–¼å¯¦é©—è­‰æ“šï¼‰
         - ä½¿ç”¨å…¨éƒ¨ 11 å¹´æ•¸æ“š + æ’é™¤ COVID æœŸé–“ (2020-02 to 2022-06)
         - MAE å¾ 19.66 é™è‡³ 16.52 (æ”¹å–„ 16%)
         - ç ”ç©¶åŸºç¤: Gama et al. (2014), Tukey (1977)
"""
import sys
import io

# Fix Windows encoding for emoji/unicode output
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score as sklearn_r2_score
import json
import os
import datetime
import time
import subprocess
try:
    from zoneinfo import ZoneInfo
except ImportError:
    from backports.zoneinfo import ZoneInfo
from feature_engineering import create_comprehensive_features, get_feature_columns

# å˜—è©¦å°å…¥ Optunaï¼ˆå¯é¸ï¼‰
try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("â„¹ï¸ Optuna æœªå®‰è£ï¼Œå°‡ä½¿ç”¨é è¨­è¶…åƒæ•¸")

# HKT æ™‚å€
HKT = ZoneInfo('Asia/Hong_Kong')

def update_dynamic_factors():
    """
    è¨“ç·´å‰æ›´æ–°å‹•æ…‹ factorsï¼ˆå¾ Railway Databaseï¼‰
    ç¢ºä¿ä½¿ç”¨æœ€æ–°çš„çœŸå¯¦æ•¸æ“š
    """
    print("\n" + "=" * 80)
    print("STEP 0: Updating Dynamic Factors from Railway Database")
    print("=" * 80)
    
    try:
        script_path = os.path.join(os.path.dirname(__file__), 'calculate_dynamic_factors.py')
        result = subprocess.run(
            [sys.executable, script_path],
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode == 0:
            print(result.stdout)
            print("âœ… Dynamic factors updated successfully")
            return True
        else:
            print(f"âš ï¸ Warning: Could not update dynamic factors")
            print(f"Error: {result.stderr}")
            print("Continuing with existing factors...")
            return False
    except Exception as e:
        print(f"âš ï¸ Warning: Error updating dynamic factors: {e}")
        print("Continuing with existing factors...")
        return False

def load_ai_factors_from_db(conn):
    """å¾æ•¸æ“šåº«åŠ è¼‰ AI å› å­æ•¸æ“š"""
    try:
        import sqlalchemy
        # ä½¿ç”¨ SQLAlchemy å‰µå»ºé€£æ¥ä»¥é¿å…è­¦å‘Š
        from sqlalchemy import create_engine
        # å¾ psycopg2 é€£æ¥ç²å–é€£æ¥å­—ç¬¦ä¸²
        dsn = conn.get_dsn_parameters()
        connection_string = f"postgresql://{dsn.get('user')}:{dsn.get('password', '')}@{dsn.get('host')}:{dsn.get('port', 5432)}/{dsn.get('dbname')}"
        engine = create_engine(connection_string)
        
        query = """
            SELECT factors_cache
            FROM ai_factors_cache
            WHERE id = 1
        """
        result = pd.read_sql_query(query, engine)
        if len(result) > 0 and result.iloc[0]['factors_cache'] is not None:
            import json
            factors_cache = result.iloc[0]['factors_cache']
            if isinstance(factors_cache, str):
                factors_cache = json.loads(factors_cache)
            elif isinstance(factors_cache, dict):
                pass  # å·²ç¶“æ˜¯å­—å…¸
            else:
                factors_cache = {}
            return factors_cache
        return {}
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•åŠ è¼‰ AI å› å­æ•¸æ“š: {e}")
        return {}

def load_data_from_db():
    """å¾æ•¸æ“šåº«åŠ è¼‰æ•¸æ“šï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    try:
        import psycopg2
        from dotenv import load_dotenv
        load_dotenv()
        
        # ä½¿ç”¨ç’°å¢ƒè®Šæ•¸æˆ– Railway é»˜èªå€¼
        password = os.getenv('PGPASSWORD') or os.getenv('DATABASE_PASSWORD') or 'nIdJPREHqkBdMgUifrazOsVlWbxsmDGq'
        host = os.getenv('PGHOST') or 'tramway.proxy.rlwy.net'
        port = int(os.getenv('PGPORT') or '45703')
        user = os.getenv('PGUSER') or 'postgres'
        database = os.getenv('PGDATABASE') or 'railway'
        
        print(f"   ğŸ“¡ é€£æ¥è³‡æ–™åº«: {host}:{port}/{database}")
        
        conn = psycopg2.connect(
            host=host,
            port=port,
            user=user,
            password=password,
            database=database,
            sslmode='require'
        )
        
        # ä½¿ç”¨ SQLAlchemy å‰µå»ºé€£æ¥ï¼ˆç›´æ¥ä½¿ç”¨å·²çŸ¥çš„é€£æ¥åƒæ•¸ï¼‰
        from sqlalchemy import create_engine
        from urllib.parse import quote_plus
        # ä½¿ç”¨ quote_plus ç¢ºä¿å¯†ç¢¼ä¸­çš„ç‰¹æ®Šå­—ç¬¦è¢«æ­£ç¢ºç·¨ç¢¼
        connection_string = f"postgresql://{user}:{quote_plus(password)}@{host}:{port}/{database}?sslmode=require"
        engine = create_engine(connection_string)
        
        query = """
            SELECT date as Date, patient_count as Attendance
            FROM actual_data
            ORDER BY date ASC
        """
        df = pd.read_sql_query(query, engine)
        
        # åŠ è¼‰ AI å› å­æ•¸æ“šï¼ˆä½¿ç”¨åŸå§‹é€£æ¥ï¼Œå› ç‚º load_ai_factors_from_db æœƒå‰µå»ºè‡ªå·±çš„ engineï¼‰
        ai_factors = load_ai_factors_from_db(conn)
        if ai_factors:
            print(f"âœ… åŠ è¼‰äº† {len(ai_factors)} å€‹æ—¥æœŸçš„ AI å› å­æ•¸æ“š")
        else:
            print("â„¹ï¸ æ²’æœ‰æ‰¾åˆ° AI å› å­æ•¸æ“šï¼Œå°‡ä½¿ç”¨é»˜èªå€¼")
        
        conn.close()
        
        # ç¢ºä¿åˆ—åæ­£ç¢ºï¼ˆpandas å¯èƒ½æœƒå°‡åˆ—åè½‰ç‚ºå°å¯«ï¼‰
        # æª¢æŸ¥ä¸¦æ˜ å°„ Date åˆ—
        if 'date' in df.columns and 'Date' not in df.columns:
            df['Date'] = df['date']
            df = df.drop(columns=['date'])
        elif 'Date' not in df.columns:
            print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° Date åˆ—ã€‚å¯ç”¨åˆ—: {df.columns.tolist()}")
            return None
        
        # æª¢æŸ¥ä¸¦æ˜ å°„ Attendance åˆ—ï¼ˆå¯èƒ½æ˜¯ attendance æˆ– patient_countï¼‰
        if 'attendance' in df.columns and 'Attendance' not in df.columns:
            df['Attendance'] = df['attendance']
            df = df.drop(columns=['attendance'])
        elif 'patient_count' in df.columns and 'Attendance' not in df.columns:
            df['Attendance'] = df['patient_count']
            df = df.drop(columns=['patient_count'])
        elif 'Attendance' not in df.columns:
            print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° Attendance åˆ—ã€‚å¯ç”¨åˆ—: {df.columns.tolist()}")
            return None
        
        # ç¢ºä¿åªè¿”å›éœ€è¦çš„åˆ—å’Œ AI å› å­
        if 'Date' in df.columns and 'Attendance' in df.columns:
            # å°‡ AI å› å­é™„åŠ åˆ° DataFrameï¼ˆä½œç‚ºå…ƒæ•¸æ“šï¼Œç¨å¾Œåœ¨ç‰¹å¾µå·¥ç¨‹ä¸­ä½¿ç”¨ï¼‰
            df_with_ai = df[['Date', 'Attendance']].copy()
            df_with_ai.attrs['ai_factors'] = ai_factors
            return df_with_ai
        else:
            print(f"è­¦å‘Š: æ•¸æ“šåˆ—ä¸å®Œæ•´ã€‚å¯ç”¨åˆ—: {df.columns.tolist()}")
            df.attrs['ai_factors'] = ai_factors
            return df
    except Exception as e:
        print(f"ç„¡æ³•å¾æ•¸æ“šåº«åŠ è¼‰æ•¸æ“š: {e}")
        return None

def load_data_from_csv(csv_path):
    """å¾ CSV æ–‡ä»¶åŠ è¼‰æ•¸æ“š"""
    try:
        df = pd.read_csv(csv_path)
        # è™•ç†ä¸åŒçš„åˆ—åæ ¼å¼
        if 'Date' not in df.columns:
            if 'date' in df.columns:
                df['Date'] = df['date']
            elif 'Date' in df.columns:
                df['Date'] = df['Date']
        if 'Attendance' not in df.columns:
            if 'patient_count' in df.columns:
                df['Attendance'] = df['patient_count']
            elif 'Attendance' in df.columns:
                df['Attendance'] = df['Attendance']
        
        return df[['Date', 'Attendance']]
    except Exception as e:
        print(f"ç„¡æ³•å¾ CSV åŠ è¼‰æ•¸æ“š: {e}")
        return None

def load_old_metrics_from_db():
    """å¾æ•¸æ“šåº«åŠ è¼‰ä¸Šæ¬¡è¨“ç·´çš„æ¨¡å‹æŒ‡æ¨™ï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰"""
    try:
        import psycopg2
        from dotenv import load_dotenv
        load_dotenv()
        
        conn = psycopg2.connect(
            host=os.getenv('PGHOST') or os.getenv('DATABASE_URL', '').split('@')[1].split('/')[0] if '@' in os.getenv('DATABASE_URL', '') else None,
            database=os.getenv('PGDATABASE') or os.getenv('DATABASE_URL', '').split('/')[-1] if '/' in os.getenv('DATABASE_URL', '') else None,
            user=os.getenv('PGUSER') or os.getenv('DATABASE_URL', '').split('://')[1].split(':')[0] if '://' in os.getenv('DATABASE_URL', '') else None,
            password=os.getenv('PGPASSWORD') or os.getenv('DATABASE_URL', '').split('@')[0].split(':')[-1] if '@' in os.getenv('DATABASE_URL', '') else None,
        )
        
        cursor = conn.cursor()
        cursor.execute("""
            SELECT mae, rmse, mape, training_date, data_count
            FROM model_metrics 
            WHERE model_name = 'xgboost'
            LIMIT 1
        """)
        
        row = cursor.fetchone()
        conn.close()
        
        if row and row[0] is not None:
            return {
                'mae': float(row[0]) if row[0] else None,
                'rmse': float(row[1]) if row[1] else None,
                'mape': float(row[2]) if row[2] else None,
                'training_date': str(row[3]) if row[3] else None,
                'data_count': int(row[4]) if row[4] else None
            }
        return None
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•å¾æ•¸æ“šåº«åŠ è¼‰èˆŠæ¨¡å‹æŒ‡æ¨™: {e}")
        return None

def optuna_optimize(X_train, y_train, X_val, y_val, n_trials=50):
    """
    ä½¿ç”¨ Optuna é€²è¡Œè¶…åƒæ•¸å„ªåŒ–
    
    åƒæ•¸:
        X_train, y_train: è¨“ç·´æ•¸æ“š
        X_val, y_val: é©—è­‰æ•¸æ“š
        n_trials: å„ªåŒ–è©¦é©—æ¬¡æ•¸
    
    è¿”å›:
        æœ€ä½³è¶…åƒæ•¸å­—å…¸
    """
    if not OPTUNA_AVAILABLE:
        print("âš ï¸ Optuna æœªå®‰è£ï¼Œä½¿ç”¨é è¨­åƒæ•¸")
        return None
    
    print(f"\n{'='*60}")
    print("ğŸ” Optuna è¶…åƒæ•¸å„ªåŒ– (TPE Sampler)")
    print(f"{'='*60}")
    print(f"   è©¦é©—æ¬¡æ•¸: {n_trials}")
    print(f"   è¨“ç·´é›†å¤§å°: {len(X_train)}")
    print(f"   é©—è­‰é›†å¤§å°: {len(X_val)}")
    
    def objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 200, 800),
            'max_depth': trial.suggest_int('max_depth', 4, 12),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'subsample': trial.suggest_float('subsample', 0.6, 0.95),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),
            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 0.95),
            'gamma': trial.suggest_float('gamma', 0, 1.0),
            'alpha': trial.suggest_float('alpha', 0, 2.0),
            'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 3.0),
        }
        
        model = xgb.XGBRegressor(
            **params,
            objective='reg:squarederror',
            tree_method='hist',
            random_state=42,
            n_jobs=-1,
            early_stopping_rounds=30,
            eval_metric='mae'
        )
        
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
        
        y_pred = model.predict(X_val)
        mae = mean_absolute_error(y_val, y_pred)
        
        return mae
    
    # å‰µå»º Optuna ç ”ç©¶
    sampler = TPESampler(seed=42)
    study = optuna.create_study(direction='minimize', sampler=sampler)
    
    # éœéŸ³ Optuna æ—¥èªŒ
    optuna.logging.set_verbosity(optuna.logging.WARNING)
    
    # é‹è¡Œå„ªåŒ–
    start_time = time.time()
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
    opt_time = time.time() - start_time
    
    print(f"\nâœ… å„ªåŒ–å®Œæˆ!")
    print(f"   â±ï¸ è€—æ™‚: {opt_time:.1f} ç§’")
    print(f"   ğŸ† æœ€ä½³ MAE: {study.best_value:.2f}")
    print(f"\n   ğŸ“‹ æœ€ä½³è¶…åƒæ•¸:")
    for key, value in study.best_params.items():
        if isinstance(value, float):
            print(f"      {key}: {value:.4f}")
        else:
            print(f"      {key}: {value}")
    
    return study.best_params


def time_series_cross_validate(df, feature_cols, n_splits=3):
    """
    æ™‚é–“åºåˆ—äº¤å‰é©—è­‰ (Walk-Forward Validation) - å„ªåŒ–ç‰ˆ v2.9.21
    
    ç¢ºä¿æ¨¡å‹åœ¨è¨“ç·´æœŸé–“æ°¸é ä¸æœƒçœ‹åˆ°æœªä¾†æ•¸æ“šï¼š
    - æ¯å€‹ fold åªä½¿ç”¨éå»çš„æ•¸æ“šé€²è¡Œè¨“ç·´
    - é©—è­‰é›†ç¸½æ˜¯åœ¨è¨“ç·´é›†ä¹‹å¾Œçš„æ™‚é–“æ®µ
    - æœ€çµ‚æ¸¬è©¦é›†å®Œå…¨ç¨ç«‹ï¼Œå¾æœªåƒèˆ‡ä»»ä½•è¨“ç·´éç¨‹
    
    å„ªåŒ–ï¼šä½¿ç”¨ 3-fold å’Œ 100 æ£µæ¨¹ï¼ˆè€Œé 5-fold å’Œ 300 æ£µæ¨¹ï¼‰ä»¥åŠ é€Ÿè¨“ç·´
    """
    print(f"\n{'='*60}")
    print("ğŸ”„ æ™‚é–“åºåˆ—äº¤å‰é©—è­‰ (Walk-Forward Validation) - å¿«é€Ÿæ¨¡å¼")
    print(f"{'='*60}")
    print(f"âš ï¸ é‡è¦ï¼šç¢ºä¿æ¨¡å‹ç„¡æ³•è¨ªå•æœªä¾†æ•¸æ“šï¼")
    print(f"ğŸ“Š äº¤å‰é©—è­‰æŠ˜æ•¸: {n_splits}")
    
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    cv_scores = {'mae': [], 'rmse': [], 'mape': []}
    
    # XGBoost åŸç”Ÿæ”¯æŒ NaN è™•ç†ï¼Œä¸éœ€è¦å¡«å……
    X = df[feature_cols]
    y = df['Attendance']
    dates = df['Date'].values
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        # ç²å–è¨“ç·´å’Œé©—è­‰çš„æ—¥æœŸç¯„åœ
        train_dates = dates[train_idx]
        val_dates = dates[val_idx]
        
        # é©—è­‰ï¼šç¢ºä¿é©—è­‰é›†æ—¥æœŸéƒ½åœ¨è¨“ç·´é›†æ—¥æœŸä¹‹å¾Œ
        train_max = pd.to_datetime(train_dates).max()
        val_min = pd.to_datetime(val_dates).min()
        
        if val_min <= train_max:
            print(f"âŒ Fold {fold+1}: æ•¸æ“šæ´©æ¼ï¼é©—è­‰é›†åŒ…å«è¨“ç·´æœŸé–“çš„æ•¸æ“š")
            continue
        
        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]
        
        print(f"\nğŸ“‚ Fold {fold+1}/{n_splits}:")
        print(f"   è¨“ç·´é›†: {len(train_idx)} ç­† ({train_dates[0]} è‡³ {train_dates[-1]})")
        print(f"   é©—è­‰é›†: {len(val_idx)} ç­† ({val_dates[0]} è‡³ {val_dates[-1]})")
        print(f"   âœ… æ™‚é–“é †åºé©—è­‰é€šéï¼šé©—è­‰é›†é–‹å§‹æ—¥æœŸ > è¨“ç·´é›†çµæŸæ—¥æœŸ")
        
        # å‰µå»ºæ¨¡å‹ - ä½¿ç”¨è¼ƒå°‘æ¨¹æ•¸åŠ é€Ÿ CVï¼ˆv2.9.21 å„ªåŒ–ï¼‰
        model = xgb.XGBRegressor(
            n_estimators=100,  # æ¸›å°‘åˆ° 100 æ£µæ¨¹ï¼ˆåŸ 300ï¼‰
            max_depth=6,
            learning_rate=0.1,  # æé«˜å­¸ç¿’ç‡ä»¥è£œå„Ÿè¼ƒå°‘æ¨¹æ•¸
            subsample=0.8,
            colsample_bytree=0.8,
            objective='reg:squarederror',
            alpha=1.0,
            reg_lambda=1.0,
            tree_method='hist',  # ä½¿ç”¨ histogram åŠ é€Ÿ
            random_state=42,
            n_jobs=-1
        )
        
        model.fit(X_train_cv, y_train_cv, verbose=False)
        
        y_pred_cv = model.predict(X_val_cv)
        
        mae = mean_absolute_error(y_val_cv, y_pred_cv)
        rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred_cv))
        mape = np.mean(np.abs((y_val_cv - y_pred_cv) / y_val_cv)) * 100
        
        cv_scores['mae'].append(mae)
        cv_scores['rmse'].append(rmse)
        cv_scores['mape'].append(mape)
        
        print(f"   ğŸ“ˆ MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
    
    # è¨ˆç®—å¹³å‡åˆ†æ•¸
    avg_scores = {
        'cv_mae_mean': np.mean(cv_scores['mae']),
        'cv_mae_std': np.std(cv_scores['mae']),
        'cv_rmse_mean': np.mean(cv_scores['rmse']),
        'cv_rmse_std': np.std(cv_scores['rmse']),
        'cv_mape_mean': np.mean(cv_scores['mape']),
        'cv_mape_std': np.std(cv_scores['mape']),
    }
    
    print(f"\n{'='*60}")
    print("ğŸ“Š äº¤å‰é©—è­‰çµæœç¸½çµ:")
    print(f"{'='*60}")
    print(f"   MAE:  {avg_scores['cv_mae_mean']:.2f} Â± {avg_scores['cv_mae_std']:.2f} ç—…äºº")
    print(f"   RMSE: {avg_scores['cv_rmse_mean']:.2f} Â± {avg_scores['cv_rmse_std']:.2f} ç—…äºº")
    print(f"   MAPE: {avg_scores['cv_mape_mean']:.2f} Â± {avg_scores['cv_mape_std']:.2f}%")
    
    return avg_scores


def train_xgboost_model(train_data, test_data, feature_cols, sample_weights=None):
    """
    è¨“ç·´ XGBoost æ¨¡å‹ï¼ˆä½¿ç”¨æ­£ç¢ºçš„æ™‚é–“åºåˆ—é©—è­‰ï¼‰
    
    é—œéµï¼šEarly stopping ä½¿ç”¨è¨“ç·´é›†å…§çš„é©—è­‰é›†ï¼Œè€Œéæ¸¬è©¦é›†ï¼
    é€™æ¨£ç¢ºä¿æ¸¬è©¦é›†åœ¨æ•´å€‹è¨“ç·´éç¨‹ä¸­å®Œå…¨æœªè¢«æ¨¡å‹çœ‹åˆ°ã€‚
    
    åƒæ•¸:
        sample_weights: æ¨£æœ¬æ¬Šé‡ï¼ˆç”¨æ–¼æ™‚é–“è¡°æ¸›ï¼Œè¿‘æœŸæ•¸æ“šæ¬Šé‡æ›´é«˜ï¼‰
    """
    print(f"\n{'='*60}")
    print("ğŸš€ XGBoost æ¨¡å‹è¨“ç·´é–‹å§‹")
    print(f"{'='*60}")
    print(f"\nğŸ“Š æ•¸æ“šé›†çµ±è¨ˆ:")
    print(f"   â”œâ”€ è¨“ç·´é›†: {len(train_data)} ç­†")
    print(f"   â””â”€ æ¸¬è©¦é›†: {len(test_data)} ç­† (å®Œå…¨éš”é›¢)")
    print(f"   ğŸ“ ç‰¹å¾µç¶­åº¦: {len(feature_cols)} å€‹")
    
    # å¾è¨“ç·´é›†ä¸­åˆ†å‡ºä¸€éƒ¨åˆ†ä½œç‚º early stopping é©—è­‰é›†
    # ä½¿ç”¨è¨“ç·´é›†çš„æœ€å¾Œ 15% ä½œç‚ºé©—è­‰é›†ï¼ˆä¿æŒæ™‚é–“é †åºï¼‰
    val_split_idx = int(len(train_data) * 0.85)
    train_subset = train_data[:val_split_idx].copy()
    val_subset = train_data[val_split_idx:].copy()
    
    # XGBoost åŸç”Ÿæ”¯æŒ NaN è™•ç†ï¼Œä¸éœ€è¦å¡«å……
    X_train = train_subset[feature_cols]
    y_train = train_subset['Attendance']
    X_val = val_subset[feature_cols]
    y_val = val_subset['Attendance']
    X_test = test_data[feature_cols]
    y_test = test_data['Attendance']
    
    print(f"\nğŸ“… æ™‚é–“åºåˆ—æ•¸æ“šåˆ†å‰²:")
    print(f"   â”œâ”€ è¨“ç·´å­é›†: {len(train_subset)} ç­†")
    print(f"   â”‚     æ—¥æœŸ: {train_subset['Date'].min()} â†’ {train_subset['Date'].max()}")
    print(f"   â”œâ”€ é©—è­‰å­é›†: {len(val_subset)} ç­†")
    print(f"   â”‚     æ—¥æœŸ: {val_subset['Date'].min()} â†’ {val_subset['Date'].max()}")
    print(f"   â””â”€ æ¸¬è©¦é›†:   {len(test_data)} ç­†")
    print(f"         æ—¥æœŸ: {test_data['Date'].min()} â†’ {test_data['Date'].max()}")
    
    # é©—è­‰æ™‚é–“é †åº
    train_max_date = pd.to_datetime(train_subset['Date']).max()
    val_min_date = pd.to_datetime(val_subset['Date']).min()
    test_min_date = pd.to_datetime(test_data['Date']).min()
    val_max_date = pd.to_datetime(val_subset['Date']).max()
    
    print(f"\nğŸ”’ æ•¸æ“šæ´©æ¼æª¢æŸ¥:")
    if val_min_date > train_max_date:
        print(f"   âœ… é©—è­‰é›†æ—¥æœŸ > è¨“ç·´é›†æ—¥æœŸ (å®‰å…¨)")
    else:
        print(f"   âŒ è­¦å‘Šï¼šé©—è­‰é›†å¯èƒ½åŒ…å«è¨“ç·´æœŸé–“çš„æ•¸æ“šï¼")
    
    if test_min_date > val_max_date:
        print(f"   âœ… æ¸¬è©¦é›†æ—¥æœŸ > é©—è­‰é›†æ—¥æœŸ (å®‰å…¨)")
    else:
        print(f"   âŒ è­¦å‘Šï¼šæ¸¬è©¦é›†å¯èƒ½åŒ…å«é©—è­‰æœŸé–“çš„æ•¸æ“šï¼")
    
    print(f"\nğŸ“ˆ ç›®æ¨™è®Šé‡ (Attendance) çµ±è¨ˆ:")
    print(f"   è¨“ç·´é›†: {y_train.min():.0f} - {y_train.max():.0f} äºº (Î¼={y_train.mean():.1f}, Ïƒ={y_train.std():.1f})")
    print(f"   é©—è­‰é›†: {y_val.min():.0f} - {y_val.max():.0f} äºº (Î¼={y_val.mean():.1f}, Ïƒ={y_val.std():.1f})")
    print(f"   æ¸¬è©¦é›†: {y_test.min():.0f} - {y_test.max():.0f} äºº (Î¼={y_test.mean():.1f}, Ïƒ={y_test.std():.1f})")
    
    # å‰µå»ºè‡ªå®šç¾© XGBoost é¡ä»¥ä¿®å¾© _estimator_type éŒ¯èª¤
    class XGBoostModel(xgb.XGBRegressor):
        _estimator_type = "regressor"
    
    # ============ è¶…åƒæ•¸å„ªåŒ– v2.9.30 ============
    # ä½¿ç”¨ Optuna è‡ªå‹•æœç´¢æœ€ä½³è¶…åƒæ•¸
    print(f"\n{'='*60}")
    print("âš™ï¸ XGBoost è¶…åƒæ•¸é…ç½® (v2.9.30 Optuna å„ªåŒ–)")
    print(f"{'='*60}")
    
    # å˜—è©¦ä½¿ç”¨ Optuna å„ªåŒ–
    use_optuna = os.environ.get('USE_OPTUNA', '1') == '1' and OPTUNA_AVAILABLE
    n_trials = int(os.environ.get('OPTUNA_TRIALS', '30'))
    
    if use_optuna:
        best_params = optuna_optimize(X_train, y_train, X_val, y_val, n_trials=n_trials)
        if best_params:
            params = best_params
        else:
            # Fallback åˆ°é è¨­åƒæ•¸
            params = {
                'n_estimators': 500,
                'max_depth': 8,
                'learning_rate': 0.05,
                'min_child_weight': 3,
                'subsample': 0.85,
                'colsample_bytree': 0.85,
                'colsample_bylevel': 0.85,
                'gamma': 0.1,
                'alpha': 0.5,
                'reg_lambda': 1.5,
            }
    else:
        # ä½¿ç”¨é è¨­è¶…åƒæ•¸ï¼ˆåŸºæ–¼ç ”ç©¶ï¼‰
        print("   â„¹ï¸ ä½¿ç”¨é è¨­è¶…åƒæ•¸ï¼ˆè¨­ç½® USE_OPTUNA=1 å•Ÿç”¨å„ªåŒ–ï¼‰")
        params = {
            'n_estimators': 500,
            'max_depth': 8,
            'learning_rate': 0.05,
            'min_child_weight': 3,
            'subsample': 0.85,
            'colsample_bytree': 0.85,
            'colsample_bylevel': 0.85,
            'gamma': 0.1,
            'alpha': 0.5,
            'reg_lambda': 1.5,
        }
    
    print(f"\n   ğŸ“‹ æœ€çµ‚è¶…åƒæ•¸:")
    print(f"   ğŸŒ² n_estimators: {params.get('n_estimators', 500)}")
    print(f"   ğŸ“ max_depth: {params.get('max_depth', 8)}")
    print(f"   ğŸ“‰ learning_rate: {params.get('learning_rate', 0.05):.4f}")
    print(f"   ğŸ‘¶ min_child_weight: {params.get('min_child_weight', 3)}")
    print(f"   ğŸ² subsample: {params.get('subsample', 0.85):.4f}")
    print(f"   ğŸ¯ colsample_bytree: {params.get('colsample_bytree', 0.85):.4f}")
    print(f"   ğŸ“ gamma: {params.get('gamma', 0.1):.4f}")
    print(f"   ğŸ”§ alpha (L1): {params.get('alpha', 0.5):.4f}")
    print(f"   ğŸ”§ reg_lambda (L2): {params.get('reg_lambda', 1.5):.4f}")
    print(f"   ğŸ¯ objective: reg:squarederror")
    print(f"   ğŸ“Š eval_metric: mae")
    print(f"   â¹ï¸ early_stopping_rounds: 50")
    
    model = XGBoostModel(
        n_estimators=params.get('n_estimators', 500),
        max_depth=params.get('max_depth', 8),
        learning_rate=params.get('learning_rate', 0.05),
        min_child_weight=params.get('min_child_weight', 3),
        subsample=params.get('subsample', 0.85),
        colsample_bytree=params.get('colsample_bytree', 0.85),
        colsample_bylevel=params.get('colsample_bylevel', 0.85),
        gamma=params.get('gamma', 0.1),
        objective='reg:squarederror',
        alpha=params.get('alpha', 0.5),
        reg_lambda=params.get('reg_lambda', 1.5),
        tree_method='hist',
        grow_policy='depthwise',
        early_stopping_rounds=50,
        eval_metric='mae',
        random_state=42,
        n_jobs=-1
    )
    
    # ============ æ¨£æœ¬æ¬Šé‡ï¼ˆæ™‚é–“è¡°æ¸› + COVID èª¿æ•´ï¼‰============
    # ç ”ç©¶åŸºç¤: JMIR Medical Informatics 2025 - è¿‘æœŸæ•¸æ“šæ›´é‡è¦
    print(f"\n{'='*60}")
    print("âš–ï¸ è¨ˆç®—æ¨£æœ¬æ¬Šé‡ (ç ”ç©¶åŸºç¤: æ™‚é–“è¡°æ¸›)")
    print(f"{'='*60}")
    
    def calculate_sample_weights(dates, target_values):
        """
        è¨ˆç®—æ¨£æœ¬æ¬Šé‡:
        1. æ™‚é–“è¡°æ¸›: è¿‘æœŸæ•¸æ“šæ¬Šé‡æ›´é«˜
        2. COVID èª¿æ•´: æ¸›å°‘ COVID ç•°å¸¸æœŸé–“çš„æ¬Šé‡
        """
        weights = np.ones(len(dates))
        
        # 1. æ™‚é–“è¡°æ¸›æ¬Šé‡ (åŠè¡°æœŸ = 365 å¤©)
        max_date = dates.max()
        days_from_latest = (max_date - dates).dt.days
        half_life = 365  # ä¸€å¹´åŠè¡°æœŸ
        time_weights = np.exp(-0.693 * days_from_latest / half_life)
        weights *= time_weights
        
        # 2. COVID æœŸé–“æ¬Šé‡èª¿æ•´ (2020-02 åˆ° 2022-06)
        covid_start = pd.Timestamp('2020-02-01')
        covid_end = pd.Timestamp('2022-06-30')
        is_covid = (dates >= covid_start) & (dates <= covid_end)
        weights[is_covid] *= 0.3  # COVID æœŸé–“æ¬Šé‡é™ä½åˆ° 30%
        
        # 3. ç•°å¸¸å€¼æ¬Šé‡èª¿æ•´
        mean_val = target_values.mean()
        std_val = target_values.std()
        z_scores = np.abs((target_values - mean_val) / std_val)
        outlier_mask = z_scores > 3
        weights[outlier_mask] *= 0.5  # æ¥µç«¯ç•°å¸¸å€¼æ¬Šé‡é™ä½
        
        # æ­¸ä¸€åŒ–
        weights = weights / weights.mean()
        
        return weights
    
    # ä½¿ç”¨å¤–éƒ¨æä¾›çš„æ¬Šé‡æˆ–è¨ˆç®—æ–°æ¬Šé‡
    if sample_weights is not None:
        print(f"   ğŸ“Š ä½¿ç”¨å¤–éƒ¨æä¾›çš„æ¨£æœ¬æ¬Šé‡ (å‘½ä»¤è¡Œ --time-decay)")
        train_weights = sample_weights.values if hasattr(sample_weights, 'values') else sample_weights
        # å°æ‡‰è¨“ç·´å­é›†
        train_subset_weights = train_weights[:len(train_subset)]
    else:
        train_weights = calculate_sample_weights(
            pd.to_datetime(train_data['Date']), 
            train_data['Attendance'].values
        )
        # è¨ˆç®—è¨“ç·´å­é›†çš„æ¬Šé‡
        train_subset_weights = calculate_sample_weights(
            pd.to_datetime(train_subset['Date']), 
            train_subset['Attendance'].values
        )
    
    covid_count = ((pd.to_datetime(train_subset['Date']) >= '2020-02-01') & 
                   (pd.to_datetime(train_subset['Date']) <= '2022-06-30')).sum()
    print(f"   ğŸ“Š COVID æœŸé–“æ¨£æœ¬æ•¸: {covid_count}")
    print(f"   ğŸ“Š æ¬Šé‡ç¯„åœ: {train_subset_weights.min():.3f} - {train_subset_weights.max():.3f}")
    print(f"   ğŸ“Š å¹³å‡æ¬Šé‡: {train_subset_weights.mean():.3f}")
    
    print(f"\n{'='*60}")
    print("ğŸ”¥ é–‹å§‹æ¢¯åº¦æå‡è¨“ç·´ (Gradient Boosting)")
    print(f"{'='*60}")
    print(f"   æ¯ 10 è¼ªè¼¸å‡ºä¸€æ¬¡è¨“ç·´é€²åº¦...")
    print(f"   Early stopping: è‹¥ 50 è¼ªç„¡æ”¹å–„å‰‡åœæ­¢")
    print(f"   ä½¿ç”¨æ¨£æœ¬æ¬Šé‡: âœ… (æ™‚é–“è¡°æ¸› + COVID èª¿æ•´)")
    print(f"")
    import time
    
    # ä½¿ç”¨é©—è­‰å­é›†é€²è¡Œ early stopping
    print("   è¨“ç·´ä¸­...")
    fit_start_time = time.time()
    
    try:
        # ä½¿ç”¨æ¨£æœ¬æ¬Šé‡è¨“ç·´ï¼ˆç ”ç©¶å»ºè­°ï¼‰
        model.fit(
            X_train, y_train,
            sample_weight=train_subset_weights,  # æ™‚é–“è¡°æ¸› + COVID èª¿æ•´æ¬Šé‡
            eval_set=[(X_val, y_val)],
            verbose=10
        )
    except TypeError as e:
        # å…¼å®¹æ€§è™•ç†
        print(f"   âš ï¸ XGBoost ç‰ˆæœ¬å…¼å®¹æ€§èª¿æ•´: {e}")
        try:
            model.fit(
                X_train, y_train,
                sample_weight=train_subset_weights,
                eval_set=[(X_val, y_val)]
            )
        except:
            # æœ€å¾Œçš„ fallback - ä¸ä½¿ç”¨æ¬Šé‡
            print(f"   âš ï¸ ç„¡æ³•ä½¿ç”¨æ¨£æœ¬æ¬Šé‡ï¼Œä½¿ç”¨æ¨™æº–è¨“ç·´")
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)]
            )
    
    fit_time = time.time() - fit_start_time
    best_iter = model.best_iteration + 1 if hasattr(model, 'best_iteration') and model.best_iteration is not None else 300
    
    print(f"\nâœ… è¨“ç·´å®Œæˆ!")
    print(f"   â±ï¸ ç¸½è€—æ™‚: {fit_time:.2f} ç§’")
    print(f"   ğŸŒ² æœ€çµ‚æ¨¹æ•¸: {best_iter} æ£µ")
    if hasattr(model, 'best_score') and model.best_score is not None:
        print(f"   ğŸ“Š æœ€ä½³é©—è­‰ MAE: {model.best_score:.2f} äºº")
    
    # åœ¨å®Œå…¨æœªè¦‹éçš„æ¸¬è©¦é›†ä¸Šè©•ä¼°
    print(f"\nğŸ“ˆ é–‹å§‹æ¨¡å‹è©•ä¼° (åœ¨å®Œå…¨æœªè¦‹éçš„æ¸¬è©¦é›†ä¸Š)...")
    y_pred = model.predict(X_test)
    
    # è¨ˆç®—å„ç¨®èª¤å·®æŒ‡æ¨™
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    
    # è¨ˆç®—å…¶ä»–çµ±è¨ˆæŒ‡æ¨™
    mean_error = np.mean(y_pred - y_test)
    std_error = np.std(y_pred - y_test)
    r2 = sklearn_r2_score(y_test, y_pred)
    
    # è¨ˆç®—èª¿æ•´ RÂ² (Adjusted RÂ²)
    n = len(y_test)
    p = len(feature_cols)
    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) if n > p + 1 else r2
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š XGBoost æ¨¡å‹æ€§èƒ½æŒ‡æ¨™ (æ¸¬è©¦é›† - å®Œå…¨ç¨ç«‹)")
    print(f"{'='*60}")
    print(f"  MAE (å¹³å‡çµ•å°èª¤å·®): {mae:.2f} ç—…äºº")
    print(f"  RMSE (å‡æ–¹æ ¹èª¤å·®): {rmse:.2f} ç—…äºº")
    print(f"  MAPE (å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·®): {mape:.2f}%")
    print(f"  å¹³å‡èª¤å·® (åå·®): {mean_error:.2f} ç—…äºº")
    print(f"  èª¤å·®æ¨™æº–å·®: {std_error:.2f} ç—…äºº")
    print(f"  RÂ² å¾—åˆ†: {r2:.4f} ({r2*100:.1f}%)")
    print(f"  èª¿æ•´ RÂ² å¾—åˆ†: {adj_r2:.4f} ({adj_r2*100:.1f}%)")
    print(f"  é æ¸¬å€¼ç¯„åœ: {y_pred.min():.1f} - {y_pred.max():.1f} ç—…äºº")
    
    return model, {
        'mae': mae, 
        'rmse': rmse, 
        'mape': mape, 
        'r2': r2,
        'adj_r2': adj_r2,
        'mean_error': mean_error,
        'std_error': std_error,
        'optuna_optimized': use_optuna
    }

def main():
    import argparse
    import time
    
    print(f"\n{'='*60}")
    print("ğŸ¥ NDH AED XGBoost æ¨¡å‹è¨“ç·´ç³»çµ±")
    print(f"{'='*60}")
    print(f"â° é–‹å§‹æ™‚é–“: {datetime.datetime.now(HKT).strftime('%Y-%m-%d %H:%M:%S')} HKT")
    
    parser = argparse.ArgumentParser(description='Train XGBoost model')
    parser.add_argument('--csv', type=str, help='Path to CSV file with historical data')
    parser.add_argument('--full', action='store_true', help='Use full feature set (161 features) instead of optimized')
    parser.add_argument('--optimize', action='store_true', help='Run feature optimization before training')
    parser.add_argument('--quick-optimize', action='store_true', help='Run quick feature optimization')
    parser.add_argument('--sliding-window', type=int, default=0, help='Use only recent N years of data (0=all data)')
    parser.add_argument('--time-decay', type=float, default=0.0, help='Time decay rate for sample weights (0=no decay, 0.001=recommended)')
    args = parser.parse_args()
    
    # å‹•æ…‹åŠ è¼‰å„ªåŒ–ç‰¹å¾µé›†ï¼ˆå¾ optimal_features.jsonï¼‰
    def load_optimal_features():
        """å¾ JSON æ–‡ä»¶åŠ è¼‰æœ€ä½³ç‰¹å¾µé…ç½®"""
        optimal_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'models', 'optimal_features.json')
        if os.path.exists(optimal_path):
            try:
                with open(optimal_path, 'r') as f:
                    config = json.load(f)
                if 'optimal_features' in config:
                    print(f"   ğŸ“‚ å¾ optimal_features.json åŠ è¼‰ {len(config['optimal_features'])} å€‹ç‰¹å¾µ")
                    print(f"   ğŸ“Š ä¸Šæ¬¡å„ªåŒ–: {config.get('updated', 'N/A')}")
                    print(f"   ğŸ“ˆ é æœŸ MAE: {config.get('metrics', {}).get('mae', 'N/A')}")
                    return config['optimal_features']
            except Exception as e:
                print(f"   âš ï¸ ç„¡æ³•åŠ è¼‰ optimal_features.json: {e}")
        return None
    
    # é»˜èªå„ªåŒ–ç‰¹å¾µé›†ï¼ˆå‚™ç”¨ï¼‰
    DEFAULT_OPTIMAL_FEATURES = [
        "Attendance_EWMA7",        # æ ¸å¿ƒç‰¹å¾µ
        "Attendance_EWMA14",
        "Daily_Change",
        "Monthly_Change",
        "Attendance_Lag1",
        "Weekly_Change",
        "Attendance_Rolling7",
        "Attendance_Position7",
        "Attendance_Lag30",
        "Attendance_Lag7",
        "Day_of_Week",
        "Lag1_Diff",
        "DayOfWeek_sin",
        "Attendance_Rolling14",
        "Attendance_Position14",
        "Attendance_Position30",
        "Attendance_Rolling3",
        "Attendance_Min7",
        "Attendance_Median14",
        "DayOfWeek_Target_Mean",
        "Attendance_Median3",
        "Attendance_EWMA30",
        "Is_Winter_Flu_Season",
        "Is_Weekend",
        "Holiday_Factor",
    ]
    
    # å¦‚æœè«‹æ±‚å„ªåŒ–ï¼Œå…ˆé‹è¡Œç‰¹å¾µå„ªåŒ–å™¨
    if args.optimize or args.quick_optimize:
        print("\n" + "=" * 60)
        print("ğŸ”¬ é‹è¡Œè‡ªå‹•ç‰¹å¾µå„ªåŒ–å™¨...")
        print("=" * 60)
        try:
            from auto_feature_optimizer import run_optimization
            run_optimization(quick=args.quick_optimize)
            print("\n" + "=" * 60)
            print("âœ… ç‰¹å¾µå„ªåŒ–å®Œæˆï¼Œç¹¼çºŒè¨“ç·´...")
            print("=" * 60)
        except ImportError:
            print("âš ï¸ ç„¡æ³•å°å…¥ auto_feature_optimizerï¼Œä½¿ç”¨é»˜èªç‰¹å¾µé›†")
        except Exception as e:
            print(f"âš ï¸ å„ªåŒ–éç¨‹å‡ºéŒ¯: {e}")
    
    # åŠ è¼‰æœ€ä½³ç‰¹å¾µ
    OPTIMAL_FEATURES = load_optimal_features() or DEFAULT_OPTIMAL_FEATURES
    
    # å‰µå»ºæ¨¡å‹ç›®éŒ„ï¼ˆç›¸å°æ–¼ç•¶å‰è…³æœ¬ç›®éŒ„ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(script_dir, 'models')
    os.makedirs(models_dir, exist_ok=True)
    print(f"ğŸ“ æ¨¡å‹ç›®éŒ„: {models_dir}")
    
    # ============ éšæ®µ 1: æ•¸æ“šåŠ è¼‰ ============
    print(f"\n{'='*60}")
    print("ğŸ“¥ éšæ®µ 1/4: æ•¸æ“šåŠ è¼‰")
    print(f"{'='*60}")
    
    df = None
    data_source = None
    load_start = time.time()
    
    # å„ªå…ˆä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„ CSV æ–‡ä»¶
    if args.csv and os.path.exists(args.csv):
        print(f"   ğŸ“„ å˜—è©¦å¾å‘½ä»¤è¡Œ CSV åŠ è¼‰: {args.csv}")
        df = load_data_from_csv(args.csv)
        if df is not None and len(df) > 0:
            data_source = f"CSV: {args.csv}"
    
    # å¦‚æœæ²’æœ‰æŒ‡å®š CSVï¼Œå˜—è©¦å¾æ•¸æ“šåº«åŠ è¼‰æ•¸æ“š
    if df is None or len(df) == 0:
        print(f"   ğŸ—„ï¸ å˜—è©¦å¾ PostgreSQL æ•¸æ“šåº«åŠ è¼‰...")
        df = load_data_from_db()
        if df is not None and len(df) > 0:
            data_source = "PostgreSQL æ•¸æ“šåº«"
    
    # å¦‚æœæ•¸æ“šåº«ä¸å¯ç”¨ï¼Œå˜—è©¦å¾é»˜èª CSV åŠ è¼‰
    if df is None or len(df) == 0:
        print(f"   ğŸ“„ å˜—è©¦å¾æœ¬åœ° CSV æ–‡ä»¶åŠ è¼‰...")
        csv_paths = [
            '../NDH_AED_Clean.csv',
            'NDH_AED_Clean.csv',
            '../NDH_AED_Attendance_2025-12-01_to_2025-12-21.csv',
            'NDH_AED_Attendance_2025-12-01_to_2025-12-21.csv',
        ]
        for csv_path in csv_paths:
            if os.path.exists(csv_path):
                print(f"      å˜—è©¦: {csv_path}")
                df = load_data_from_csv(csv_path)
                if df is not None and len(df) > 0:
                    data_source = f"CSV: {csv_path}"
                    break
    
    if df is None or len(df) == 0:
        print("âŒ éŒ¯èª¤: ç„¡æ³•åŠ è¼‰æ•¸æ“š")
        sys.exit(1)
    
    load_time = time.time() - load_start
    print(f"\nâœ… æ•¸æ“šåŠ è¼‰å®Œæˆ!")
    print(f"   ğŸ“Š æ•¸æ“šä¾†æº: {data_source}")
    print(f"   ğŸ“ ç¸½è¨˜éŒ„æ•¸: {len(df)} ç­†")
    print(f"   ğŸ“… æ—¥æœŸç¯„åœ: {df['Date'].min()} â†’ {df['Date'].max()}")
    print(f"   â±ï¸ åŠ è¼‰è€—æ™‚: {load_time:.2f} ç§’")
    
    # ============ éšæ®µ 2: AI å› å­åŠ è¼‰ ============
    print(f"\n{'='*60}")
    print("ğŸ¤– éšæ®µ 2/4: AI å› å­åŠ è¼‰")
    print(f"{'='*60}")
    
    ai_factors = df.attrs.get('ai_factors', {}) if hasattr(df, 'attrs') else {}
    
    # å¦‚æœæ²’æœ‰å¾æ•¸æ“šåº«ç²å– AI å› å­ï¼Œå˜—è©¦å¾æœ¬åœ° JSON æ–‡ä»¶åŠ è¼‰
    if not ai_factors:
        ai_factors_path = os.path.join(models_dir, 'ai_factors.json')
        print(f"   ğŸ“„ å˜—è©¦å¾æœ¬åœ°åŠ è¼‰: {ai_factors_path}")
        if os.path.exists(ai_factors_path):
            try:
                with open(ai_factors_path, 'r', encoding='utf-8') as f:
                    ai_factors = json.load(f)
                print(f"   âœ… å¾æœ¬åœ°æ–‡ä»¶åŠ è¼‰äº† {len(ai_factors)} å€‹æ—¥æœŸçš„ AI å› å­")
            except Exception as e:
                print(f"   âš ï¸ ç„¡æ³•å¾æœ¬åœ°æ–‡ä»¶åŠ è¼‰: {e}")
    
    if ai_factors:
        print(f"\nâœ… AI å› å­çµ±è¨ˆ:")
        print(f"   ğŸ“Š è¦†è“‹æ—¥æœŸæ•¸: {len(ai_factors)} å¤©")
        # è¨ˆç®— AI å› å­çš„çµ±è¨ˆ
        factors_values = []
        for date_key, factor_data in ai_factors.items():
            if isinstance(factor_data, dict) and 'impact_factor' in factor_data:
                factors_values.append(factor_data['impact_factor'])
        if factors_values:
            print(f"   ğŸ“ˆ å½±éŸ¿å› å­ç¯„åœ: {min(factors_values):.3f} - {max(factors_values):.3f}")
            print(f"   ğŸ“Š å½±éŸ¿å› å­å¹³å‡: {np.mean(factors_values):.3f}")
    else:
        print(f"   â„¹ï¸ æ²’æœ‰æ‰¾åˆ° AI å› å­æ•¸æ“šï¼Œå°‡ä½¿ç”¨é»˜èªå€¼ (1.0)")
    
    # ============ éšæ®µ 3: ç‰¹å¾µå·¥ç¨‹ ============
    print(f"\n{'='*60}")
    print("ğŸ”§ éšæ®µ 3/4: ç‰¹å¾µå·¥ç¨‹")
    print(f"{'='*60}")
    
    fe_start = time.time()
    print(f"   åŸå§‹æ•¸æ“šåˆ—æ•¸: {len(df.columns)}")
    print(f"\n   æ­£åœ¨å‰µå»ºç‰¹å¾µ...")
    print(f"   â”œâ”€ æ™‚é–“ç‰¹å¾µ: å¹´ã€æœˆã€æ—¥ã€æ˜ŸæœŸã€å­£åº¦...")
    print(f"   â”œâ”€ å¾ªç’°ç·¨ç¢¼: sin/cos è®Šæ›ï¼ˆæ•æ‰å‘¨æœŸæ€§ï¼‰...")
    print(f"   â”œâ”€ æ»¯å¾Œç‰¹å¾µ: Lag1, Lag7, Lag14, Lag30, Lag365...")
    print(f"   â”œâ”€ æ»¾å‹•çµ±è¨ˆ: 7å¤©/14å¤©/30å¤© å‡å€¼ã€æ¨™æº–å·®...")
    print(f"   â”œâ”€ å‡æœŸç‰¹å¾µ: é¦™æ¸¯å…¬çœ¾å‡æœŸï¼ˆå«è¾²æ›†ç¯€æ—¥ï¼‰...")
    print(f"   â”œâ”€ äº‹ä»¶ç‰¹å¾µ: COVIDæœŸé–“ã€æµæ„Ÿå­£ç¯€...")
    print(f"   â””â”€ AIå› å­ç‰¹å¾µ: 13ç¶­åº¦å½±éŸ¿å› å­...")
    
    df = create_comprehensive_features(df, ai_factors_dict=ai_factors if ai_factors else None)
    
    fe_time = time.time() - fe_start
    print(f"\nâœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆ!")
    print(f"   ğŸ“ æœ€çµ‚ç‰¹å¾µæ•¸: {len(df.columns)} åˆ—")
    print(f"   â±ï¸ è™•ç†è€—æ™‚: {fe_time:.2f} ç§’")
    
    # ç§»é™¤åŒ…å« NaN çš„è¡Œï¼ˆé™¤äº†æˆ‘å€‘å·²ç¶“å¡«å……çš„åˆ—ï¼‰
    original_len = len(df)
    df = df.dropna(subset=['Attendance'])
    if len(df) < original_len:
        print(f"   âš ï¸ ç§»é™¤äº† {original_len - len(df)} ç­†ç„¡æ•ˆæ•¸æ“š")
    
    # ============ COVID æœŸé–“æ’é™¤ (åŸºæ–¼å¯¦é©—è­‰æ“š) ============
    # ç ”ç©¶çµæœ: COVID æ’é™¤æ³•å„ªæ–¼ Sliding Window (MAE 16.52 vs 19.66, æ”¹å–„ 16%)
    # åƒè€ƒ: experiment_covid_exclusion_comparison.py å¯¦é©—çµæœ
    # æ’é™¤æœŸé–“: 2020-02-01 è‡³ 2022-06-30 (WHO å®£å¸ƒ COVID å¤§æµè¡Œè‡³é¦™æ¸¯æ”¾å¯¬é™åˆ¶)
    use_covid_exclusion = os.environ.get('USE_COVID_EXCLUSION', '1') == '1'
    covid_start = pd.Timestamp('2020-02-01')
    covid_end = pd.Timestamp('2022-06-30')
    
    if use_covid_exclusion:
        original_len = len(df)
        covid_mask = (df['Date'] >= covid_start) & (df['Date'] <= covid_end)
        covid_count = covid_mask.sum()
        df = df[~covid_mask].copy()
        print(f"\nğŸ¦  COVID æœŸé–“æ’é™¤æ¨¡å¼ (ç ”ç©¶åŸºç¤: å¯¦é©—è­‰æ“š):")
        print(f"   â”œâ”€ æ’é™¤æœŸé–“: {covid_start.strftime('%Y-%m-%d')} è‡³ {covid_end.strftime('%Y-%m-%d')}")
        print(f"   â”œâ”€ æ’é™¤ç­†æ•¸: {covid_count} ç­† COVID æœŸé–“æ•¸æ“š")
        print(f"   â”œâ”€ æ•¸æ“šé‡: {original_len} â†’ {len(df)} ç­†")
        print(f"   â””â”€ ç ”ç©¶çµæœ: MAE 16.52 (vs Sliding Window 3yr: 19.66, æ”¹å–„ 16%)")
    
    # ============ æ»‘å‹•çª—å£éæ¿¾ (å‚™ç”¨é¸é …) ============
    # æ³¨æ„: å¯¦é©—è­‰æ˜ COVID æ’é™¤æ³•å„ªæ–¼ Sliding Window
    sliding_window_years = args.sliding_window or int(os.environ.get('SLIDING_WINDOW_YEARS', '0'))
    if sliding_window_years > 0 and not use_covid_exclusion:
        cutoff_date = df['Date'].max() - pd.Timedelta(days=sliding_window_years * 365)
        original_len = len(df)
        df = df[df['Date'] >= cutoff_date].copy()
        print(f"\nğŸ“… æ»‘å‹•çª—å£è¨“ç·´æ¨¡å¼ (å‚™ç”¨):")
        print(f"   â”œâ”€ çª—å£å¤§å°: æœ€è¿‘ {sliding_window_years} å¹´")
        print(f"   â”œâ”€ æˆªæ­¢æ—¥æœŸ: {cutoff_date.strftime('%Y-%m-%d')}")
        print(f"   â”œâ”€ æ•¸æ“šé‡: {original_len} â†’ {len(df)} ç­† (-{original_len - len(df)} ç­†èˆŠæ•¸æ“š)")
        print(f"   â””â”€ âš ï¸ å»ºè­°ä½¿ç”¨ COVID æ’é™¤æ³• (è¨­ç½® USE_COVID_EXCLUSION=1)")
    
    # ============ æ•¸æ“šåˆ†å‰² ============
    print(f"\nâœ‚ï¸ æ™‚é–“åºåˆ—åˆ†å‰² (80/20):")
    split_idx = int(len(df) * 0.8)
    train_data = df[:split_idx].copy()
    test_data = df[split_idx:].copy()
    
    # ============ æ™‚é–“è¡°æ¸›æ¬Šé‡ (è§£æ±º Concept Drift) ============
    time_decay_rate = args.time_decay or float(os.environ.get('TIME_DECAY_RATE', '0'))
    sample_weights = None
    if time_decay_rate > 0:
        days_from_end = (train_data['Date'].max() - train_data['Date']).dt.days
        sample_weights = np.exp(-time_decay_rate * days_from_end)
        sample_weights = sample_weights / sample_weights.mean()  # æ­¸ä¸€åŒ–
        print(f"\nâš–ï¸ æ™‚é–“è¡°æ¸›æ¬Šé‡æ¨¡å¼:")
        print(f"   â”œâ”€ è¡°æ¸›ç‡: {time_decay_rate}")
        print(f"   â”œâ”€ æœ€æ–°æ•¸æ“šæ¬Šé‡: {sample_weights.iloc[-1]:.2f}")
        print(f"   â””â”€ æœ€èˆŠæ•¸æ“šæ¬Šé‡: {sample_weights.iloc[0]:.2f}")
    
    print(f"   â”œâ”€ è¨“ç·´é›†: {len(train_data)} ç­†")
    print(f"   â”‚     æ—¥æœŸ: {train_data['Date'].min()} â†’ {train_data['Date'].max()}")
    print(f"   â””â”€ æ¸¬è©¦é›†: {len(test_data)} ç­†")
    print(f"         æ—¥æœŸ: {test_data['Date'].min()} â†’ {test_data['Date'].max()}")
    
    # ç²å–ç‰¹å¾µåˆ— - é»˜èªä½¿ç”¨å„ªåŒ–ç‰¹å¾µé›†ï¼ˆç ”ç©¶è¡¨æ˜ 25 ç‰¹å¾µæ•ˆæœæœ€ä½³ï¼‰
    use_full = args.full or os.environ.get('USE_FULL_FEATURES', '0') == '1'
    
    if use_full:
        print(f"\n   ğŸ“Š ä½¿ç”¨å®Œæ•´ç‰¹å¾µé›†æ¨¡å¼ï¼ˆ161 ç‰¹å¾µï¼‰")
        feature_cols = get_feature_columns()
        original_feature_count = len(feature_cols)
        feature_cols = [col for col in feature_cols if col in df.columns]
        if len(feature_cols) < original_feature_count:
            print(f"   âš ï¸ {original_feature_count - len(feature_cols)} å€‹é æœŸç‰¹å¾µä¸å­˜åœ¨")
        print(f"   ğŸ“ ä½¿ç”¨ {len(feature_cols)} å€‹ç‰¹å¾µé€²è¡Œè¨“ç·´")
    else:
        print(f"\n   ğŸš€ ä½¿ç”¨å„ªåŒ–ç‰¹å¾µé›†ï¼ˆç ”ç©¶è¡¨æ˜ 25 ç‰¹å¾µæ•ˆæœæœ€ä½³ï¼‰")
        print(f"   ğŸ“Š æ ¸å¿ƒç‰¹å¾µ: EWMA7+EWMA14 ä½” 90% é‡è¦æ€§")
        feature_cols = [col for col in OPTIMAL_FEATURES if col in df.columns]
        print(f"   ğŸ“ ä½¿ç”¨ {len(feature_cols)} å€‹ç²¾é¸ç‰¹å¾µé€²è¡Œè¨“ç·´")
    
    # ============ éšæ®µ 4: æ¨¡å‹è¨“ç·´ ============
    print(f"\n{'='*60}")
    print("ğŸ¯ éšæ®µ 4/4: æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°")
    print(f"{'='*60}")
    
    # æ™‚é–“åºåˆ—äº¤å‰é©—è­‰ï¼ˆç¢ºä¿ç„¡æ•¸æ“šæ´©æ¼ï¼‰- v2.9.21 å„ªåŒ–ç‚º 3-fold
    cv_scores = time_series_cross_validate(train_data, feature_cols, n_splits=3)
    
    # è¨“ç·´æœ€çµ‚æ¨¡å‹
    model, metrics = train_xgboost_model(train_data, test_data, feature_cols, sample_weights=sample_weights)
    
    # ä¿å­˜æ¨¡å‹ï¼ˆä½¿ç”¨çµ•å°è·¯å¾‘ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(script_dir, 'models')
    
    # å®šç¾©æŒ‡æ¨™æ–‡ä»¶è·¯å¾‘ï¼ˆç”¨æ–¼ä¿å­˜æ–°æŒ‡æ¨™ï¼‰
    metrics_path = os.path.join(models_dir, 'xgboost_metrics.json')
    
    # åŠ è¼‰èˆŠæ¨¡å‹æŒ‡æ¨™ï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰- å„ªå…ˆå¾æ•¸æ“šåº«è®€å–
    old_metrics = load_old_metrics_from_db()
    if old_metrics:
        print(f"ğŸ“Š å¾æ•¸æ“šåº«åŠ è¼‰èˆŠæ¨¡å‹æŒ‡æ¨™: MAE={old_metrics.get('mae', 'N/A'):.2f}, MAPE={old_metrics.get('mape', 'N/A'):.2f}%")
    else:
        # æ•¸æ“šåº«ä¸å¯ç”¨ï¼Œå˜—è©¦å¾æœ¬åœ°æ–‡ä»¶è®€å–
        if os.path.exists(metrics_path):
            try:
                with open(metrics_path, 'r') as f:
                    old_metrics = json.load(f)
                print(f"ğŸ“Š å¾æœ¬åœ°æ–‡ä»¶åŠ è¼‰èˆŠæ¨¡å‹æŒ‡æ¨™: MAE={old_metrics.get('mae', 'N/A'):.2f}")
            except:
                old_metrics = None
    
    model_path = os.path.join(models_dir, 'xgboost_model.json')
    model.save_model(model_path)
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
    
    # ä¿å­˜ç‰¹å¾µåˆ—å
    features_path = os.path.join(models_dir, 'xgboost_features.json')
    with open(features_path, 'w') as f:
        json.dump(feature_cols, f)
    
    # æ·»åŠ æ›´å¤šè¨“ç·´ä¿¡æ¯åˆ°æŒ‡æ¨™
    training_info = {
        'mae': metrics['mae'],
        'rmse': metrics['rmse'],
        'mape': metrics['mape'],
        'r2': metrics['r2'],              # RÂ² åˆ†æ•¸ (v2.9.30)
        'adj_r2': metrics['adj_r2'],      # èª¿æ•´ RÂ² (v2.9.30)
        'mean_error': metrics['mean_error'],
        'std_error': metrics['std_error'],
        'training_date': datetime.datetime.now(HKT).strftime('%Y-%m-%d %H:%M:%S HKT'),
        'data_count': len(df),
        'train_count': len(train_data),
        'test_count': len(test_data),
        'feature_count': len(feature_cols),
        'ai_factors_count': len(ai_factors) if ai_factors else 0,
        # äº¤å‰é©—è­‰åˆ†æ•¸ï¼ˆç¢ºä¿ç„¡æœªä¾†æ•¸æ“šæ´©æ¼ï¼‰
        'cv_mae_mean': cv_scores['cv_mae_mean'],
        'cv_mae_std': cv_scores['cv_mae_std'],
        'cv_rmse_mean': cv_scores['cv_rmse_mean'],
        'cv_rmse_std': cv_scores['cv_rmse_std'],
        'cv_mape_mean': cv_scores['cv_mape_mean'],
        'cv_mape_std': cv_scores['cv_mape_std'],
        'time_series_validation': True,  # æ¨™è¨˜ä½¿ç”¨äº†æ­£ç¢ºçš„æ™‚é–“åºåˆ—é©—è­‰
        'version': '2.9.52',
        'optuna_optimized': metrics.get('optuna_optimized', False)
    }
    
    # ä¿å­˜è©•ä¼°æŒ‡æ¨™
    with open(metrics_path, 'w') as f:
        json.dump(training_info, f, indent=2)
    
    # è¨ˆç®—ç‰¹å¾µé‡è¦æ€§
    print(f"\n{'='*60}")
    print("ğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ (Top 15 æœ€é‡è¦ç‰¹å¾µ):")
    print(f"{'='*60}")
    
    importance = model.feature_importances_
    feature_importance = list(zip(feature_cols, importance))
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    
    for i, (feat, imp) in enumerate(feature_importance[:15]):
        bar_length = int(imp / max(importance) * 30)
        bar = "â–ˆ" * bar_length + "â–‘" * (30 - bar_length)
        print(f"  {i+1:2}. {feat:25} {bar} {imp:.4f}")
    
    # é¡¯ç¤ºè¨“ç·´å‰å¾Œå°æ¯”
    print(f"\n{'='*60}")
    print("ğŸ“ˆ æ¨¡å‹æ€§èƒ½è®ŠåŒ–:")
    print(f"{'='*60}")
    
    if old_metrics:
        old_mae = old_metrics.get('mae', 0)
        old_rmse = old_metrics.get('rmse', 0)
        old_mape = old_metrics.get('mape', 0)
        
        mae_change = metrics['mae'] - old_mae
        rmse_change = metrics['rmse'] - old_rmse
        mape_change = metrics['mape'] - old_mape
        
        # ä½¿ç”¨å®¹å·®åˆ¤æ–·ï¼Œé¿å…æµ®é»æ•¸ç²¾åº¦å•é¡Œï¼ˆé¡¯ç¤ºç‚º 0.00 æ™‚æ‡‰ç‚ºç„¡è®ŠåŒ–ï¼‰
        tolerance = 0.005
        def get_change_icon(change, tol=tolerance):
            if abs(change) < tol:
                return "â¡ï¸ ç„¡è®ŠåŒ–"
            return "âœ… æ”¹å–„" if change < 0 else "âš ï¸ ä¸‹é™"
        
        mae_icon = get_change_icon(mae_change)
        rmse_icon = get_change_icon(rmse_change)
        mape_icon = get_change_icon(mape_change)
        
        print(f"\n  ğŸ“Š MAE (å¹³å‡çµ•å°èª¤å·®):")
        print(f"     èˆŠæ¨¡å‹: {old_mae:.2f} ç—…äºº")
        print(f"     æ–°æ¨¡å‹: {metrics['mae']:.2f} ç—…äºº")
        print(f"     è®ŠåŒ–: {mae_change:+.2f} ç—…äºº {mae_icon}")
        
        print(f"\n  ğŸ“Š RMSE (å‡æ–¹æ ¹èª¤å·®):")
        print(f"     èˆŠæ¨¡å‹: {old_rmse:.2f} ç—…äºº")
        print(f"     æ–°æ¨¡å‹: {metrics['rmse']:.2f} ç—…äºº")
        print(f"     è®ŠåŒ–: {rmse_change:+.2f} ç—…äºº {rmse_icon}")
        
        print(f"\n  ğŸ“Š MAPE (å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·®):")
        print(f"     èˆŠæ¨¡å‹: {old_mape:.2f}%")
        print(f"     æ–°æ¨¡å‹: {metrics['mape']:.2f}%")
        print(f"     è®ŠåŒ–: {mape_change:+.2f}% {mape_icon}")
        
        # è¨ˆç®—ç¸½é«”æ”¹å–„ï¼ˆä½¿ç”¨ç›¸åŒå®¹å·®ï¼‰
        improvements = sum([1 for c in [mae_change, rmse_change, mape_change] if c < -tolerance])
        degradations = sum([1 for c in [mae_change, rmse_change, mape_change] if c > tolerance])
        
        print(f"\n  ğŸ“‹ ç¸½çµ:")
        if improvements > degradations:
            print(f"     ğŸ‰ æ¨¡å‹æ•´é«”æ€§èƒ½æå‡ï¼({improvements}/3 æŒ‡æ¨™æ”¹å–„)")
        elif degradations > improvements:
            print(f"     âš ï¸ æ¨¡å‹æ•´é«”æ€§èƒ½ä¸‹é™ ({degradations}/3 æŒ‡æ¨™ä¸‹é™)")
            print(f"     ğŸ’¡ å»ºè­°ï¼šæª¢æŸ¥æ–°æ•¸æ“šè³ªé‡æˆ–å¢åŠ è¨“ç·´æ•¸æ“š")
        else:
            print(f"     â¡ï¸ æ¨¡å‹æ€§èƒ½ç¶­æŒç©©å®š")
    else:
        print(f"\n  â„¹ï¸ é€™æ˜¯é¦–æ¬¡è¨“ç·´ï¼Œç„¡èˆŠæ¨¡å‹å¯æ¯”è¼ƒ")
        print(f"\n  ğŸ“Š ç•¶å‰æ¨¡å‹æ€§èƒ½:")
        print(f"     MAE: {metrics['mae']:.2f} ç—…äºº")
        print(f"     RMSE: {metrics['rmse']:.2f} ç—…äºº")
        print(f"     MAPE: {metrics['mape']:.2f}%")
    
    # è¨“ç·´ç¸½çµ
    total_time = time.time() - load_start + fe_time
    print(f"\n{'='*60}")
    print("ğŸ† è¨“ç·´å®Œæˆç¸½çµ")
    print(f"{'='*60}")
    print(f"")
    print(f"   ğŸ“… è¨“ç·´æ™‚é–“: {training_info['training_date']}")
    print(f"   â±ï¸ ç¸½è€—æ™‚: {total_time:.1f} ç§’")
    print(f"")
    print(f"   ğŸ“Š æ•¸æ“šçµ±è¨ˆ:")
    print(f"      â”œâ”€ ç¸½æ•¸æ“šé‡: {training_info['data_count']} ç­†")
    print(f"      â”œâ”€ è¨“ç·´é›†: {training_info['train_count']} ç­†")
    print(f"      â””â”€ æ¸¬è©¦é›†: {training_info['test_count']} ç­†")
    print(f"")
    print(f"   ğŸ”§ æ¨¡å‹é…ç½®:")
    print(f"      â”œâ”€ ç‰¹å¾µæ•¸: {training_info['feature_count']} å€‹")
    if training_info['ai_factors_count'] > 0:
        print(f"      â””â”€ AIå› å­: {training_info['ai_factors_count']} å€‹æ—¥æœŸ")
    else:
        print(f"      â””â”€ AIå› å­: ç„¡")
    print(f"")
    print(f"   ğŸ“ˆ æ¨¡å‹æ€§èƒ½ (æ¸¬è©¦é›†):")
    print(f"      â”œâ”€ MAE: {metrics['mae']:.2f} äºº (å¹³å‡èª¤å·®)")
    print(f"      â”œâ”€ RMSE: {metrics['rmse']:.2f} äºº (å‡æ–¹æ ¹èª¤å·®)")
    print(f"      â”œâ”€ MAPE: {metrics['mape']:.2f}% (ç™¾åˆ†æ¯”èª¤å·®)")
    print(f"      â”œâ”€ RÂ²: {metrics['r2']*100:.1f}% (æ¨¡å‹æ“¬åˆåº¦)")
    print(f"      â””â”€ èª¿æ•´ RÂ²: {metrics['adj_r2']*100:.1f}% (è€ƒæ…®ç‰¹å¾µæ•¸)")
    print(f"")
    print(f"   ğŸ“Š äº¤å‰é©—è­‰ (5-Fold):")
    print(f"      â””â”€ MAE: {cv_scores['cv_mae_mean']:.2f} Â± {cv_scores['cv_mae_std']:.2f} äºº")
    print(f"")
    print(f"{'='*60}")
    print(f"âœ… XGBoost æ¨¡å‹è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ã€‚")
    print(f"{'='*60}")
    
    # v3.0.10: è¨“ç·´å¾Œè‡ªå‹•æ›´æ–°ç‰¹å¾µæ–‡æª”
    try:
        from update_feature_docs import update_docs
        print(f"\nğŸ“ æ›´æ–°ç‰¹å¾µæ–‡æª”...")
        update_docs()
        print(f"âœ… ç‰¹å¾µæ–‡æª”å·²æ›´æ–°")
    except Exception as e:
        print(f"âš ï¸ æ›´æ–°ç‰¹å¾µæ–‡æª”å¤±æ•—: {e}")

if __name__ == '__main__':
    main()

