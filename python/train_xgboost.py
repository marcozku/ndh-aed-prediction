"""
XGBoost æ¨¡å‹è¨“ç·´è…³æœ¬
æ ¹æ“š AI-AED-Algorithm-Specification.txt Section 6.1
"""
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error
import json
import os
import sys
from feature_engineering import create_comprehensive_features, get_feature_columns

def load_ai_factors_from_db(conn):
    """å¾æ•¸æ“šåº«åŠ è¼‰ AI å› å­æ•¸æ“š"""
    try:
        import sqlalchemy
        # ä½¿ç”¨ SQLAlchemy å‰µå»ºé€£æ¥ä»¥é¿å…è­¦å‘Š
        from sqlalchemy import create_engine
        # å¾ psycopg2 é€£æ¥ç²å–é€£æ¥å­—ç¬¦ä¸²
        dsn = conn.get_dsn_parameters()
        connection_string = f"postgresql://{dsn.get('user')}:{dsn.get('password', '')}@{dsn.get('host')}:{dsn.get('port', 5432)}/{dsn.get('dbname')}"
        engine = create_engine(connection_string)
        
        query = """
            SELECT factors_cache
            FROM ai_factors_cache
            WHERE id = 1
        """
        result = pd.read_sql_query(query, engine)
        if len(result) > 0 and result.iloc[0]['factors_cache'] is not None:
            import json
            factors_cache = result.iloc[0]['factors_cache']
            if isinstance(factors_cache, str):
                factors_cache = json.loads(factors_cache)
            elif isinstance(factors_cache, dict):
                pass  # å·²ç¶“æ˜¯å­—å…¸
            else:
                factors_cache = {}
            return factors_cache
        return {}
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•åŠ è¼‰ AI å› å­æ•¸æ“š: {e}")
        return {}

def load_data_from_db():
    """å¾æ•¸æ“šåº«åŠ è¼‰æ•¸æ“šï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    try:
        import psycopg2
        from dotenv import load_dotenv
        load_dotenv()
        
        conn = psycopg2.connect(
            host=os.getenv('PGHOST') or os.getenv('DATABASE_URL', '').split('@')[1].split('/')[0] if '@' in os.getenv('DATABASE_URL', '') else None,
            database=os.getenv('PGDATABASE') or os.getenv('DATABASE_URL', '').split('/')[-1] if '/' in os.getenv('DATABASE_URL', '') else None,
            user=os.getenv('PGUSER') or os.getenv('DATABASE_URL', '').split('://')[1].split(':')[0] if '://' in os.getenv('DATABASE_URL', '') else None,
            password=os.getenv('PGPASSWORD') or os.getenv('DATABASE_URL', '').split('@')[0].split(':')[-1] if '@' in os.getenv('DATABASE_URL', '') else None,
        )
        
        # ä½¿ç”¨ SQLAlchemy å‰µå»ºé€£æ¥ä»¥é¿å…è­¦å‘Š
        from sqlalchemy import create_engine
        # å¾ psycopg2 é€£æ¥ç²å–é€£æ¥å­—ç¬¦ä¸²
        dsn = conn.get_dsn_parameters()
        connection_string = f"postgresql://{dsn.get('user')}:{dsn.get('password', '')}@{dsn.get('host')}:{dsn.get('port', 5432)}/{dsn.get('dbname')}"
        engine = create_engine(connection_string)
        
        query = """
            SELECT date as Date, patient_count as Attendance
            FROM actual_data
            ORDER BY date ASC
        """
        df = pd.read_sql_query(query, engine)
        
        # åŠ è¼‰ AI å› å­æ•¸æ“šï¼ˆä½¿ç”¨åŸå§‹é€£æ¥ï¼Œå› ç‚º load_ai_factors_from_db æœƒå‰µå»ºè‡ªå·±çš„ engineï¼‰
        ai_factors = load_ai_factors_from_db(conn)
        if ai_factors:
            print(f"âœ… åŠ è¼‰äº† {len(ai_factors)} å€‹æ—¥æœŸçš„ AI å› å­æ•¸æ“š")
        else:
            print("â„¹ï¸ æ²’æœ‰æ‰¾åˆ° AI å› å­æ•¸æ“šï¼Œå°‡ä½¿ç”¨é»˜èªå€¼")
        
        conn.close()
        
        # ç¢ºä¿åˆ—åæ­£ç¢ºï¼ˆpandas å¯èƒ½æœƒå°‡åˆ—åè½‰ç‚ºå°å¯«ï¼‰
        # æª¢æŸ¥ä¸¦æ˜ å°„ Date åˆ—
        if 'date' in df.columns and 'Date' not in df.columns:
            df['Date'] = df['date']
            df = df.drop(columns=['date'])
        elif 'Date' not in df.columns:
            print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° Date åˆ—ã€‚å¯ç”¨åˆ—: {df.columns.tolist()}")
            return None
        
        # æª¢æŸ¥ä¸¦æ˜ å°„ Attendance åˆ—ï¼ˆå¯èƒ½æ˜¯ attendance æˆ– patient_countï¼‰
        if 'attendance' in df.columns and 'Attendance' not in df.columns:
            df['Attendance'] = df['attendance']
            df = df.drop(columns=['attendance'])
        elif 'patient_count' in df.columns and 'Attendance' not in df.columns:
            df['Attendance'] = df['patient_count']
            df = df.drop(columns=['patient_count'])
        elif 'Attendance' not in df.columns:
            print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° Attendance åˆ—ã€‚å¯ç”¨åˆ—: {df.columns.tolist()}")
            return None
        
        # ç¢ºä¿åªè¿”å›éœ€è¦çš„åˆ—å’Œ AI å› å­
        if 'Date' in df.columns and 'Attendance' in df.columns:
            # å°‡ AI å› å­é™„åŠ åˆ° DataFrameï¼ˆä½œç‚ºå…ƒæ•¸æ“šï¼Œç¨å¾Œåœ¨ç‰¹å¾µå·¥ç¨‹ä¸­ä½¿ç”¨ï¼‰
            df_with_ai = df[['Date', 'Attendance']].copy()
            df_with_ai.attrs['ai_factors'] = ai_factors
            return df_with_ai
        else:
            print(f"è­¦å‘Š: æ•¸æ“šåˆ—ä¸å®Œæ•´ã€‚å¯ç”¨åˆ—: {df.columns.tolist()}")
            df.attrs['ai_factors'] = ai_factors
            return df
    except Exception as e:
        print(f"ç„¡æ³•å¾æ•¸æ“šåº«åŠ è¼‰æ•¸æ“š: {e}")
        return None

def load_data_from_csv(csv_path):
    """å¾ CSV æ–‡ä»¶åŠ è¼‰æ•¸æ“š"""
    try:
        df = pd.read_csv(csv_path)
        # è™•ç†ä¸åŒçš„åˆ—åæ ¼å¼
        if 'Date' not in df.columns:
            if 'date' in df.columns:
                df['Date'] = df['date']
            elif 'Date' in df.columns:
                df['Date'] = df['Date']
        if 'Attendance' not in df.columns:
            if 'patient_count' in df.columns:
                df['Attendance'] = df['patient_count']
            elif 'Attendance' in df.columns:
                df['Attendance'] = df['Attendance']
        
        return df[['Date', 'Attendance']]
    except Exception as e:
        print(f"ç„¡æ³•å¾ CSV åŠ è¼‰æ•¸æ“š: {e}")
        return None

def time_series_cross_validate(df, feature_cols, n_splits=5):
    """
    æ™‚é–“åºåˆ—äº¤å‰é©—è­‰ (Walk-Forward Validation)
    
    ç¢ºä¿æ¨¡å‹åœ¨è¨“ç·´æœŸé–“æ°¸é ä¸æœƒçœ‹åˆ°æœªä¾†æ•¸æ“šï¼š
    - æ¯å€‹ fold åªä½¿ç”¨éå»çš„æ•¸æ“šé€²è¡Œè¨“ç·´
    - é©—è­‰é›†ç¸½æ˜¯åœ¨è¨“ç·´é›†ä¹‹å¾Œçš„æ™‚é–“æ®µ
    - æœ€çµ‚æ¸¬è©¦é›†å®Œå…¨ç¨ç«‹ï¼Œå¾æœªåƒèˆ‡ä»»ä½•è¨“ç·´éç¨‹
    """
    print(f"\n{'='*60}")
    print("ğŸ”„ æ™‚é–“åºåˆ—äº¤å‰é©—è­‰ (Walk-Forward Validation)")
    print(f"{'='*60}")
    print(f"âš ï¸ é‡è¦ï¼šç¢ºä¿æ¨¡å‹ç„¡æ³•è¨ªå•æœªä¾†æ•¸æ“šï¼")
    print(f"ğŸ“Š äº¤å‰é©—è­‰æŠ˜æ•¸: {n_splits}")
    
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    cv_scores = {'mae': [], 'rmse': [], 'mape': []}
    
    # XGBoost åŸç”Ÿæ”¯æŒ NaN è™•ç†ï¼Œä¸éœ€è¦å¡«å……
    X = df[feature_cols]
    y = df['Attendance']
    dates = df['Date'].values
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        # ç²å–è¨“ç·´å’Œé©—è­‰çš„æ—¥æœŸç¯„åœ
        train_dates = dates[train_idx]
        val_dates = dates[val_idx]
        
        # é©—è­‰ï¼šç¢ºä¿é©—è­‰é›†æ—¥æœŸéƒ½åœ¨è¨“ç·´é›†æ—¥æœŸä¹‹å¾Œ
        train_max = pd.to_datetime(train_dates).max()
        val_min = pd.to_datetime(val_dates).min()
        
        if val_min <= train_max:
            print(f"âŒ Fold {fold+1}: æ•¸æ“šæ´©æ¼ï¼é©—è­‰é›†åŒ…å«è¨“ç·´æœŸé–“çš„æ•¸æ“š")
            continue
        
        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]
        
        print(f"\nğŸ“‚ Fold {fold+1}/{n_splits}:")
        print(f"   è¨“ç·´é›†: {len(train_idx)} ç­† ({train_dates[0]} è‡³ {train_dates[-1]})")
        print(f"   é©—è­‰é›†: {len(val_idx)} ç­† ({val_dates[0]} è‡³ {val_dates[-1]})")
        print(f"   âœ… æ™‚é–“é †åºé©—è­‰é€šéï¼šé©—è­‰é›†é–‹å§‹æ—¥æœŸ > è¨“ç·´é›†çµæŸæ—¥æœŸ")
        
        # å‰µå»ºæ¨¡å‹ï¼ˆä¸ä½¿ç”¨ early stopping ä»¥é¿å…éœ€è¦é¡å¤–é©—è­‰é›†ï¼‰
        model = xgb.XGBRegressor(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            objective='reg:squarederror',
            alpha=1.0,
            reg_lambda=1.0,
            random_state=42,
            n_jobs=-1
        )
        
        model.fit(X_train_cv, y_train_cv, verbose=False)
        
        y_pred_cv = model.predict(X_val_cv)
        
        mae = mean_absolute_error(y_val_cv, y_pred_cv)
        rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred_cv))
        mape = np.mean(np.abs((y_val_cv - y_pred_cv) / y_val_cv)) * 100
        
        cv_scores['mae'].append(mae)
        cv_scores['rmse'].append(rmse)
        cv_scores['mape'].append(mape)
        
        print(f"   ğŸ“ˆ MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
    
    # è¨ˆç®—å¹³å‡åˆ†æ•¸
    avg_scores = {
        'cv_mae_mean': np.mean(cv_scores['mae']),
        'cv_mae_std': np.std(cv_scores['mae']),
        'cv_rmse_mean': np.mean(cv_scores['rmse']),
        'cv_rmse_std': np.std(cv_scores['rmse']),
        'cv_mape_mean': np.mean(cv_scores['mape']),
        'cv_mape_std': np.std(cv_scores['mape']),
    }
    
    print(f"\n{'='*60}")
    print("ğŸ“Š äº¤å‰é©—è­‰çµæœç¸½çµ:")
    print(f"{'='*60}")
    print(f"   MAE:  {avg_scores['cv_mae_mean']:.2f} Â± {avg_scores['cv_mae_std']:.2f} ç—…äºº")
    print(f"   RMSE: {avg_scores['cv_rmse_mean']:.2f} Â± {avg_scores['cv_rmse_std']:.2f} ç—…äºº")
    print(f"   MAPE: {avg_scores['cv_mape_mean']:.2f} Â± {avg_scores['cv_mape_std']:.2f}%")
    
    return avg_scores


def train_xgboost_model(train_data, test_data, feature_cols):
    """
    è¨“ç·´ XGBoost æ¨¡å‹ï¼ˆä½¿ç”¨æ­£ç¢ºçš„æ™‚é–“åºåˆ—é©—è­‰ï¼‰
    
    é—œéµï¼šEarly stopping ä½¿ç”¨è¨“ç·´é›†å…§çš„é©—è­‰é›†ï¼Œè€Œéæ¸¬è©¦é›†ï¼
    é€™æ¨£ç¢ºä¿æ¸¬è©¦é›†åœ¨æ•´å€‹è¨“ç·´éç¨‹ä¸­å®Œå…¨æœªè¢«æ¨¡å‹çœ‹åˆ°ã€‚
    """
    print(f"\nğŸ“Š é–‹å§‹è¨“ç·´ XGBoost æ¨¡å‹...")
    print(f"è¨“ç·´é›†å¤§å°: {len(train_data)} ç­†")
    print(f"æ¸¬è©¦é›†å¤§å°: {len(test_data)} ç­† (å®Œå…¨éš”é›¢ï¼Œæœªåƒèˆ‡è¨“ç·´)")
    print(f"ç‰¹å¾µæ•¸é‡: {len(feature_cols)} å€‹")
    
    # å¾è¨“ç·´é›†ä¸­åˆ†å‡ºä¸€éƒ¨åˆ†ä½œç‚º early stopping é©—è­‰é›†
    # ä½¿ç”¨è¨“ç·´é›†çš„æœ€å¾Œ 15% ä½œç‚ºé©—è­‰é›†ï¼ˆä¿æŒæ™‚é–“é †åºï¼‰
    val_split_idx = int(len(train_data) * 0.85)
    train_subset = train_data[:val_split_idx].copy()
    val_subset = train_data[val_split_idx:].copy()
    
    # XGBoost åŸç”Ÿæ”¯æŒ NaN è™•ç†ï¼Œä¸éœ€è¦å¡«å……
    X_train = train_subset[feature_cols]
    y_train = train_subset['Attendance']
    X_val = val_subset[feature_cols]
    y_val = val_subset['Attendance']
    X_test = test_data[feature_cols]
    y_test = test_data['Attendance']
    
    print(f"\nâš ï¸ æ™‚é–“åºåˆ—æ•¸æ“šåˆ†å‰²é©—è­‰:")
    print(f"   è¨“ç·´å­é›†: {len(train_subset)} ç­† ({train_subset['Date'].min()} è‡³ {train_subset['Date'].max()})")
    print(f"   é©—è­‰å­é›†: {len(val_subset)} ç­† ({val_subset['Date'].min()} è‡³ {val_subset['Date'].max()})")
    print(f"   æ¸¬è©¦é›†:   {len(test_data)} ç­† ({test_data['Date'].min()} è‡³ {test_data['Date'].max()})")
    
    # é©—è­‰æ™‚é–“é †åº
    train_max_date = pd.to_datetime(train_subset['Date']).max()
    val_min_date = pd.to_datetime(val_subset['Date']).min()
    test_min_date = pd.to_datetime(test_data['Date']).min()
    val_max_date = pd.to_datetime(val_subset['Date']).max()
    
    if val_min_date > train_max_date:
        print(f"   âœ… é©—è­‰é›†æ—¥æœŸ > è¨“ç·´é›†æ—¥æœŸ (ç„¡æ•¸æ“šæ´©æ¼)")
    else:
        print(f"   âŒ è­¦å‘Šï¼šé©—è­‰é›†å¯èƒ½åŒ…å«è¨“ç·´æœŸé–“çš„æ•¸æ“šï¼")
    
    if test_min_date > val_max_date:
        print(f"   âœ… æ¸¬è©¦é›†æ—¥æœŸ > é©—è­‰é›†æ—¥æœŸ (ç„¡æ•¸æ“šæ´©æ¼)")
    else:
        print(f"   âŒ è­¦å‘Šï¼šæ¸¬è©¦é›†å¯èƒ½åŒ…å«é©—è­‰æœŸé–“çš„æ•¸æ“šï¼")
    
    print(f"\nè¨“ç·´é›†ç›®æ¨™å€¼ç¯„åœ: {y_train.min():.1f} - {y_train.max():.1f} ç—…äºº (å¹³å‡: {y_train.mean():.1f})")
    print(f"é©—è­‰é›†ç›®æ¨™å€¼ç¯„åœ: {y_val.min():.1f} - {y_val.max():.1f} ç—…äºº (å¹³å‡: {y_val.mean():.1f})")
    print(f"æ¸¬è©¦é›†ç›®æ¨™å€¼ç¯„åœ: {y_test.min():.1f} - {y_test.max():.1f} ç—…äºº (å¹³å‡: {y_test.mean():.1f})")
    
    # å‰µå»ºè‡ªå®šç¾© XGBoost é¡ä»¥ä¿®å¾© _estimator_type éŒ¯èª¤
    class XGBoostModel(xgb.XGBRegressor):
        _estimator_type = "regressor"
    
    # æ ¹æ“šç®—æ³•è¦æ ¼æ–‡ä»¶é…ç½®
    print(f"\nğŸ”§ æ¨¡å‹åƒæ•¸é…ç½®:")
    print(f"  n_estimators (æ¨¹çš„æ•¸é‡): 500")
    print(f"  max_depth (æœ€å¤§æ·±åº¦): 6")
    print(f"  learning_rate (å­¸ç¿’ç‡): 0.05")
    print(f"  subsample (æ¨£æœ¬æ¡æ¨£ç‡): 0.8")
    print(f"  colsample_bytree (ç‰¹å¾µæ¡æ¨£ç‡): 0.8")
    print(f"  colsample_bylevel (å±¤ç´šç‰¹å¾µæ¡æ¨£ç‡): 0.8")
    print(f"  objective (ç›®æ¨™å‡½æ•¸): reg:squarederror (å‡æ–¹èª¤å·®)")
    print(f"  alpha (L1 æ­£å‰‡åŒ–): 1.0")
    print(f"  reg_lambda (L2 æ­£å‰‡åŒ–): 1.0")
    print(f"  tree_method (æ¨¹æ§‹å»ºæ–¹æ³•): hist (ç›´æ–¹åœ–)")
    print(f"  grow_policy (ç”Ÿé•·ç­–ç•¥): depthwise (æ·±åº¦å„ªå…ˆ)")
    print(f"  early_stopping_rounds (æ—©åœè¼ªæ•¸): 50")
    print(f"  eval_metric (è©•ä¼°æŒ‡æ¨™): mae (å¹³å‡çµ•å°èª¤å·®)")
    print(f"  random_state (éš¨æ©Ÿç¨®å­): 42")
    
    model = XGBoostModel(
        n_estimators=500,
        max_depth=6,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        colsample_bylevel=0.8,
        objective='reg:squarederror',
        alpha=1.0,
        reg_lambda=1.0,
        tree_method='hist',
        grow_policy='depthwise',
        early_stopping_rounds=50,
        eval_metric='mae',
        random_state=42,
        n_jobs=-1
    )
    
    print(f"\nğŸš€ é–‹å§‹æ¨¡å‹è¨“ç·´ (æ¢¯åº¦æå‡éç¨‹)...")
    print(f"âš ï¸ Early stopping ä½¿ç”¨è¨“ç·´é›†å…§çš„é©—è­‰å­é›†ï¼Œéæ¸¬è©¦é›†ï¼")
    import time
    fit_start = time.time()
    
    # ä½¿ç”¨é©—è­‰å­é›†é€²è¡Œ early stoppingï¼Œè€Œéæ¸¬è©¦é›†
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],  # ä½¿ç”¨è¨“ç·´é›†å…§çš„é©—è­‰å­é›†
        verbose=False
    )
    
    fit_time = time.time() - fit_start
    print(f"è¨“ç·´å®Œæˆï¼Œè€—æ™‚: {fit_time:.2f} ç§’")
    print(f"å¯¦éš›è¨“ç·´è¼ªæ•¸: {model.best_iteration + 1 if hasattr(model, 'best_iteration') else model.n_estimators} è¼ª")
    
    # åœ¨å®Œå…¨æœªè¦‹éçš„æ¸¬è©¦é›†ä¸Šè©•ä¼°
    print(f"\nğŸ“ˆ é–‹å§‹æ¨¡å‹è©•ä¼° (åœ¨å®Œå…¨æœªè¦‹éçš„æ¸¬è©¦é›†ä¸Š)...")
    y_pred = model.predict(X_test)
    
    # è¨ˆç®—å„ç¨®èª¤å·®æŒ‡æ¨™
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    
    # è¨ˆç®—å…¶ä»–çµ±è¨ˆæŒ‡æ¨™
    mean_error = np.mean(y_pred - y_test)
    std_error = np.std(y_pred - y_test)
    r2_score = 1 - (np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))
    
    print(f"\nXGBoost æ¨¡å‹æ€§èƒ½æŒ‡æ¨™ (æ¸¬è©¦é›† - å®Œå…¨ç¨ç«‹):")
    print(f"  MAE (å¹³å‡çµ•å°èª¤å·®): {mae:.2f} ç—…äºº")
    print(f"  RMSE (å‡æ–¹æ ¹èª¤å·®): {rmse:.2f} ç—…äºº")
    print(f"  MAPE (å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·®): {mape:.2f}%")
    print(f"  å¹³å‡èª¤å·®: {mean_error:.2f} ç—…äºº")
    print(f"  èª¤å·®æ¨™æº–å·®: {std_error:.2f} ç—…äºº")
    print(f"  RÂ² å¾—åˆ†: {r2_score:.4f}")
    print(f"  é æ¸¬å€¼ç¯„åœ: {y_pred.min():.1f} - {y_pred.max():.1f} ç—…äºº")
    
    return model, {'mae': mae, 'rmse': rmse, 'mape': mape}

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Train XGBoost model')
    parser.add_argument('--csv', type=str, help='Path to CSV file with historical data')
    args = parser.parse_args()
    
    # å‰µå»ºæ¨¡å‹ç›®éŒ„ï¼ˆç›¸å°æ–¼ç•¶å‰è…³æœ¬ç›®éŒ„ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(script_dir, 'models')
    os.makedirs(models_dir, exist_ok=True)
    print(f"æ¨¡å‹ç›®éŒ„: {models_dir}")
    
    df = None
    
    # å„ªå…ˆä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„ CSV æ–‡ä»¶
    if args.csv and os.path.exists(args.csv):
        print(f"å¾å‘½ä»¤è¡ŒæŒ‡å®šçš„ CSV åŠ è¼‰æ•¸æ“š: {args.csv}")
        df = load_data_from_csv(args.csv)
    
    # å¦‚æœæ²’æœ‰æŒ‡å®š CSVï¼Œå˜—è©¦å¾æ•¸æ“šåº«åŠ è¼‰æ•¸æ“š
    if df is None or len(df) == 0:
        df = load_data_from_db()
    
    # å¦‚æœæ•¸æ“šåº«ä¸å¯ç”¨ï¼Œå˜—è©¦å¾é»˜èª CSV åŠ è¼‰
    if df is None or len(df) == 0:
        csv_paths = [
            '../NDH_AED_Clean.csv',
            'NDH_AED_Clean.csv',
            '../NDH_AED_Attendance_2025-12-01_to_2025-12-21.csv',
            'NDH_AED_Attendance_2025-12-01_to_2025-12-21.csv',
        ]
        for csv_path in csv_paths:
            if os.path.exists(csv_path):
                df = load_data_from_csv(csv_path)
                if df is not None and len(df) > 0:
                    break
    
    if df is None or len(df) == 0:
        print("éŒ¯èª¤: ç„¡æ³•åŠ è¼‰æ•¸æ“š")
        sys.exit(1)
    
    print(f"åŠ è¼‰äº† {len(df)} ç­†æ•¸æ“š")
    
    # ç²å– AI å› å­æ•¸æ“šï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    ai_factors = df.attrs.get('ai_factors', {}) if hasattr(df, 'attrs') else {}
    
    # å¦‚æœæ²’æœ‰å¾æ•¸æ“šåº«ç²å– AI å› å­ï¼Œå˜—è©¦å¾æœ¬åœ° JSON æ–‡ä»¶åŠ è¼‰
    if not ai_factors:
        ai_factors_path = os.path.join(models_dir, 'ai_factors.json')
        if os.path.exists(ai_factors_path):
            try:
                with open(ai_factors_path, 'r', encoding='utf-8') as f:
                    ai_factors = json.load(f)
                print(f"âœ… å¾æœ¬åœ°æ–‡ä»¶åŠ è¼‰äº† {len(ai_factors)} å€‹æ—¥æœŸçš„ AI å› å­æ•¸æ“š")
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•å¾æœ¬åœ°æ–‡ä»¶åŠ è¼‰ AI å› å­: {e}")
    
    if ai_factors:
        print(f"âœ… åŠ è¼‰äº† {len(ai_factors)} å€‹æ—¥æœŸçš„ AI å› å­æ•¸æ“š")
    else:
        print(f"â„¹ï¸ æ²’æœ‰æ‰¾åˆ° AI å› å­æ•¸æ“šï¼Œå°‡ä½¿ç”¨é»˜èªå€¼")
    
    # å‰µå»ºç‰¹å¾µï¼ˆåŒ…å« AI å› å­ï¼‰
    print(f"\nğŸ”¨ é–‹å§‹ç‰¹å¾µå·¥ç¨‹ (Feature Engineering)...")
    print(f"åŸå§‹æ•¸æ“šåˆ—æ•¸: {len(df.columns)}")
    df = create_comprehensive_features(df, ai_factors_dict=ai_factors if ai_factors else None)
    print(f"ç‰¹å¾µå·¥ç¨‹å¾Œåˆ—æ•¸: {len(df.columns)}")
    
    # ç§»é™¤åŒ…å« NaN çš„è¡Œï¼ˆé™¤äº†æˆ‘å€‘å·²ç¶“å¡«å……çš„åˆ—ï¼‰
    original_len = len(df)
    df = df.dropna(subset=['Attendance'])
    if len(df) < original_len:
        print(f"ç§»é™¤äº† {original_len - len(df)} ç­†åŒ…å« NaN çš„æ•¸æ“š")
    
    # æ™‚é–“åºåˆ—åˆ†å‰²ï¼ˆä¸èƒ½éš¨æ©Ÿåˆ†å‰²ï¼ï¼‰
    print(f"\nâœ‚ï¸ æ•¸æ“šåˆ†å‰² (Time Series Split)...")
    split_idx = int(len(df) * 0.8)
    print(f"åˆ†å‰²é»ç´¢å¼•: {split_idx} (80% è¨“ç·´, 20% æ¸¬è©¦)")
    train_data = df[:split_idx].copy()
    test_data = df[split_idx:].copy()
    
    print(f"è¨“ç·´é›†: {len(train_data)} ç­† (æ—¥æœŸç¯„åœ: {train_data['Date'].min()} è‡³ {train_data['Date'].max()})")
    print(f"æ¸¬è©¦é›†: {len(test_data)} ç­† (æ—¥æœŸç¯„åœ: {test_data['Date'].min()} è‡³ {test_data['Date'].max()})")
    
    # ç²å–ç‰¹å¾µåˆ—
    feature_cols = get_feature_columns()
    # åªä¿ç•™å¯¦éš›å­˜åœ¨çš„åˆ—
    original_feature_count = len(feature_cols)
    feature_cols = [col for col in feature_cols if col in df.columns]
    if len(feature_cols) < original_feature_count:
        print(f"âš ï¸ è­¦å‘Š: {original_feature_count - len(feature_cols)} å€‹é æœŸç‰¹å¾µåœ¨æ•¸æ“šä¸­ä¸å­˜åœ¨")
    
    print(f"ä½¿ç”¨ {len(feature_cols)} å€‹ç‰¹å¾µé€²è¡Œè¨“ç·´")
    
    # æ™‚é–“åºåˆ—äº¤å‰é©—è­‰ï¼ˆç¢ºä¿ç„¡æ•¸æ“šæ´©æ¼ï¼‰
    cv_scores = time_series_cross_validate(train_data, feature_cols, n_splits=5)
    
    # è¨“ç·´æœ€çµ‚æ¨¡å‹
    model, metrics = train_xgboost_model(train_data, test_data, feature_cols)
    
    # ä¿å­˜æ¨¡å‹ï¼ˆä½¿ç”¨çµ•å°è·¯å¾‘ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(script_dir, 'models')
    
    # åŠ è¼‰èˆŠæ¨¡å‹æŒ‡æ¨™ï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰
    metrics_path = os.path.join(models_dir, 'xgboost_metrics.json')
    old_metrics = None
    if os.path.exists(metrics_path):
        try:
            with open(metrics_path, 'r') as f:
                old_metrics = json.load(f)
        except:
            old_metrics = None
    
    model_path = os.path.join(models_dir, 'xgboost_model.json')
    model.save_model(model_path)
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
    
    # ä¿å­˜ç‰¹å¾µåˆ—å
    features_path = os.path.join(models_dir, 'xgboost_features.json')
    with open(features_path, 'w') as f:
        json.dump(feature_cols, f)
    
    # æ·»åŠ æ›´å¤šè¨“ç·´ä¿¡æ¯åˆ°æŒ‡æ¨™
    import datetime
    training_info = {
        'mae': metrics['mae'],
        'rmse': metrics['rmse'],
        'mape': metrics['mape'],
        'training_date': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_count': len(df),
        'train_count': len(train_data),
        'test_count': len(test_data),
        'feature_count': len(feature_cols),
        'ai_factors_count': len(ai_factors) if ai_factors else 0,
        # äº¤å‰é©—è­‰åˆ†æ•¸ï¼ˆç¢ºä¿ç„¡æœªä¾†æ•¸æ“šæ´©æ¼ï¼‰
        'cv_mae_mean': cv_scores['cv_mae_mean'],
        'cv_mae_std': cv_scores['cv_mae_std'],
        'cv_rmse_mean': cv_scores['cv_rmse_mean'],
        'cv_rmse_std': cv_scores['cv_rmse_std'],
        'cv_mape_mean': cv_scores['cv_mape_mean'],
        'cv_mape_std': cv_scores['cv_mape_std'],
        'time_series_validation': True  # æ¨™è¨˜ä½¿ç”¨äº†æ­£ç¢ºçš„æ™‚é–“åºåˆ—é©—è­‰
    }
    
    # ä¿å­˜è©•ä¼°æŒ‡æ¨™
    with open(metrics_path, 'w') as f:
        json.dump(training_info, f, indent=2)
    
    # è¨ˆç®—ç‰¹å¾µé‡è¦æ€§
    print(f"\n{'='*60}")
    print("ğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ (Top 15 æœ€é‡è¦ç‰¹å¾µ):")
    print(f"{'='*60}")
    
    importance = model.feature_importances_
    feature_importance = list(zip(feature_cols, importance))
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    
    for i, (feat, imp) in enumerate(feature_importance[:15]):
        bar_length = int(imp / max(importance) * 30)
        bar = "â–ˆ" * bar_length + "â–‘" * (30 - bar_length)
        print(f"  {i+1:2}. {feat:25} {bar} {imp:.4f}")
    
    # é¡¯ç¤ºè¨“ç·´å‰å¾Œå°æ¯”
    print(f"\n{'='*60}")
    print("ğŸ“ˆ æ¨¡å‹æ€§èƒ½è®ŠåŒ–:")
    print(f"{'='*60}")
    
    if old_metrics:
        old_mae = old_metrics.get('mae', 0)
        old_rmse = old_metrics.get('rmse', 0)
        old_mape = old_metrics.get('mape', 0)
        
        mae_change = metrics['mae'] - old_mae
        rmse_change = metrics['rmse'] - old_rmse
        mape_change = metrics['mape'] - old_mape
        
        mae_icon = "âœ… æ”¹å–„" if mae_change < 0 else ("âš ï¸ ä¸‹é™" if mae_change > 0 else "â¡ï¸ ç„¡è®ŠåŒ–")
        rmse_icon = "âœ… æ”¹å–„" if rmse_change < 0 else ("âš ï¸ ä¸‹é™" if rmse_change > 0 else "â¡ï¸ ç„¡è®ŠåŒ–")
        mape_icon = "âœ… æ”¹å–„" if mape_change < 0 else ("âš ï¸ ä¸‹é™" if mape_change > 0 else "â¡ï¸ ç„¡è®ŠåŒ–")
        
        print(f"\n  ğŸ“Š MAE (å¹³å‡çµ•å°èª¤å·®):")
        print(f"     èˆŠæ¨¡å‹: {old_mae:.2f} ç—…äºº")
        print(f"     æ–°æ¨¡å‹: {metrics['mae']:.2f} ç—…äºº")
        print(f"     è®ŠåŒ–: {mae_change:+.2f} ç—…äºº {mae_icon}")
        
        print(f"\n  ğŸ“Š RMSE (å‡æ–¹æ ¹èª¤å·®):")
        print(f"     èˆŠæ¨¡å‹: {old_rmse:.2f} ç—…äºº")
        print(f"     æ–°æ¨¡å‹: {metrics['rmse']:.2f} ç—…äºº")
        print(f"     è®ŠåŒ–: {rmse_change:+.2f} ç—…äºº {rmse_icon}")
        
        print(f"\n  ğŸ“Š MAPE (å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·®):")
        print(f"     èˆŠæ¨¡å‹: {old_mape:.2f}%")
        print(f"     æ–°æ¨¡å‹: {metrics['mape']:.2f}%")
        print(f"     è®ŠåŒ–: {mape_change:+.2f}% {mape_icon}")
        
        # è¨ˆç®—ç¸½é«”æ”¹å–„
        improvements = sum([1 for c in [mae_change, rmse_change, mape_change] if c < 0])
        degradations = sum([1 for c in [mae_change, rmse_change, mape_change] if c > 0])
        
        print(f"\n  ğŸ“‹ ç¸½çµ:")
        if improvements > degradations:
            print(f"     ğŸ‰ æ¨¡å‹æ•´é«”æ€§èƒ½æå‡ï¼({improvements}/3 æŒ‡æ¨™æ”¹å–„)")
        elif degradations > improvements:
            print(f"     âš ï¸ æ¨¡å‹æ•´é«”æ€§èƒ½ä¸‹é™ ({degradations}/3 æŒ‡æ¨™ä¸‹é™)")
            print(f"     ğŸ’¡ å»ºè­°ï¼šæª¢æŸ¥æ–°æ•¸æ“šè³ªé‡æˆ–å¢åŠ è¨“ç·´æ•¸æ“š")
        else:
            print(f"     â¡ï¸ æ¨¡å‹æ€§èƒ½ç¶­æŒç©©å®š")
    else:
        print(f"\n  â„¹ï¸ é€™æ˜¯é¦–æ¬¡è¨“ç·´ï¼Œç„¡èˆŠæ¨¡å‹å¯æ¯”è¼ƒ")
        print(f"\n  ğŸ“Š ç•¶å‰æ¨¡å‹æ€§èƒ½:")
        print(f"     MAE: {metrics['mae']:.2f} ç—…äºº")
        print(f"     RMSE: {metrics['rmse']:.2f} ç—…äºº")
        print(f"     MAPE: {metrics['mape']:.2f}%")
    
    # è¨“ç·´ç¸½çµ
    print(f"\n{'='*60}")
    print("ğŸ¯ è¨“ç·´ç¸½çµ:")
    print(f"{'='*60}")
    print(f"  ğŸ“… è¨“ç·´æ™‚é–“: {training_info['training_date']}")
    print(f"  ğŸ“Š æ•¸æ“šé‡: {training_info['data_count']} ç­†")
    print(f"  ğŸ”§ ç‰¹å¾µæ•¸: {training_info['feature_count']} å€‹")
    if training_info['ai_factors_count'] > 0:
        print(f"  ğŸ¤– AIå› å­: {training_info['ai_factors_count']} å€‹æ—¥æœŸ")
    print(f"  ğŸ“ˆ MAE: {metrics['mae']:.2f} ç—…äºº")
    print(f"  ğŸ“ˆ RMSE: {metrics['rmse']:.2f} ç—…äºº")
    print(f"  ğŸ“ˆ MAPE: {metrics['mape']:.2f}%")
    print(f"{'='*60}")
    
    print("\nâœ… XGBoost æ¨¡å‹è¨“ç·´å®Œæˆï¼")

if __name__ == '__main__':
    main()

