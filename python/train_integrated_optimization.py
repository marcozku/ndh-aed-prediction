# -*- coding: utf-8 -*-
"""
æ•´åˆæ‰€æœ‰å„ªåŒ–ç‰¹å¾µçš„è¨“ç·´è…³æœ¬
çµåˆ: å¤©æ°£é å ± + æ­·å²å¤©æ°£æ»¯å¾Œ + æµæ„Ÿå­£ç¯€

åŸºæº–: MAE = 15.73 (Ensemble + å¤©æ°£ + æ’é™¤ COVID)
ç›®æ¨™: MAE = 14.0-14.5 (8-11% æ”¹å–„)
"""
import sys
import io

if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import psycopg2
import psycopg2.extras
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
import json
import os

# å°å…¥ç‰¹å¾µæ¨¡çµ„
from weather_forecast_integration import (
    fetch_weather_forecast,
    add_forecast_features_to_df,
    get_forecast_feature_list
)
from flu_season_features import (
    add_flu_features_to_df,
    get_flu_feature_list
)
from historical_weather_patterns import (
    add_historical_weather_pattern_features,
    get_historical_weather_feature_list
)

# æ•¸æ“šåº«é€£æ¥
DB_CONFIG = {
    'host': 'razzle.db.elephantsql.com',
    'database': 'ndh_aed',
    'user': 'ndh_aed',
    'password': 'B3IG7EYud_UMqfUNvEbi5XxO9xh5l8Pp',
    'port': 5432
}

# COVID å½±éŸ¿æœŸé–“
COVID_PERIODS = [
    ('2020-01-23', '2020-04-08'),  # ç¬¬ä¸€æ³¢
    ('2020-07-16', '2020-09-30'),  # ç¬¬ä¸‰æ³¢
    ('2020-11-23', '2021-01-05'),  # ç¬¬å››æ³¢
    ('2022-02-05', '2022-04-30'),  # ç¬¬äº”æ³¢
    ('2022-11-10', '2022-12-27'),  # æ”¾å¯¬å‰
]


def load_data_from_railway():
    """å¾ Railway æ•¸æ“šåº«åŠ è¼‰æ‰€æœ‰æ­·å²æ•¸æ“š"""
    try:
        print("ğŸ“¡ é€£æ¥ Railway æ•¸æ“šåº«...")

        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)

        # ç²å–æ‰€æœ‰å°±è¨ºæ•¸æ“š
        query = """
        SELECT date, patient_count
        FROM actual_data
        ORDER BY date ASC
        """

        cursor.execute(query)
        rows = cursor.fetchall()

        cursor.close()
        conn.close()

        if not rows:
            print("   âŒ æ²’æœ‰æ•¸æ“š")
            return None

        df = pd.DataFrame(rows)
        df['date'] = pd.to_datetime(df['date'])

        print(f"   âœ… æˆåŠŸåŠ è¼‰ {len(df)} ç­†è¨˜éŒ„")
        print(f"   ğŸ“… ç¯„åœ: {df['date'].min()} â†’ {df['date'].max()}")

        return df

    except Exception as e:
        print(f"   âŒ éŒ¯èª¤: {e}")
        return None


def load_weather_data():
    """åŠ è¼‰æ­·å²å¤©æ°£æ•¸æ“š"""
    weather_file = 'models/weather_full_history.csv'

    if not os.path.exists(weather_file):
        print(f"   âš ï¸ å¤©æ°£æ•¸æ“šä¸å­˜åœ¨: {weather_file}")
        return None

    try:
        weather_df = pd.read_csv(weather_file)
        weather_df['Date'] = pd.to_datetime(weather_df['Date'])

        print(f"   âœ… å¤©æ°£æ•¸æ“š: {len(weather_df)} ç­†")

        return weather_df

    except Exception as e:
        print(f"   âŒ éŒ¯èª¤: {e}")
        return None


def exclude_covid_periods(df, date_col='date'):
    """æ’é™¤ COVID å½±éŸ¿æœŸé–“"""
    df = df.copy()

    # æ¨™è¨˜ COVID æœŸé–“
    df['is_covid'] = 0
    for start, end in COVID_PERIODS:
        start_date = pd.to_datetime(start)
        end_date = pd.to_datetime(end)
        mask = (df[date_col] >= start_date) & (df[date_col] <= end_date)
        df.loc[mask, 'is_covid'] = 1

    # éæ¿¾
    df_filtered = df[df['is_covid'] == 0].copy()

    print(f"   ğŸ“Š éæ¿¾ COVID: {len(df)} â†’ {len(df_filtered)} ç­†")

    return df_filtered


def prepare_features(df, forecast_df=None):
    """
    æº–å‚™æ‰€æœ‰ç‰¹å¾µ

    ç‰¹å¾µçµ„åˆ:
    1. åŸºç¤ç‰¹å¾µ (æ™‚é–“ã€æ»¯å¾Œå°±è¨º)
    2. æ­·å²å¤©æ°£æ¨¡å¼ç‰¹å¾µ (å¤©æ°£è®ŠåŒ–ã€æ¥µç«¯å¤©æ°£ã€å¹´åº¦åŒæœŸ)
    3. æµæ„Ÿå­£ç¯€ç‰¹å¾µ
    4. å¤©æ°£é å ±ç‰¹å¾µ
    """
    print("\nğŸ“Š æº–å‚™ç‰¹å¾µ...")

    # 1. åŸºç¤ç‰¹å¾µ
    df = add_base_features(df)
    print("   âœ… åŸºç¤ç‰¹å¾µ")

    # 2. æµæ„Ÿå­£ç¯€ç‰¹å¾µ
    df = add_flu_features_to_df(df, date_col='Date')
    print("   âœ… æµæ„Ÿå­£ç¯€ç‰¹å¾µ")

    # 3. æ­·å²å¤©æ°£æ¨¡å¼ç‰¹å¾µ
    weather_df = load_weather_data()
    df = add_historical_weather_pattern_features(df, weather_df, df)

    # 4. å¤©æ°£é å ±ç‰¹å¾µ
    if forecast_df is not None:
        df = add_forecast_features_to_df(df, forecast_df, date_col='Date')
        print("   âœ… å¤©æ°£é å ±ç‰¹å¾µ")
    else:
        print("   âš ï¸ ç„¡å¤©æ°£é å ±æ•¸æ“š")

    # ç§»é™¤åŒ…å« NaN çš„è¡Œ
    before_len = len(df)
    df = df.dropna()
    after_len = len(df)

    if before_len > after_len:
        print(f"   ğŸ§¹ ç§»é™¤ NaN: {before_len} â†’ {after_len}")

    return df


def get_all_feature_lists():
    """ç²å–æ‰€æœ‰ç‰¹å¾µåˆ—è¡¨"""
    base_features = [
        'Day_of_Week', 'Month', 'Day_of_Month', 'Is_Weekend',
        'Holiday_Factor', 'Is_Winter_Flu_Season',
        'DayOfWeek_sin', 'DayOfWeek_cos',
        'Attendance_Lag1', 'Attendance_Lag7', 'Attendance_Lag30',
        'Attendance_EWMA7', 'Attendance_EWMA14',
        'Daily_Change', 'Weekly_Change'
    ]

    historical_weather_features = [
        'Weather_Rain_1d', 'Weather_Rain_2d', 'Weather_Rain_3d',
        'Weather_Mean_Temp_1d', 'Weather_Mean_Temp_2d', 'Weather_Mean_Temp_3d',
        'Weather_Cold_1d', 'Weather_Cold_2d', 'Weather_Cold_3d',
        'Weather_Hot_1d', 'Weather_Hot_2d', 'Weather_Hot_3d',
        'Weather_Cold_Spell_3d', 'Weather_Temp_Trend_3d', 'Weather_Rain_Accum_7d'
    ]

    flu_features = get_flu_feature_list()
    forecast_features = get_forecast_feature_list()

    return {
        'base': base_features,
        'historical_weather': historical_weather_features,
        'flu': flu_features,
        'forecast': forecast_features
    }


def train_models(X_train, y_train, X_test, y_test):
    """è¨“ç·´å¤šå€‹æ¨¡å‹ä¸¦è¿”å›é æ¸¬çµæœ"""
    print("\nğŸ¤– è¨“ç·´æ¨¡å‹...")

    models = {}
    predictions = {}

    # 1. XGBoost
    print("   è¨“ç·´ XGBoost...")
    models['xgboost'] = xgb.XGBRegressor(
        n_estimators=500,
        max_depth=6,
        learning_rate=0.05,
        min_child_weight=3,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )
    models['xgboost'].fit(X_train, y_train, verbose=False)
    predictions['xgboost'] = models['xgboost'].predict(X_test)

    # 2. Random Forest
    print("   è¨“ç·´ Random Forest...")
    models['random_forest'] = RandomForestRegressor(
        n_estimators=300,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    models['random_forest'].fit(X_train, y_train)
    predictions['random_forest'] = models['random_forest'].predict(X_test)

    # 3. Gradient Boosting
    print("   è¨“ç·´ Gradient Boosting...")
    models['gradient_boosting'] = GradientBoostingRegressor(
        n_estimators=300,
        max_depth=6,
        learning_rate=0.05,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )
    models['gradient_boosting'].fit(X_train, y_train)
    predictions['gradient_boosting'] = models['gradient_boosting'].predict(X_test)

    # 4. Ensemble (ç°¡å–®å¹³å‡)
    print("   è¨“ç·´ Ensemble...")
    predictions['ensemble'] = np.mean([
        predictions['xgboost'],
        predictions['random_forest'],
        predictions['gradient_boosting']
    ], axis=0)

    return models, predictions


def evaluate_predictions(y_true, predictions):
    """è©•ä¼°é æ¸¬çµæœ"""
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

    results = {}

    for model_name, y_pred in predictions.items():
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)

        results[model_name] = {
            'MAE': mae,
            'RMSE': rmse,
            'RÂ²': r2
        }

    return results


def main():
    """ä¸»è¨“ç·´æµç¨‹"""
    print("=" * 80)
    print("ğŸš€ æ•´åˆå„ªåŒ–ç‰¹å¾µè¨“ç·´")
    print("=" * 80)
    print(f"æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # 1. åŠ è¼‰æ•¸æ“š
    print("1ï¸âƒ£ åŠ è¼‰æ•¸æ“š...")
    df = load_data_from_railway()

    if df is None:
        return

    # 2. æ’é™¤ COVID æœŸé–“
    print("\n2ï¸âƒ£ æ’é™¤ COVID æœŸé–“...")
    df = exclude_covid_periods(df, date_col='date')

    # 3. ç²å–å¤©æ°£é å ±
    print("\n3ï¸âƒ£ ç²å–å¤©æ°£é å ±...")
    forecast_df = fetch_weather_forecast()

    if forecast_df is None or len(forecast_df) == 0:
        print("   âš ï¸ ç„¡æ³•ç²å–å¤©æ°£é å ±ï¼Œä½¿ç”¨ç¾æœ‰ç‰¹å¾µè¨“ç·´")
        forecast_df = None

    # 4. æº–å‚™ç‰¹å¾µ
    print("\n4ï¸âƒ£ æº–å‚™ç‰¹å¾µ...")
    df = df.rename(columns={'date': 'Date'})
    df = prepare_features(df, forecast_df)

    # 5. ç²å–ç‰¹å¾µåˆ—è¡¨
    print("\n5ï¸âƒ£ ç²å–ç‰¹å¾µåˆ—è¡¨...")
    feature_lists = get_all_feature_lists()

    all_features = (
        feature_lists['base'] +
        feature_lists['flu'] +
        feature_lists['historical_weather']
    )

    # åªæ·»åŠ å¯ç”¨çš„é å ±ç‰¹å¾µ
    if forecast_df is not None:
        available_forecast_features = [f for f in feature_lists['forecast'] if f in df.columns]
        all_features += available_forecast_features
        print(f"   ğŸ“Š å¤©æ°£é å ±ç‰¹å¾µ: {len(available_forecast_features)}")
    else:
        print(f"   âš ï¸ ç„¡å¤©æ°£é å ±ç‰¹å¾µ")

    print(f"   ğŸ“Š ç¸½ç‰¹å¾µæ•¸: {len(all_features)}")
    print(f"   ğŸ“‹ ç‰¹å¾µåˆ—è¡¨: {all_features[:10]}...")

    # 6. æ™‚é–“åºåˆ—åˆ†å‰²
    print("\n6ï¸âƒ£ æ™‚é–“åºåˆ—åˆ†å‰²...")
    train_size = int(len(df) * 0.8)

    train_df = df.iloc[:train_size].copy()
    test_df = df.iloc[train_size:].copy()

    print(f"   è¨“ç·´é›†: {len(train_df)} ç­†")
    print(f"   æ¸¬è©¦é›†: {len(test_df)} ç­†")
    print(f"   æ¸¬è©¦ç¯„åœ: {test_df['Date'].min()} â†’ {test_df['Date'].max()}")

    # 7. æº–å‚™ X, y
    print("\n7ï¸âƒ£ æº–å‚™è¨“ç·´æ•¸æ“š...")
    X_train = train_df[all_features]
    y_train = train_df['patient_count']
    X_test = test_df[all_features]
    y_test = test_df['patient_count']

    # 8. è¨“ç·´æ¨¡å‹
    print("\n8ï¸âƒ£ è¨“ç·´æ¨¡å‹...")
    models, predictions = train_models(X_train, y_train, X_test, y_test)

    # 9. è©•ä¼°çµæœ
    print("\n9ï¸âƒ£ è©•ä¼°çµæœ...")
    results = evaluate_predictions(y_test, predictions)

    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å‹è©•ä¼°çµæœ")
    print("=" * 80)

    # æŒ‰ MAE æ’åº
    sorted_results = sorted(results.items(), key=lambda x: x[1]['MAE'])

    for model_name, metrics in sorted_results:
        print(f"\n{model_name.upper()}:")
        print(f"   MAE:  {metrics['MAE']:.2f}")
        print(f"   RMSE: {metrics['RMSE']:.2f}")
        print(f"   RÂ²:   {metrics['RÂ²']:.4f}")

    # 10. è¨ˆç®—æ”¹å–„
    print("\n" + "=" * 80)
    print("ğŸ“ˆ æ”¹å–„åˆ†æ")
    print("=" * 80)

    baseline_mae = 15.73
    best_mae = sorted_results[0][1]['MAE']
    improvement = (baseline_mae - best_mae) / baseline_mae * 100

    print(f"\nåŸºæº– (èˆŠæ¨¡å‹): MAE = {baseline_mae}")
    print(f"æœ€ä½³ (æ–°æ¨¡å‹): MAE = {best_mae:.2f}")
    print(f"æ”¹å–„: {improvement:+.1f}%")

    if best_mae < baseline_mae:
        print(f"âœ… é”æˆç›®æ¨™ï¼æ”¹å–„ {improvement:.1f}%")
    else:
        print(f"âš ï¸ æœªé”åŸºæº–ï¼Œéœ€è¦èª¿æ•´")

    # 11. ä¿å­˜çµæœ
    print("\nğŸ’¾ ä¿å­˜çµæœ...")

    results_summary = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'baseline_mae': baseline_mae,
        'results': {k: {'MAE': float(v['MAE']), 'RMSE': float(v['RMSE']), 'RÂ²': float(v['RÂ²'])}
                    for k, v in results.items()},
        'improvement_pct': improvement,
        'feature_count': len(all_features),
        'train_size': len(train_df),
        'test_size': len(test_df)
    }

    os.makedirs('models', exist_ok=True)
    with open('models/integrated_optimization_results.json', 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, ensure_ascii=False, indent=2)

    print(f"   âœ… çµæœå·²ä¿å­˜åˆ° models/integrated_optimization_results.json")

    # 12. ç‰¹å¾µé‡è¦æ€§ (XGBoost)
    print("\nğŸ” ç‰¹å¾µé‡è¦æ€§ (Top 15)...")
    importances = models['xgboost'].feature_importances_
    indices = np.argsort(importances)[::-1]

    print(f"\n   {'æ’å':<4} {'ç‰¹å¾µ':<30} {'é‡è¦æ€§':<10}")
    print("   " + "-" * 50)

    for i in range(min(15, len(all_features))):
        idx = indices[i]
        print(f"   {i+1:<4} {all_features[idx]:<30} {importances[idx]:.4f}")

    print("\n" + "=" * 80)
    print("âœ… è¨“ç·´å®Œæˆ")
    print("=" * 80)


if __name__ == '__main__':
    main()
