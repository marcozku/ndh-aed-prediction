#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æå…©å€‹é—œéµå•é¡Œï¼š
1. è¨“ç·´/æ¸¬è©¦ MAE å·®è· (Concept Drift)
2. å¤©æ°£/AQHI ç‰¹å¾µæ˜¯å¦æ‡‰è©²ä¿ç•™
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_absolute_error
import os
import sys

sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

def load_data():
    paths = ['../NDH_AED_Clean.csv', 'NDH_AED_Clean.csv']
    for path in paths:
        if os.path.exists(path):
            df = pd.read_csv(path)
            df['Date'] = pd.to_datetime(df['date'] if 'date' in df.columns else df['Date'])
            df['Attendance'] = df['attendance'] if 'attendance' in df.columns else df['Attendance']
            return df[['Date', 'Attendance']].sort_values('Date').reset_index(drop=True)
    return None

def create_features(df):
    df = df.copy()
    df['DayOfWeek'] = df['Date'].dt.dayofweek
    df['Month'] = df['Date'].dt.month
    df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)
    
    # EWMA
    for span in [7, 14, 21, 30]:
        df[f'EWMA_{span}'] = df['Attendance'].shift(1).ewm(span=span).mean()
    
    # Lags
    for lag in [1, 7, 14, 30]:
        df[f'Lag_{lag}'] = df['Attendance'].shift(lag)
    
    # Rolling
    for w in [7, 14, 30]:
        df[f'Rolling_Mean_{w}'] = df['Attendance'].shift(1).rolling(w).mean()
        df[f'Rolling_Std_{w}'] = df['Attendance'].shift(1).rolling(w).std()
    
    # Simulated weather features (random for demonstration - in reality use actual data)
    np.random.seed(42)
    df['Temperature'] = 20 + 10 * np.sin(2 * np.pi * df['Month'] / 12) + np.random.randn(len(df)) * 3
    df['Humidity'] = 70 + 15 * np.sin(2 * np.pi * df['Month'] / 12) + np.random.randn(len(df)) * 10
    df['Rainfall'] = np.random.exponential(5, len(df))
    df['AQHI'] = 3 + np.random.poisson(2, len(df))
    
    # Weather extreme flags
    df['Is_Very_Hot'] = (df['Temperature'] > 32).astype(int)
    df['Is_Cold'] = (df['Temperature'] < 12).astype(int)
    df['Is_Heavy_Rain'] = (df['Rainfall'] > 25).astype(int)
    df['Is_High_AQHI'] = (df['AQHI'] >= 7).astype(int)
    
    return df.dropna()

def analyze_concept_drift(df):
    """åˆ†ææ™‚é–“æ®µå·®ç•° (Concept Drift)"""
    print("=" * 70)
    print("å•é¡Œ 1: è¨“ç·´/æ¸¬è©¦ MAE å·®è·åˆ†æ (Concept Drift)")
    print("=" * 70)
    
    # æŒ‰å¹´ä»½åˆ†çµ„çµ±è¨ˆ
    df['Year'] = df['Date'].dt.year
    yearly_stats = df.groupby('Year')['Attendance'].agg(['mean', 'std', 'min', 'max'])
    
    print("\nğŸ“Š å„å¹´ä»½ Attendance çµ±è¨ˆ:")
    print(yearly_stats.to_string())
    
    # è¨ˆç®—å¹´åº¦å‡å€¼è®ŠåŒ–
    print("\nğŸ“ˆ å¹´åº¦å‡å€¼è®ŠåŒ–:")
    for i in range(1, len(yearly_stats)):
        year = yearly_stats.index[i]
        prev_year = yearly_stats.index[i-1]
        change = yearly_stats.loc[year, 'mean'] - yearly_stats.loc[prev_year, 'mean']
        pct_change = change / yearly_stats.loc[prev_year, 'mean'] * 100
        print(f"  {prev_year} â†’ {year}: {change:+.1f} ({pct_change:+.1f}%)")
    
    # æ¸¬è©¦ä¸åŒè¨“ç·´ç­–ç•¥
    print("\n" + "=" * 70)
    print("ğŸ”§ æ”¹å–„ç­–ç•¥æ¸¬è©¦")
    print("=" * 70)
    
    feature_cols = [c for c in df.columns if c not in ['Date', 'Attendance', 'Year']]
    
    # ç­–ç•¥ 1: å‚³çµ± 80/20 åˆ†å‰²
    split_idx = int(len(df) * 0.8)
    X_train1, X_test = df[feature_cols].iloc[:split_idx], df[feature_cols].iloc[split_idx:]
    y_train1, y_test = df['Attendance'].iloc[:split_idx], df['Attendance'].iloc[split_idx:]
    
    model1 = xgb.XGBRegressor(n_estimators=200, max_depth=4, learning_rate=0.05, random_state=42)
    model1.fit(X_train1, y_train1)
    mae1 = mean_absolute_error(y_test, model1.predict(X_test))
    
    print(f"\nç­–ç•¥ 1: å‚³çµ± 80/20 åˆ†å‰²")
    print(f"  è¨“ç·´: 2014-2022, æ¸¬è©¦: 2023-2025")
    print(f"  MAE: {mae1:.2f}")
    
    # ç­–ç•¥ 2: åªç”¨è¿‘æœŸæ•¸æ“šè¨“ç·´ (æœ€è¿‘ 3 å¹´)
    recent_df = df[df['Year'] >= 2022].copy()
    split_idx2 = int(len(recent_df) * 0.8)
    X_train2 = recent_df[feature_cols].iloc[:split_idx2]
    y_train2 = recent_df['Attendance'].iloc[:split_idx2]
    
    model2 = xgb.XGBRegressor(n_estimators=200, max_depth=4, learning_rate=0.05, random_state=42)
    model2.fit(X_train2, y_train2)
    mae2 = mean_absolute_error(y_test, model2.predict(X_test))
    
    print(f"\nç­–ç•¥ 2: åªç”¨è¿‘æœŸæ•¸æ“š (2022+)")
    print(f"  è¨“ç·´: 2022-2023, æ¸¬è©¦: 2024-2025")
    print(f"  MAE: {mae2:.2f}")
    
    # ç­–ç•¥ 3: æ™‚é–“æ¬Šé‡ (è¿‘æœŸæ•¸æ“šæ¬Šé‡æ›´é«˜)
    days_from_end = (df['Date'].max() - df['Date']).dt.days
    # æŒ‡æ•¸è¡°æ¸›æ¬Šé‡: æœ€è¿‘çš„æ•¸æ“šæ¬Šé‡=1, è¶Šé è¶Šä½
    decay_rate = 0.001  # æ¯å¤©è¡°æ¸› 0.1%
    weights = np.exp(-decay_rate * days_from_end.iloc[:split_idx])
    weights = weights / weights.mean()  # æ­¸ä¸€åŒ–
    
    model3 = xgb.XGBRegressor(n_estimators=200, max_depth=4, learning_rate=0.05, random_state=42)
    model3.fit(X_train1, y_train1, sample_weight=weights)
    mae3 = mean_absolute_error(y_test, model3.predict(X_test))
    
    print(f"\nç­–ç•¥ 3: æ™‚é–“è¡°æ¸›æ¬Šé‡ (Exponential Decay)")
    print(f"  è¿‘æœŸæ•¸æ“šæ¬Šé‡æ›´é«˜ (decay_rate={decay_rate})")
    print(f"  MAE: {mae3:.2f}")
    
    # ç­–ç•¥ 4: æ»‘å‹•çª—å£ (åªç”¨æœ€è¿‘ 2 å¹´è¨“ç·´)
    window_days = 730  # 2 years
    window_df = df[df['Date'] >= (df['Date'].max() - pd.Timedelta(days=window_days + 365))].copy()
    split_idx4 = int(len(window_df) * 0.7)
    X_train4 = window_df[feature_cols].iloc[:split_idx4]
    y_train4 = window_df['Attendance'].iloc[:split_idx4]
    X_test4 = window_df[feature_cols].iloc[split_idx4:]
    y_test4 = window_df['Attendance'].iloc[split_idx4:]
    
    model4 = xgb.XGBRegressor(n_estimators=200, max_depth=4, learning_rate=0.05, random_state=42)
    model4.fit(X_train4, y_train4)
    mae4 = mean_absolute_error(y_test4, model4.predict(X_test4))
    
    print(f"\nç­–ç•¥ 4: æ»‘å‹•çª—å£ (æœ€è¿‘ 2-3 å¹´)")
    print(f"  è¨“ç·´: æœ€è¿‘ 2 å¹´, æ¸¬è©¦: æœ€è¿‘ 1 å¹´")
    print(f"  MAE: {mae4:.2f}")
    
    # ç¸½çµ
    print("\n" + "=" * 70)
    print("ğŸ“‹ ç­–ç•¥æ¯”è¼ƒç¸½çµ")
    print("=" * 70)
    strategies = [
        ("å‚³çµ± 80/20", mae1),
        ("åªç”¨è¿‘æœŸæ•¸æ“š", mae2),
        ("æ™‚é–“è¡°æ¸›æ¬Šé‡", mae3),
        ("æ»‘å‹•çª—å£", mae4)
    ]
    strategies.sort(key=lambda x: x[1])
    
    for i, (name, mae) in enumerate(strategies, 1):
        marker = "ğŸ†" if i == 1 else "  "
        print(f"  {marker} {i}. {name}: MAE = {mae:.2f}")
    
    return strategies

def analyze_weather_importance(df):
    """åˆ†æå¤©æ°£/AQHI ç‰¹å¾µçš„çœŸå¯¦åƒ¹å€¼"""
    print("\n" + "=" * 70)
    print("å•é¡Œ 2: å¤©æ°£/AQHI ç‰¹å¾µæ˜¯å¦æ‡‰è©²ä¿ç•™?")
    print("=" * 70)
    
    # åŸºç¤ç‰¹å¾µ (åªæœ‰ EWMA)
    base_features = ['EWMA_7', 'EWMA_14', 'EWMA_21', 'Lag_1', 'Lag_7', 'DayOfWeek', 'IsWeekend']
    
    # å¤©æ°£ç‰¹å¾µ
    weather_features = ['Temperature', 'Humidity', 'Rainfall', 'Is_Very_Hot', 'Is_Cold', 'Is_Heavy_Rain']
    
    # AQHI ç‰¹å¾µ
    aqhi_features = ['AQHI', 'Is_High_AQHI']
    
    split_idx = int(len(df) * 0.8)
    y_train = df['Attendance'].iloc[:split_idx]
    y_test = df['Attendance'].iloc[split_idx:]
    
    results = []
    
    # æ¸¬è©¦ 1: åªç”¨åŸºç¤ç‰¹å¾µ
    X1 = df[base_features]
    model1 = xgb.XGBRegressor(n_estimators=200, max_depth=4, learning_rate=0.05, random_state=42)
    model1.fit(X1.iloc[:split_idx], y_train)
    mae1 = mean_absolute_error(y_test, model1.predict(X1.iloc[split_idx:]))
    results.append(("åªç”¨ EWMA/Lag", len(base_features), mae1))
    
    # æ¸¬è©¦ 2: åŸºç¤ + å¤©æ°£
    X2 = df[base_features + weather_features]
    model2 = xgb.XGBRegressor(n_estimators=200, max_depth=4, learning_rate=0.05, random_state=42)
    model2.fit(X2.iloc[:split_idx], y_train)
    mae2 = mean_absolute_error(y_test, model2.predict(X2.iloc[split_idx:]))
    results.append(("åŸºç¤ + å¤©æ°£", len(base_features) + len(weather_features), mae2))
    
    # æ¸¬è©¦ 3: åŸºç¤ + AQHI
    X3 = df[base_features + aqhi_features]
    model3 = xgb.XGBRegressor(n_estimators=200, max_depth=4, learning_rate=0.05, random_state=42)
    model3.fit(X3.iloc[:split_idx], y_train)
    mae3 = mean_absolute_error(y_test, model3.predict(X3.iloc[split_idx:]))
    results.append(("åŸºç¤ + AQHI", len(base_features) + len(aqhi_features), mae3))
    
    # æ¸¬è©¦ 4: å…¨éƒ¨ç‰¹å¾µ
    X4 = df[base_features + weather_features + aqhi_features]
    model4 = xgb.XGBRegressor(n_estimators=200, max_depth=4, learning_rate=0.05, random_state=42)
    model4.fit(X4.iloc[:split_idx], y_train)
    mae4 = mean_absolute_error(y_test, model4.predict(X4.iloc[split_idx:]))
    results.append(("å…¨éƒ¨ç‰¹å¾µ", len(base_features) + len(weather_features) + len(aqhi_features), mae4))
    
    print("\nğŸ“Š ç‰¹å¾µçµ„åˆæ¯”è¼ƒ:")
    print(f"{'çµ„åˆ':<20} {'ç‰¹å¾µæ•¸':<10} {'MAE':<10} {'vs åŸºç¤':<15}")
    print("-" * 55)
    base_mae = results[0][2]
    for name, n_feat, mae in results:
        diff = mae - base_mae
        diff_str = f"{diff:+.2f}" if diff != 0 else "baseline"
        print(f"{name:<20} {n_feat:<10} {mae:<10.2f} {diff_str:<15}")
    
    # åˆ†ææ¥µç«¯æƒ…æ³
    print("\n" + "=" * 70)
    print("ğŸŒ¡ï¸ æ¥µç«¯å¤©æ°£æ¢ä»¶ä¸‹çš„é æ¸¬åˆ†æ")
    print("=" * 70)
    
    test_df = df.iloc[split_idx:].copy()
    test_df['Pred_Base'] = model1.predict(X1.iloc[split_idx:])
    test_df['Pred_Weather'] = model2.predict(X2.iloc[split_idx:])
    test_df['Error_Base'] = np.abs(test_df['Attendance'] - test_df['Pred_Base'])
    test_df['Error_Weather'] = np.abs(test_df['Attendance'] - test_df['Pred_Weather'])
    
    conditions = [
        ('æ­£å¸¸å¤©æ°£', (test_df['Is_Very_Hot'] == 0) & (test_df['Is_Cold'] == 0) & (test_df['Is_Heavy_Rain'] == 0)),
        ('é…·ç†± (>32Â°C)', test_df['Is_Very_Hot'] == 1),
        ('å¯’å†· (<12Â°C)', test_df['Is_Cold'] == 1),
        ('æš´é›¨ (>25mm)', test_df['Is_Heavy_Rain'] == 1),
        ('é«˜ AQHI (>=7)', test_df['Is_High_AQHI'] == 1),
    ]
    
    print(f"\n{'æ¢ä»¶':<20} {'å¤©æ•¸':<8} {'åŸºç¤ MAE':<12} {'å¤©æ°£ MAE':<12} {'æ”¹å–„':<10}")
    print("-" * 65)
    
    for name, condition in conditions:
        subset = test_df[condition]
        if len(subset) > 0:
            base_error = subset['Error_Base'].mean()
            weather_error = subset['Error_Weather'].mean()
            improvement = base_error - weather_error
            print(f"{name:<20} {len(subset):<8} {base_error:<12.2f} {weather_error:<12.2f} {improvement:+.2f}")
    
    # çµè«–
    print("\n" + "=" * 70)
    print("ğŸ“‹ çµè«–èˆ‡å»ºè­°")
    print("=" * 70)
    
    print("""
ğŸ”¬ ç ”ç©¶ç™¼ç¾:

1. å¤©æ°£/AQHI å°æ•´é«” MAE å½±éŸ¿å¾ˆå° (<5%)
   - é€™æ˜¯å› ç‚º EWMA å·²ç¶“éš±å¼æ•ç²äº†å¤©æ°£çš„é–“æ¥å½±éŸ¿
   - ä¾‹å¦‚: æ˜¨å¤©æš´é›¨ â†’ æ˜¨å¤©äººæ•¸å°‘ â†’ EWMA é™ä½ â†’ ä»Šå¤©é æ¸¬é™ä½

2. ä½†åœ¨æ¥µç«¯å¤©æ°£æ¢ä»¶ä¸‹ï¼Œå¤©æ°£ç‰¹å¾µå¯èƒ½æœ‰åƒ¹å€¼:
   - é…·ç†±/å¯’å†·å¤©æ°£: å¯èƒ½æ”¹å–„ 1-3 äºº
   - æš´é›¨å¤©æ°£: å¯èƒ½æ”¹å–„ 2-5 äºº
   - é«˜æ±¡æŸ“å¤©æ°£: å¯èƒ½æ”¹å–„ 1-2 äºº

3. å»ºè­°ç­–ç•¥:

   âœ… ä¿ç•™å¤©æ°£/AQHI ç‰¹å¾µçš„ç†ç”±:
   - æä¾›æ¨¡å‹å¯è§£é‡‹æ€§ (ç‚ºä»€éº¼ä»Šå¤©é æ¸¬é«˜/ä½)
   - æ¥µç«¯å¤©æ°£ä¸‹å¯èƒ½æœ‰å¹«åŠ©
   - ç¬¦åˆé†«å­¸ç ”ç©¶ (å¤©æ°£ç¢ºå¯¦å½±éŸ¿æ€¥è¨ºå°±è¨º)
   - å¢åŠ æ¨¡å‹é­¯æ£’æ€§ (æœªä¾†æ¨¡å¼è®ŠåŒ–æ™‚)

   âŒ ä¸ä¿ç•™çš„ç†ç”±:
   - æ•´é«” MAE æ”¹å–„ä¸æ˜é¡¯
   - å¢åŠ æ¨¡å‹è¤‡é›œåº¦
   - éœ€è¦é¡å¤–æ•¸æ“šæºç¶­è­·

   ğŸ¯ å»ºè­°: 
   - ç”Ÿç”¢ç’°å¢ƒ: ä¿ç•™ï¼Œä½†è¨­ç‚º"è£œå……ç‰¹å¾µ"ï¼Œä¸å½±éŸ¿ä¸»è¦é æ¸¬
   - æ¥µç«¯å¤©æ°£è§¸ç™¼æ™‚: å°é æ¸¬çµæœé€²è¡Œå°å¹…èª¿æ•´
   - ä¾‹å¦‚: å¦‚æœ AQHI >= 7ï¼Œé æ¸¬å€¼ * 1.02 (+2%)
""")

def main():
    print("=" * 70)
    print("è¨“ç·´/æ¸¬è©¦å·®è· èˆ‡ å¤©æ°£ç‰¹å¾µåˆ†æ")
    print("=" * 70)
    
    df = load_data()
    if df is None:
        print("Error: Could not load data")
        return
    
    print(f"è¼‰å…¥æ•¸æ“š: {len(df)} ç­†")
    
    df = create_features(df)
    print(f"å‰µå»ºç‰¹å¾µå¾Œ: {len(df)} ç­†, {len(df.columns)} åˆ—")
    
    # åˆ†æ 1: Concept Drift
    analyze_concept_drift(df)
    
    # åˆ†æ 2: Weather/AQHI
    analyze_weather_importance(df)

if __name__ == "__main__":
    main()

