"""
å…¨é¢çš„æ¨¡å‹æ¯”è¼ƒæ¸¬è©¦è…³æœ¬
å¾ Railway Database ç²å–æ‰€æœ‰æ­·å²æ•¸æ“šï¼ˆNDH AED é–‹æ¥­è‡³ä»Šï¼‰

æ¸¬è©¦å ´æ™¯:
1. Ensemble vs å–®ä¸€ XGBoost
2. AI å› å­å½±éŸ¿åˆ†æ
3. å¤©æ°£å› ç´ å½±éŸ¿åˆ†æ
4. å…¨æ•¸æ“š vs éƒ¨åˆ†æ•¸æ“šæ•ˆæœæ¯”è¼ƒ
5. çµ±è¨ˆé¡¯è‘—æ€§æ¸¬è©¦ (t-test, Wilcoxon, Diebold-Mariano)
"""
import sys
import io

if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy import stats
import json
import os
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor
import warnings
warnings.filterwarnings('ignore')

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from feature_engineering import create_comprehensive_features, load_weather_history, add_weather_features
from ensemble_predict import load_ai_factors_from_db

# ============ æ•¸æ“šåº«é€£æ¥ ============
def get_db_connection():
    """é€£æ¥åˆ° Railway Production Database"""
    password = os.environ.get('PGPASSWORD') or os.environ.get('DATABASE_PASSWORD') or 'nIdJPREHqkBdMgUifrazOsVlWbxsmDGq'
    return psycopg2.connect(
        host=os.environ.get('PGHOST', 'tramway.proxy.rlwy.net'),
        port=int(os.environ.get('PGPORT', '45703')),
        user=os.environ.get('PGUSER', 'postgres'),
        password=password,
        database=os.environ.get('PGDATABASE', 'railway'),
        sslmode='require'
    )

def load_all_historical_data():
    """å¾æ•¸æ“šåº«åŠ è¼‰å®Œæ•´æ­·å²æ•¸æ“š"""
    print("=" * 80)
    print("ğŸ“¥ å¾ Railway Database åŠ è¼‰æ‰€æœ‰æ­·å²æ•¸æ“š")
    print("=" * 80)

    try:
        conn = get_db_connection()
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # ç²å–å¯¦éš›å°±è¨ºæ•¸æ“š
        cur.execute("""
            SELECT date as "Date", patient_count as "Attendance"
            FROM actual_data
            ORDER BY date ASC
        """)

        rows = cur.fetchall()
        df = pd.DataFrame(rows)

        cur.close()
        conn.close()

        df['Date'] = pd.to_datetime(df['Date'])
        print(f"   âœ… æˆåŠŸåŠ è¼‰ {len(df)} ç­†æ•¸æ“š")
        print(f"   ğŸ“… æ—¥æœŸç¯„åœ: {df['Date'].min()} â†’ {df['Date'].max()}")
        print(f"   ğŸ“Š å¹³å‡å°±è¨º: {df['Attendance'].mean():.1f} äºº/å¤©")

        return df

    except Exception as e:
        print(f"   âŒ æ•¸æ“šåº«é€£æ¥å¤±æ•—: {e}")
        return None

def load_ai_factors():
    """åŠ è¼‰ AI å› å­"""
    print("\nğŸ“¥ åŠ è¼‰ AI Factors...")

    try:
        conn = get_db_connection()
        cur = conn.cursor(cursor_factory=RealDictCursor)

        cur.execute("""
            SELECT factors_cache
            FROM ai_factors_cache
            WHERE id = 1
        """)

        row = cur.fetchone()
        cur.close()
        conn.close()

        if row and row['factors_cache']:
            if isinstance(row['factors_cache'], str):
                factors_cache = json.loads(row['factors_cache'])
            else:
                factors_cache = row['factors_cache']

            print(f"   âœ… AI Factors æ•¸æ“š: {len(factors_cache)} å€‹æ—¥æœŸ")
            return factors_cache
        else:
            print("   â„¹ï¸ æ²’æœ‰ AI Factors æ•¸æ“š")
            return {}

    except Exception as e:
        print(f"   âš ï¸ ç„¡æ³•åŠ è¼‰ AI Factors: {e}")
        return {}

# ============ è©•ä¼°æŒ‡æ¨™ ============
def calculate_metrics(y_true, y_pred):
    """è¨ˆç®—æ‰€æœ‰è©•ä¼°æŒ‡æ¨™"""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100
    r2 = r2_score(y_true, y_pred)

    # è¨ˆç®—èª¤å·®åˆ†ä½ˆ
    errors = y_pred - y_true
    mean_error = np.mean(errors)
    std_error = np.std(errors)
    median_ae = np.median(np.abs(errors))

    # è¨ˆç®— Theil's U ç³»æ•¸ (ç›¸å°æ–¼ç°¡å–®é æ¸¬)
    naive_forecast = np.roll(y_true, 1)
    naive_forecast[0] = y_true[0]
    naive_mse = np.mean((y_true[1:] - naive_forecast[1:]) ** 2)
    model_mse = np.mean((y_true - y_pred) ** 2)
    theils_u = np.sqrt(model_mse) / np.sqrt(naive_mse) if naive_mse > 0 else 1

    return {
        'mae': mae,
        'rmse': rmse,
        'mape': mape,
        'r2': r2,
        'mean_error': mean_error,
        'std_error': std_error,
        'median_ae': median_ae,
        'theils_u': theils_u
    }

# ============ çµ±è¨ˆé¡¯è‘—æ€§æ¸¬è©¦ ============
def statistical_significance_tests(y_true, pred1, pred2, model1_name="Model 1", model2_name="Model 2"):
    """
    çµ±è¨ˆé¡¯è‘—æ€§æ¸¬è©¦å¥—ä»¶

    æ¸¬è©¦æ–¹æ³•:
    1. Paired t-test: æ¯”è¼ƒå…©æ¨¡å‹çµ•å°èª¤å·®å·®ç•°
    2. Wilcoxon signed-rank test: éåƒæ•¸ç‰ˆæœ¬
    3. Diebold-Mariano test: é æ¸¬æº–ç¢ºåº¦æ¯”è¼ƒ
    """
    errors1 = np.abs(pred1 - y_true)
    errors2 = np.abs(pred2 - y_true)
    diff = errors1 - errors2

    results = {}

    # 1. Paired t-test
    t_stat, t_pvalue = stats.ttest_rel(errors1, errors2)
    results['t_test'] = {
        'statistic': float(t_stat),
        'p_value': float(t_pvalue),
        'significant': t_pvalue < 0.05,
        'interpretation': 'é¡¯è‘—' if t_pvalue < 0.05 else 'ä¸é¡¯è‘—'
    }

    # 2. Wilcoxon signed-rank test
    try:
        w_stat, w_pvalue = stats.wilcoxon(diff)
        results['wilcoxon'] = {
            'statistic': float(w_stat),
            'p_value': float(w_pvalue),
            'significant': w_pvalue < 0.05,
            'interpretation': 'é¡¯è‘—' if w_pvalue < 0.05 else 'ä¸é¡¯è‘—'
        }
    except:
        results['wilcoxon'] = {'p_value': 1.0, 'significant': False, 'interpretation': 'ç„¡æ³•è¨ˆç®—'}

    # 3. Diebold-Mariano test (ç°¡åŒ–ç‰ˆ)
    loss_diff = (pred1 - y_true)**2 - (pred2 - y_true)**2
    mean_loss_diff = np.mean(loss_diff)
    var_loss_diff = np.var(loss_diff, ddof=1)

    if var_loss_diff > 0:
        dm_stat = mean_loss_diff / np.sqrt(var_loss_diff / len(loss_diff))
        dm_pvalue = 2 * (1 - stats.norm.cdf(abs(dm_stat)))
        results['diebold_mariano'] = {
            'statistic': float(dm_stat),
            'p_value': float(dm_pvalue),
            'significant': dm_pvalue < 0.05,
            'interpretation': 'é¡¯è‘—' if dm_pvalue < 0.05 else 'ä¸é¡¯è‘—'
        }
    else:
        results['diebold_mariano'] = {'p_value': 1.0, 'significant': False, 'interpretation': 'ç„¡æ³•è¨ˆç®—'}

    # 4. æ”¹å–„æ–¹å‘
    mae1 = np.mean(errors1)
    mae2 = np.mean(errors2)
    results['improvement'] = {
        'better_model': model1_name if mae1 < mae2 else model2_name,
        'mae1': float(mae1),
        'mae2': float(mae2),
        'relative_improvement_pct': float((mae1 - mae2) / mae1 * 100) if mae1 > 0 else 0
    }

    return results

# ============ æ¨¡å‹è¨“ç·´ ============
def train_xgboost_model(X_train, y_train, X_test):
    """è¨“ç·´ XGBoost æ¨¡å‹"""
    model = xgb.XGBRegressor(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.05,
        min_child_weight=3,
        subsample=0.85,
        colsample_bytree=0.85,
        objective='reg:squarederror',
        alpha=0.5,
        reg_lambda=1.5,
        tree_method='hist',
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train, verbose=False)
    return model

def train_random_forest(X_train, y_train, X_test):
    """è¨“ç·´ Random Forest æ¨¡å‹"""
    model = RandomForestRegressor(
        n_estimators=200,
        max_depth=12,
        min_samples_split=10,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    return model

def train_gradient_boosting(X_train, y_train, X_test):
    """è¨“ç·´ Gradient Boosting æ¨¡å‹"""
    model = GradientBoostingRegressor(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.05,
        random_state=42
    )
    model.fit(X_train, y_train)
    return model

def train_ensemble(X_train, y_train, X_test, y_test):
    """è¨“ç·´ä¸¦é æ¸¬ Ensemble æ¨¡å‹"""
    predictions = {}

    # XGBoost
    xgb_model = train_xgboost_model(X_train, y_train, X_test)
    predictions['xgboost'] = xgb_model.predict(X_test)

    # Random Forest
    rf_model = train_random_forest(X_train, y_train, X_test)
    predictions['randomforest'] = rf_model.predict(X_test)

    # Gradient Boosting
    gb_model = train_gradient_boosting(X_train, y_train, X_test)
    predictions['gradientboosting'] = gb_model.predict(X_test)

    # LightGBM (å¦‚æœå¯ç”¨)
    try:
        from lightgbm import LGBMRegressor
        lgb = LGBMRegressor(n_estimators=300, max_depth=8, learning_rate=0.05,
                           random_state=42, verbose=-1, n_jobs=-1)
        lgb.fit(X_train, y_train)
        predictions['lightgbm'] = lgb.predict(X_test)
    except:
        pass

    # Simple Average Ensemble
    pred_values = list(predictions.values())
    predictions['ensemble_simple'] = np.mean(pred_values, axis=0)

    # åŠ æ¬Š Ensemble (æ ¹æ“šé©—è­‰é›†æ€§èƒ½)
    # ä½¿ç”¨è¨“ç·´é›†çš„æœ€å¾Œ 20% ä½œç‚ºé©—è­‰é›†
    val_size = len(X_train) // 5
    X_val = X_train[-val_size:]
    y_val = y_train[-val_size:]

    val_scores = {}
    for name, model in [('xgboost', xgb_model), ('rf', rf_model), ('gb', gb_model)]:
        if name == 'xgboost':
            val_pred = model.predict(X_val)
        elif name == 'rf':
            val_pred = model.predict(X_val)
        else:
            val_pred = model.predict(X_val)
        val_scores[name] = mean_absolute_error(y_val, val_pred)

    # è¨ˆç®—æ¬Šé‡ (èª¤å·®è¶Šå°æ¬Šé‡è¶Šå¤§)
    weights = {k: 1/v for k, v in val_scores.items()}
    total_weight = sum(weights.values())
    weights = {k: v/total_weight for k, v in weights.items()}

    ensemble_weighted = (
        weights['xgboost'] * predictions['xgboost'] +
        weights['rf'] * predictions['randomforest'] +
        weights['gb'] * predictions['gradientboosting']
    )
    predictions['ensemble_weighted'] = ensemble_weighted

    return predictions, weights

# ============ æ¸¬è©¦å ´æ™¯ ============
def prepare_data_scenarios(df, ai_factors, weather_df):
    """æº–å‚™ä¸åŒæ¸¬è©¦å ´æ™¯çš„æ•¸æ“š"""
    scenarios = {}

    # COVID æœŸé–“å®šç¾©
    covid_start = pd.Timestamp('2020-02-01')
    covid_end = pd.Timestamp('2022-06-30')

    # ========================================
    # å ´æ™¯ 1: å…¨æ•¸æ“šï¼ˆåŒ…å« COVIDï¼‰
    # ========================================
    df_full = df.copy()
    df_full = create_comprehensive_features(df_full, ai_factors_dict=None)
    df_full = df_full.dropna(subset=['Attendance'])

    split_idx = int(len(df_full) * 0.8)
    train_full = df_full[:split_idx].copy()
    test_full = df_full[split_idx:].copy()

    scenarios['full_data'] = {
        'name': 'å…¨æ•¸æ“š (å« COVID)',
        'train': train_full,
        'test': test_full,
        'total_days': len(df_full),
        'covid_days': ((df_full['Date'] >= covid_start) & (df_full['Date'] <= covid_end)).sum()
    }

    # ========================================
    # å ´æ™¯ 2: æ’é™¤ COVID
    # ========================================
    df_no_covid = df[~((df['Date'] >= covid_start) & (df['Date'] <= covid_end))].copy()
    df_no_covid = create_comprehensive_features(df_no_covid, ai_factors_dict=None)
    df_no_covid = df_no_covid.dropna(subset=['Attendance'])

    split_idx = int(len(df_no_covid) * 0.8)
    train_no_covid = df_no_covid[:split_idx].copy()
    test_no_covid = df_no_covid[split_idx:].copy()

    scenarios['no_covid'] = {
        'name': 'æ’é™¤ COVID',
        'train': train_no_covid,
        'test': test_no_covid,
        'total_days': len(df_no_covid),
        'covid_days': 0
    }

    # ========================================
    # å ´æ™¯ 3: æœ€è¿‘ 3 å¹´æ•¸æ“š
    # ========================================
    cutoff_date = df['Date'].max() - pd.Timedelta(days=3*365)
    df_recent = df[df['Date'] >= cutoff_date].copy()
    df_recent = create_comprehensive_features(df_recent, ai_factors_dict=None)
    df_recent = df_recent.dropna(subset=['Attendance'])

    split_idx = int(len(df_recent) * 0.8)
    train_recent = df_recent[:split_idx].copy()
    test_recent = df_recent[split_idx:].copy()

    scenarios['recent_3yr'] = {
        'name': 'æœ€è¿‘ 3 å¹´',
        'train': train_recent,
        'test': test_recent,
        'total_days': len(df_recent),
        'covid_days': ((df_recent['Date'] >= covid_start) & (df_recent['Date'] <= covid_end)).sum()
    }

    # ========================================
    # å ´æ™¯ 4: æ’é™¤ COVID + AI Factors
    # ========================================
    df_with_ai = df[~((df['Date'] >= covid_start) & (df['Date'] <= covid_end))].copy()
    df_with_ai = create_comprehensive_features(df_with_ai, ai_factors_dict=ai_factors)
    df_with_ai = df_with_ai.dropna(subset=['Attendance'])

    split_idx = int(len(df_with_ai) * 0.8)
    train_ai = df_with_ai[:split_idx].copy()
    test_ai = df_with_ai[split_idx:].copy()

    scenarios['with_ai'] = {
        'name': 'æ’é™¤ COVID + AI Factors',
        'train': train_ai,
        'test': test_ai,
        'total_days': len(df_with_ai),
        'covid_days': 0,
        'has_ai': True
    }

    # ========================================
    # å ´æ™¯ 5: æ’é™¤ COVID + å¤©æ°£æ•¸æ“š
    # ========================================
    if weather_df is not None and len(weather_df) > 0:
        df_weather = df[~((df['Date'] >= covid_start) & (df['Date'] <= covid_end))].copy()
        df_weather = add_weather_features(df_weather, weather_df)
        # ç¢ºä¿ Date æ˜¯ datetime
        df_weather['Date'] = pd.to_datetime(df_weather['Date'])
        df_weather = create_comprehensive_features(df_weather, ai_factors_dict=None)
        df_weather = df_weather.dropna(subset=['Attendance'])

        split_idx = int(len(df_weather) * 0.8)
        train_weather = df_weather[:split_idx].copy()
        test_weather = df_weather[split_idx:].copy()

        scenarios['with_weather'] = {
            'name': 'æ’é™¤ COVID + å¤©æ°£æ•¸æ“š',
            'train': train_weather,
            'test': test_weather,
            'total_days': len(df_weather),
            'covid_days': 0,
            'has_weather': True
        }

    return scenarios

# ============ ä¸»æ¸¬è©¦å‡½æ•¸ ============
def run_comprehensive_comparison():
    """åŸ·è¡Œå…¨é¢æ¨¡å‹æ¯”è¼ƒæ¸¬è©¦"""
    print("=" * 80)
    print("ğŸ”¬ å…¨é¢çš„æ¨¡å‹æ¯”è¼ƒæ¸¬è©¦")
    print("=" * 80)
    print(f"â° é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # 1. åŠ è¼‰æ•¸æ“š
    df = load_all_historical_data()
    if df is None:
        print("âŒ ç„¡æ³•åŠ è¼‰æ•¸æ“š")
        return

    # 2. åŠ è¼‰ AI Factors
    ai_factors = load_ai_factors()

    # 3. åŠ è¼‰å¤©æ°£æ•¸æ“š
    print("\nğŸ“¥ åŠ è¼‰å¤©æ°£æ•¸æ“š...")
    weather_df = load_weather_history()

    # 4. æº–å‚™æ¸¬è©¦å ´æ™¯
    print("\nğŸ”§ æº–å‚™æ¸¬è©¦å ´æ™¯...")
    scenarios = prepare_data_scenarios(df, ai_factors, weather_df)

    # åŸºç¤ç‰¹å¾µåˆ—è¡¨
    base_features = [
        "Attendance_EWMA7", "Attendance_EWMA14", "Daily_Change", "Monthly_Change",
        "Attendance_Lag1", "Weekly_Change", "Attendance_Rolling7", "Attendance_Position7",
        "Attendance_Lag30", "Attendance_Lag7", "Day_of_Week", "Lag1_Diff",
        "DayOfWeek_sin", "Attendance_Rolling14", "Attendance_Position14",
        "Attendance_Position30", "Attendance_Rolling3", "Attendance_Min7",
        "Attendance_Median14", "DayOfWeek_Target_Mean", "Attendance_Median3",
        "Attendance_EWMA30", "Is_Winter_Flu_Season", "Is_Weekend", "Holiday_Factor"
    ]

    all_results = {}
    all_predictions = {}

    # ========================================
    # æ¸¬è©¦æ¯å€‹å ´æ™¯
    # ========================================
    for scenario_key, scenario in scenarios.items():
        print("\n" + "=" * 80)
        print(f"ğŸ“Š æ¸¬è©¦å ´æ™¯: {scenario['name']}")
        print("=" * 80)
        print(f"   è¨“ç·´é›†: {len(scenario['train'])} å¤©")
        print(f"   æ¸¬è©¦é›†: {len(scenario['test'])} å¤©")

        train = scenario['train']
        test = scenario['test']
        y_test = test['Attendance'].values

        # é¸æ“‡ç‰¹å¾µ
        if scenario.get('has_ai'):
            # ä½¿ç”¨ AI ç‰¹å¾µ
            ai_cols = [c for c in train.columns if c.startswith('AI_')]
            feature_cols = [c for c in base_features if c in train.columns] + ai_cols
        elif scenario.get('has_weather'):
            # ä½¿ç”¨å¤©æ°£ç‰¹å¾µ
            weather_cols = [c for c in train.columns if c.startswith('Weather_')]
            feature_cols = [c for c in base_features if c in train.columns] + weather_cols
        else:
            # åƒ…ä½¿ç”¨åŸºç¤ç‰¹å¾µ
            feature_cols = [c for c in base_features if c in train.columns]

        feature_cols = [c for c in feature_cols if c in train.columns]
        print(f"   ç‰¹å¾µæ•¸: {len(feature_cols)} å€‹")

        X_train = train[feature_cols].fillna(0)
        X_test = test[feature_cols].fillna(0)

        # è¨“ç·´æ¨¡å‹
        print("\n   è¨“ç·´æ¨¡å‹...")
        predictions, weights = train_ensemble(X_train, train['Attendance'].values, X_test, y_test)

        # è¨ˆç®—æŒ‡æ¨™
        scenario_results = {}
        for model_name, pred in predictions.items():
            scenario_results[model_name] = calculate_metrics(y_test, pred)

        all_results[scenario_key] = {
            'scenario_name': scenario['name'],
            'metrics': {k: {m: float(v) for m, v in metrics.items()} for k, metrics in scenario_results.items()},
            'feature_count': len(feature_cols),
            'train_size': len(train),
            'test_size': len(test),
            'ensemble_weights': weights
        }

        all_predictions[scenario_key] = {
            'y_true': y_test,
            'predictions': predictions
        }

        # è¼¸å‡ºçµæœ
        print(f"\n   çµæœ:")
        for model_name, metrics in scenario_results.items():
            print(f"      {model_name:20} MAE={metrics['mae']:.2f}, RMSE={metrics['rmse']:.2f}, RÂ²={metrics['r2']:.4f}")

    # ========================================
    # çµ±è¨ˆé¡¯è‘—æ€§æ¸¬è©¦
    # ========================================
    print("\n" + "=" * 80)
    print("ğŸ“Š çµ±è¨ˆé¡¯è‘—æ€§æ¸¬è©¦")
    print("=" * 80)

    significance_tests = {}

    # æ¸¬è©¦ 1: XGBoost vs Ensemble (åœ¨æ’é™¤ COVID æ•¸æ“šä¸Š)
    if 'no_covid' in all_predictions and 'no_covid' in all_results:
        y_true = all_predictions['no_covid']['y_true']
        pred_xgb = all_predictions['no_covid']['predictions']['xgboost']
        pred_ens = all_predictions['no_covid']['predictions']['ensemble_simple']

        print("\n1ï¸âƒ£ XGBoost vs Ensemble (æ’é™¤ COVID æ•¸æ“š)")
        sig_result = statistical_significance_tests(
            y_true, pred_xgb, pred_ens, "XGBoost", "Ensemble"
        )
        significance_tests['xgb_vs_ensemble'] = sig_result

        print(f"   Paired t-test: p={sig_result['t_test']['p_value']:.4f} ({sig_result['t_test']['interpretation']})")
        print(f"   Wilcoxon test: p={sig_result['wilcoxon']['p_value']:.4f} ({sig_result['wilcoxon']['interpretation']})")
        print(f"   Diebold-Mariano: p={sig_result['diebold_mariano']['p_value']:.4f} ({sig_result['diebold_mariano']['interpretation']})")

        mae1 = sig_result['improvement']['mae1']
        mae2 = sig_result['improvement']['mae2']
        better = sig_result['improvement']['better_model']
        imp_pct = sig_result['improvement']['relative_improvement_pct']
        print(f"   æ›´å¥½æ¨¡å‹: {better} (MAE: {mae1:.2f} â†’ {mae2:.2f}, {imp_pct:+.1f}%)")

    # æ¸¬è©¦ 2: ç„¡ AI vs æœ‰ AI
    if 'no_covid' in all_predictions and 'with_ai' in all_predictions:
        # ç¢ºä¿æ¸¬è©¦é›†å¤§å°ä¸€è‡´
        min_len = min(len(all_predictions['no_covid']['y_true']),
                     len(all_predictions['with_ai']['y_true']))

        y_true_no_ai = all_predictions['no_covid']['y_true'][:min_len]
        y_true_with_ai = all_predictions['with_ai']['y_true'][:min_len]
        pred_no_ai = all_predictions['no_covid']['predictions']['xgboost'][:min_len]
        pred_with_ai = all_predictions['with_ai']['predictions']['xgboost'][:min_len]

        # éœ€è¦æ¯”è¼ƒçš„æ˜¯åŒä¸€æ•¸æ“šä¸Šçš„é æ¸¬ï¼Œä½†é€™è£¡æ•¸æ“šé›†ä¸åŒ
        # æ”¹ç‚ºæ¯”è¼ƒæ•´é«”æŒ‡æ¨™
        print("\n2ï¸âƒ£ AI å› å­å½±éŸ¿åˆ†æ")
        mae_no_ai = all_results['no_covid']['metrics']['xgboost']['mae']
        mae_with_ai = all_results['with_ai']['metrics']['xgboost']['mae']
        improvement = (mae_no_ai - mae_with_ai) / mae_no_ai * 100

        print(f"   ç„¡ AI Factors: MAE = {mae_no_ai:.2f}")
        print(f"   æœ‰ AI Factors: MAE = {mae_with_ai:.2f}")
        print(f"   æ”¹å–„: {improvement:+.1f}%")

        significance_tests['ai_factors_impact'] = {
            'mae_without_ai': float(mae_no_ai),
            'mae_with_ai': float(mae_with_ai),
            'improvement_pct': float(improvement)
        }

    # æ¸¬è©¦ 3: å…¨æ•¸æ“š vs æ’é™¤ COVID
    if 'full_data' in all_results and 'no_covid' in all_results:
        print("\n3ï¸âƒ£ å…¨æ•¸æ“š vs æ’é™¤ COVID")
        mae_full = all_results['full_data']['metrics']['xgboost']['mae']
        mae_no_covid = all_results['no_covid']['metrics']['xgboost']['mae']
        improvement = (mae_full - mae_no_covid) / mae_full * 100

        print(f"   å…¨æ•¸æ“š (å« COVID): MAE = {mae_full:.2f}")
        print(f"   æ’é™¤ COVID: MAE = {mae_no_covid:.2f}")
        print(f"   æ’é™¤ COVID æ”¹å–„: {improvement:+.1f}%")

        significance_tests['covid_exclusion_impact'] = {
            'mae_full_data': float(mae_full),
            'mae_no_covid': float(mae_no_covid),
            'improvement_pct': float(improvement)
        }

    # æ¸¬è©¦ 4: å¤©æ°£å› ç´ å½±éŸ¿
    if 'no_covid' in all_results and 'with_weather' in all_results:
        print("\n4ï¸âƒ£ å¤©æ°£å› ç´ å½±éŸ¿åˆ†æ")
        mae_no_weather = all_results['no_covid']['metrics']['xgboost']['mae']
        mae_with_weather = all_results['with_weather']['metrics']['xgboost']['mae']
        improvement = (mae_no_weather - mae_with_weather) / mae_no_weather * 100

        print(f"   ç„¡å¤©æ°£æ•¸æ“š: MAE = {mae_no_weather:.2f}")
        print(f"   æœ‰å¤©æ°£æ•¸æ“š: MAE = {mae_with_weather:.2f}")
        print(f"   æ”¹å–„: {improvement:+.1f}%")

        significance_tests['weather_factors_impact'] = {
            'mae_without_weather': float(mae_no_weather),
            'mae_with_weather': float(mae_with_weather),
            'improvement_pct': float(improvement)
        }

    # æ¸¬è©¦ 5: æ•¸æ“šé‡å½±éŸ¿ (å…¨æ•¸æ“š vs æœ€è¿‘ 3 å¹´)
    if 'full_data' in all_results and 'recent_3yr' in all_results:
        print("\n5ï¸âƒ£ æ•¸æ“šé‡å½±éŸ¿åˆ†æ")
        mae_full = all_results['full_data']['metrics']['xgboost']['mae']
        mae_recent = all_results['recent_3yr']['metrics']['xgboost']['mae']
        diff_pct = (mae_recent - mae_full) / mae_full * 100

        print(f"   å…¨æ•¸æ“š ({all_results['full_data']['train_size']} è¨“ç·´æ¨£æœ¬): MAE = {mae_full:.2f}")
        print(f"   æœ€è¿‘ 3 å¹´ ({all_results['recent_3yr']['train_size']} è¨“ç·´æ¨£æœ¬): MAE = {mae_recent:.2f}")
        print(f"   å·®ç•°: {diff_pct:+.1f}%")

        significance_tests['data_size_impact'] = {
            'mae_full_data': float(mae_full),
            'mae_recent_3yr': float(mae_recent),
            'full_data_train_size': all_results['full_data']['train_size'],
            'recent_3yr_train_size': all_results['recent_3yr']['train_size'],
            'difference_pct': float(diff_pct)
        }

    # ========================================
    # ç¸½çµå ±å‘Š
    # ========================================
    print("\n" + "=" * 80)
    print("ğŸ† ç¸½çµå ±å‘Š")
    print("=" * 80)

    # æœ€ä½³æ¨¡å‹é…ç½®
    best_mae = float('inf')
    best_config = None

    for scenario_key, results in all_results.items():
        for model_name, metrics in results['metrics'].items():
            if metrics['mae'] < best_mae:
                best_mae = metrics['mae']
                best_config = {
                    'scenario': results['scenario_name'],
                    'model': model_name,
                    'mae': metrics['mae'],
                    'rmse': metrics['rmse'],
                    'r2': metrics['r2']
                }

    print(f"\nğŸ¥‡ æœ€ä½³æ¨¡å‹é…ç½®:")
    print(f"   å ´æ™¯: {best_config['scenario']}")
    print(f"   æ¨¡å‹: {best_config['model']}")
    print(f"   MAE: {best_config['mae']:.2f}")
    print(f"   RMSE: {best_config['rmse']:.2f}")
    print(f"   RÂ²: {best_config['r2']:.4f}")

    # ä¿å­˜çµæœ
    output = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_info': {
            'total_records': len(df),
            'date_range': {
                'start': df['Date'].min().strftime('%Y-%m-%d'),
                'end': df['Date'].max().strftime('%Y-%m-%d')
            },
            'mean_attendance': float(df['Attendance'].mean()),
            'std_attendance': float(df['Attendance'].std())
        },
        'scenarios': all_results,
        'significance_tests': significance_tests,
        'best_configuration': best_config
    }

    # è½‰æ›ç‚º JSON å¯åºåˆ—åŒ–æ ¼å¼
    def convert_to_serializable(obj):
        if isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(v) for v in obj]
        elif isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj

    output_serializable = convert_to_serializable(output)

    # ä¿å­˜ç‚º JSON
    os.makedirs('models', exist_ok=True)
    output_path = 'models/comprehensive_model_comparison.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_serializable, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… çµæœå·²ä¿å­˜åˆ° {output_path}")
    print(f"\nâ° çµæŸæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    return output

if __name__ == '__main__':
    run_comprehensive_comparison()
