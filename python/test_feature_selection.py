# -*- coding: utf-8 -*-
"""
ç‰¹å¾µé¸æ“‡å„ªåŒ–æ¸¬è©¦

ç›®æ¨™: æ‰¾å‡ºæœ€ä½³ç‰¹å¾µæ•¸é‡å’Œç‰¹å¾µå­é›†

æ–¹æ³•:
1. éå¢ç‰¹å¾µæ¸¬è©¦ (Forward Selection) - å¾å°‘åˆ°å¤š
2. éæ¸›ç‰¹å¾µæ¸¬è©¦ (Backward Selection) - å¾å¤šåˆ°å°‘
3. éæ­¸ç‰¹å¾µæ¶ˆé™¤ (RFECV) - è‡ªå‹•é¸æ“‡æœ€å„ª
4. ç‰¹å¾µé‡è¦æ€§æ’å (XGBoost importance)
"""
import sys
import io

if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

import pandas as pd
import numpy as np
from datetime import datetime
import psycopg2
import psycopg2.extras
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFECV, SelectKBest, f_regression, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
import json
import os
import warnings
warnings.filterwarnings('ignore')

# å°å…¥ç‰¹å¾µæ¨¡çµ„
from weather_forecast_integration import fetch_weather_forecast, get_forecast_feature_list
from flu_season_features import add_flu_features_to_df, get_flu_feature_list
from historical_weather_patterns import (
    add_historical_weather_pattern_features,
    get_historical_weather_feature_list
)

# æ•¸æ“šåº«é€£æ¥
DB_CONFIG = {
    'host': 'razzle.db.elephantsql.com',
    'database': 'ndh_aed',
    'user': 'ndh_aed',
    'password': 'B3IG7EYud_UMqfUNvEbi5XxO9xh5l8Pp',
    'port': 5432
}

# COVID æœŸé–“
COVID_PERIODS = [
    ('2020-01-23', '2020-04-08'),
    ('2020-07-16', '2020-09-30'),
    ('2020-11-23', '2021-01-05'),
    ('2022-02-05', '2022-04-30'),
    ('2022-11-10', '2022-12-27'),
]


def load_data():
    """åŠ è¼‰æ•¸æ“š"""
    try:
        print("ğŸ“¡ åŠ è¼‰æ•¸æ“š...")
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)

        query = "SELECT date, patient_count FROM actual_data ORDER BY date ASC"
        cursor.execute(query)
        rows = cursor.fetchall()

        cursor.close()
        conn.close()

        df = pd.DataFrame(rows)
        df['date'] = pd.to_datetime(df['date'])

        # æ’é™¤ COVID
        for start, end in COVID_PERIODS:
            start_date = pd.to_datetime(start)
            end_date = pd.to_datetime(end)
            mask = (df['date'] >= start_date) & (df['date'] <= end_date)
            df = df[~mask]

        print(f"   âœ… {len(df)} ç­†è¨˜éŒ„")
        return df

    except Exception as e:
        print(f"   âŒ {e}")
        return None


def load_weather_data():
    """åŠ è¼‰å¤©æ°£æ•¸æ“š"""
    weather_file = 'models/weather_full_history.csv'
    if not os.path.exists(weather_file):
        return None

    weather_df = pd.read_csv(weather_file)
    weather_df['Date'] = pd.to_datetime(weather_df['Date'])
    return weather_df


def prepare_all_features(df):
    """æº–å‚™æ‰€æœ‰ç‰¹å¾µ"""
    print("\nğŸ“Š æº–å‚™ç‰¹å¾µ...")

    df = df.rename(columns={'date': 'Date'})

    # åŸºç¤ç‰¹å¾µ
    df = add_base_features(df)

    # æµæ„Ÿå­£ç¯€
    df = add_flu_features_to_df(df, date_col='Date')

    # æ­·å²å¤©æ°£æ¨¡å¼
    weather_df = load_weather_data()
    if weather_df is not None:
        df = add_historical_weather_pattern_features(df, weather_df, df)

    # å¤©æ°£é å ±
    forecast_df = fetch_weather_forecast()
    if forecast_df is not None and len(forecast_df) > 0:
        from weather_forecast_integration import add_forecast_features_to_df
        df = add_forecast_features_to_df(df, forecast_df, date_col='Date')

    # ç§»é™¤ NaN
    df = df.dropna()

    print(f"   âœ… ç‰¹å¾µæº–å‚™å®Œæˆ: {len(df)} ç­†")
    return df


def add_base_features(df):
    """æ·»åŠ åŸºç¤ç‰¹å¾µ"""
    df = df.copy()
    df['Date'] = pd.to_datetime(df['Date'])

    # æ™‚é–“ç‰¹å¾µ
    df['Day_of_Week'] = df['Date'].dt.dayofweek
    df['Month'] = df['Date'].dt.month
    df['Day_of_Month'] = df['Date'].dt.day
    df['Is_Weekend'] = (df['Day_of_Week'] >= 5).astype(int)

    # é€±æœŸç·¨ç¢¼
    df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7)
    df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7)

    # æµæ„Ÿå­£ç¯€
    df['Is_Winter_Flu_Season'] = df['Month'].isin([1, 2]).astype(int)

    # å‹•æ…‹å› å­
    df['Holiday_Factor'] = 1.0

    # æ­·å²å°±è¨º
    df = df.sort_values('Date').reset_index(drop=True)

    df['Attendance_Lag1'] = df['patient_count'].shift(1)
    df['Attendance_Lag7'] = df['patient_count'].shift(7)
    df['Attendance_Lag30'] = df['patient_count'].shift(30)

    df['Attendance_EWMA7'] = df['patient_count'].ewm(span=7, adjust=False).mean()
    df['Attendance_EWMA14'] = df['patient_count'].ewm(span=14, adjust=False).mean()

    df['Daily_Change'] = df['patient_count'].diff()
    df['Weekly_Change'] = df['patient_count'].diff(7)

    # å¡«è£œ
    df['Attendance_Lag1'] = df['Attendance_Lag1'].fillna(df['patient_count'].mean())
    df['Attendance_Lag7'] = df['Attendance_Lag7'].fillna(df['patient_count'].mean())
    df['Attendance_Lag30'] = df['Attendance_Lag30'].fillna(df['patient_count'].mean())
    df['Attendance_EWMA7'] = df['Attendance_EWMA7'].fillna(method='bfill')
    df['Attendance_EWMA14'] = df['Attendance_EWMA14'].fillna(method='bfill')
    df['Daily_Change'] = df['Daily_Change'].fillna(0)
    df['Weekly_Change'] = df['Weekly_Change'].fillna(0)

    return df


def test_feature_counts(X_train, y_train, X_test, y_test, feature_names):
    """
    æ¸¬è©¦ä¸åŒç‰¹å¾µæ•¸é‡çš„æ•ˆæœ

    æ–¹æ³•: éå¢æ¸¬è©¦ï¼ˆå¾ 5 å€‹ç‰¹å¾µé–‹å§‹ï¼Œæ¯æ¬¡ +5ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸ” æ¸¬è©¦ 1: éå¢ç‰¹å¾µæ•¸é‡æ¸¬è©¦")
    print("=" * 80)

    results = []

    # æ¸¬è©¦ç¯„åœ: 5 åˆ°æ‰€æœ‰ç‰¹å¾µï¼Œæ¯æ¬¡ +5
    max_features = len(feature_names)
    feature_counts = list(range(5, min(max_features, 81), 5))

    # å¦‚æœ max_features ä¸æ˜¯ 5 çš„å€æ•¸ï¼Œæ·»åŠ æœ€å¾Œä¸€æ¬¡
    if max_features % 5 != 0:
        feature_counts.append(max_features)

    print(f"\næ¸¬è©¦ç¯„åœ: {min(feature_counts)} åˆ° {max(feature_counts)} å€‹ç‰¹å¾µ")
    print(f"ç¸½ç‰¹å¾µæ•¸: {max_features}\n")

    for n_features in feature_counts:
        if n_features > max_features:
            n_features = max_features

        # é¸æ“‡å‰ n_features å€‹ç‰¹å¾µï¼ˆåŸºæ–¼é‡è¦æ€§ï¼‰
        model = xgb.XGBRegressor(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.05,
            random_state=42,
            n_jobs=-1
        )

        # å…ˆè¨“ç·´ç²å–ç‰¹å¾µé‡è¦æ€§
        model.fit(X_train, y_train, verbose=False)

        # ç²å–ç‰¹å¾µé‡è¦æ€§
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]

        # é¸æ“‡ top n_features
        selected_indices = indices[:n_features]
        selected_features = [feature_names[i] for i in selected_indices]

        # é‡æ–°è¨“ç·´
        X_train_selected = X_train.iloc[:, selected_indices]
        X_test_selected = X_test.iloc[:, selected_indices]

        model_selected = xgb.XGBRegressor(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.05,
            random_state=42,
            n_jobs=-1
        )
        model_selected.fit(X_train_selected, y_train, verbose=False)

        # è©•ä¼°
        from sklearn.metrics import mean_absolute_error
        y_pred = model_selected.predict(X_test_selected)
        mae = mean_absolute_error(y_test, y_pred)

        results.append({
            'n_features': n_features,
            'mae': mae,
            'features': selected_features
        })

        print(f"   {n_features:3d} ç‰¹å¾µ: MAE = {mae:.2f}")

    # æ‰¾å‡ºæœ€ä½³
    best_result = min(results, key=lambda x: x['mae'])

    print("\n" + "=" * 80)
    print(f"ğŸ† æœ€ä½³ç‰¹å¾µæ•¸é‡: {best_result['n_features']} å€‹")
    print(f"   MAE: {best_result['mae']:.2f}")
    print(f"   æ”¹å–„: {((15.73 - best_result['mae']) / 15.73 * 100):.1f}%")
    print("=" * 80)

    return results, best_result


def test_backward_selection(X_train, y_train, X_test, y_test, feature_names):
    """
    éæ¸›ç‰¹å¾µé¸æ“‡

    æ–¹æ³•: å¾æ‰€æœ‰ç‰¹å¾µé–‹å§‹ï¼Œé€æ­¥ç§»é™¤ä¸é‡è¦çš„ç‰¹å¾µ
    """
    print("\n" + "=" * 80)
    print("ğŸ” æ¸¬è©¦ 2: éæ¸›ç‰¹å¾µé¸æ“‡")
    print("=" * 80)

    results = []
    remaining_features = list(range(len(feature_names)))

    # è¨“ç·´åˆå§‹æ¨¡å‹
    model = xgb.XGBRegressor(
        n_estimators=300,
        max_depth=6,
        learning_rate=0.05,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train.iloc[:, remaining_features], y_train, verbose=False)

    from sklearn.metrics import mean_absolute_error
    y_pred = model.predict(X_test.iloc[:, remaining_features])
    best_mae = mean_absolute_error(y_test, y_pred)

    print(f"\nåˆå§‹ ({len(remaining_features)} ç‰¹å¾µ): MAE = {best_mae:.2f}")

    # æ¯æ¬¡ç§»é™¤ 5 å€‹æœ€ä¸é‡è¦çš„ç‰¹å¾µ
    iteration = 0
    while len(remaining_features) > 10:
        iteration += 1

        # è¨“ç·´ä¸¦ç²å–é‡è¦æ€§
        model.fit(X_train.iloc[:, remaining_features], y_train, verbose=False)
        importances = model.feature_importances_

        # æ‰¾å‡ºæœ€ä¸é‡è¦çš„ 5 å€‹ç‰¹å¾µ
        indices = np.argsort(importances)
        to_remove = indices[:min(5, len(indices))]

        # ç§»é™¤
        remaining_features = [i for i in remaining_features if i not in to_remove]

        # è©•ä¼°
        model.fit(X_train.iloc[:, remaining_features], y_train, verbose=False)
        y_pred = model.predict(X_test.iloc[:, remaining_features])
        mae = mean_absolute_error(y_test, y_pred)

        results.append({
            'iteration': iteration,
            'n_features': len(remaining_features),
            'mae': mae,
            'removed': to_remove
        })

        print(f"   è¿­ä»£ {iteration} ({len(remaining_features):3d} ç‰¹å¾µ): MAE = {mae:.2f}", end="")

        if mae < best_mae:
            best_mae = mae
            print(" âœ… æ”¹å–„")
        else:
            print(" âŒ æƒ¡åŒ–")

        # å¦‚æœé€£çºŒ 3 æ¬¡æ²’æœ‰æ”¹å–„ï¼Œåœæ­¢
        if len(results) >= 3:
            recent_maes = [r['mae'] for r in results[-3:]]
            if all(m > best_mae for m in recent_maes):
                print(f"\n   âš ï¸ é€£çºŒ 3 æ¬¡ç„¡æ”¹å–„ï¼Œåœæ­¢")
                break

    # æ‰¾å‡ºæœ€ä½³
    best_result = min(results, key=lambda x: x['mae'])
    best_features_idx = remaining_features

    print("\n" + "=" * 80)
    print(f"ğŸ† æœ€ä½³ç‰¹å¾µæ•¸é‡: {best_result['n_features']} å€‹")
    print(f"   MAE: {best_result['mae']:.2f}")
    print("=" * 80)

    return results, best_result, best_features_idx


def test_rfecv(X_train, y_train, X_test, y_test, feature_names):
    """
    éæ­¸ç‰¹å¾µæ¶ˆé™¤äº¤å‰é©—è­‰

    è‡ªå‹•æ‰¾å‡ºæœ€å„ªç‰¹å¾µå­é›†
    """
    print("\n" + "=" * 80)
    print("ğŸ” æ¸¬è©¦ 3: éæ­¸ç‰¹å¾µæ¶ˆé™¤ (RFECV)")
    print("=" * 80)

    from sklearn.feature_selection import RFECV
    from sklearn.model_selection import TimeSeriesSplit

    # ä½¿ç”¨è¼ƒç°¡å–®çš„æ¨¡å‹åŠ å¿«é€Ÿåº¦
    estimator = RandomForestRegressor(
        n_estimators=100,
        max_depth=8,
        random_state=42,
        n_jobs=-1
    )

    # æ™‚é–“åºåˆ—äº¤å‰é©—è­‰
    tscv = TimeSeriesSplit(n_splits=3)

    # RFECV
    rfecv = RFECV(
        estimator=estimator,
        step=5,
        cv=tscv,
        scoring='neg_mean_absolute_error',
        min_features_to_select=10,
        n_jobs=-1
    )

    print("   â³ åŸ·è¡Œ RFECV (é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜)...")
    rfecv.fit(X_train, y_train)

    # çµæœ
    optimal_n_features = rfecv.n_features_
    selected_features = [feature_names[i] for i in range(len(feature_names)) if rfecv.support_[i]]

    print(f"   âœ… æœ€å„ªç‰¹å¾µæ•¸é‡: {optimal_n_features}")

    # è©•ä¼°
    from sklearn.metrics import mean_absolute_error
    y_pred = rfecv.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)

    print(f"   MAE: {mae:.2f}")

    # é¡¯ç¤ºé¸ä¸­çš„ç‰¹å¾µ
    print(f"\n   é¸ä¸­çš„ç‰¹å¾µ ({len(selected_features)} å€‹):")
    for i, feature in enumerate(selected_features, 1):
        print(f"      {i:2d}. {feature}")

    return {
        'n_features': optimal_n_features,
        'mae': mae,
        'features': selected_features,
        'ranking': rfecv.ranking_
    }


def get_feature_importance_ranking(X_train, y_train, feature_names):
    """
    ç²å–ç‰¹å¾µé‡è¦æ€§æ’å
    """
    print("\n" + "=" * 80)
    print("ğŸ” æ¸¬è©¦ 4: ç‰¹å¾µé‡è¦æ€§æ’å")
    print("=" * 80)

    # ä½¿ç”¨ XGBoost
    model = xgb.XGBRegressor(
        n_estimators=500,
        max_depth=6,
        learning_rate=0.05,
        random_state=42,
        n_jobs=-1
    )

    model.fit(X_train, y_train, verbose=False)

    # ç‰¹å¾µé‡è¦æ€§
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]

    print("\n   Top 30 ç‰¹å¾µ:")
    print(f"   {'æ’å':<4} {'ç‰¹å¾µ':<35} {'é‡è¦æ€§':<10}")
    print("   " + "-" * 60)

    ranking = {}
    for i, idx in enumerate(indices[:30], 1):
        feature = feature_names[idx]
        importance = importances[idx]
        ranking[feature] = {
            'rank': i,
            'importance': importance
        }
        print(f"   {i:<4} {feature:<35} {importance:.4f}")

    # åˆ†é¡çµ±è¨ˆ
    print("\n   ç‰¹å¾µåˆ†é¡çµ±è¨ˆ:")

    base_features = ['Day_of_Week', 'Month', 'Day_of_Month', 'Is_Weekend', 'Holiday_Factor',
                     'DayOfWeek_sin', 'DayOfWeek_cos', 'Is_Winter_Flu_Season',
                     'Attendance_Lag1', 'Attendance_Lag7', 'Attendance_Lag30',
                     'Attendance_EWMA7', 'Attendance_EWMA14', 'Daily_Change', 'Weekly_Change']

    flu_features = get_flu_feature_list()
    weather_forecast_features = get_forecast_feature_list()
    historical_weather_features = get_historical_weather_feature_list()

    categories = {
        'åŸºç¤ç‰¹å¾µ': base_features,
        'æµæ„Ÿå­£ç¯€': flu_features,
        'å¤©æ°£é å ±': weather_forecast_features,
        'æ­·å²å¤©æ°£': historical_weather_features
    }

    for category, features in categories.items():
        available = [f for f in features if f in feature_names]
        if len(available) > 0:
            # è¨ˆç®—å¹³å‡é‡è¦æ€§
            category_importance = []
            for feature in available:
                if feature in feature_names:
                    idx = feature_names.index(feature)
                    category_importance.append(importances[idx])

            avg_importance = np.mean(category_importance)
            print(f"      {category:<12} {len(available):2d} å€‹ç‰¹å¾µ, å¹³å‡é‡è¦æ€§: {avg_importance:.4f}")

    return ranking, importances, indices


def main():
    """ä¸»æ¸¬è©¦æµç¨‹"""
    print("=" * 80)
    print("ğŸ¯ ç‰¹å¾µé¸æ“‡å„ªåŒ–æ¸¬è©¦")
    print("=" * 80)
    print(f"æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # 1. åŠ è¼‰æ•¸æ“š
    df = load_data()
    if df is None:
        return

    # 2. æº–å‚™ç‰¹å¾µ
    df = prepare_all_features(df)

    # 3. ç²å–ç‰¹å¾µåˆ—è¡¨
    print("\nğŸ“‹ ç‰¹å¾µåˆ—è¡¨...")

    # åŸºç¤ç‰¹å¾µ
    base_features = [
        'Day_of_Week', 'Month', 'Day_of_Month', 'Is_Weekend',
        'Holiday_Factor', 'Is_Winter_Flu_Season',
        'DayOfWeek_sin', 'DayOfWeek_cos',
        'Attendance_Lag1', 'Attendance_Lag7', 'Attendance_Lag30',
        'Attendance_EWMA7', 'Attendance_EWMA14',
        'Daily_Change', 'Weekly_Change'
    ]

    # å…¶ä»–ç‰¹å¾µ
    flu_features = get_flu_feature_list()
    weather_forecast_features = [f for f in get_forecast_feature_list() if f in df.columns]
    historical_weather_features = [f for f in get_historical_weather_feature_list() if f in df.columns]

    all_features = base_features + flu_features + historical_weather_features + weather_forecast_features

    # åªä¿ç•™å­˜åœ¨çš„ç‰¹å¾µ
    all_features = [f for f in all_features if f in df.columns]

    print(f"   ç¸½ç‰¹å¾µæ•¸: {len(all_features)}")
    print(f"   - åŸºç¤ç‰¹å¾µ: {len(base_features)}")
    print(f"   - æµæ„Ÿå­£ç¯€: {len(flu_features)}")
    print(f"   - æ­·å²å¤©æ°£: {len(historical_weather_features)}")
    print(f"   - å¤©æ°£é å ±: {len(weather_forecast_features)}")

    # 4. åˆ†å‰²æ•¸æ“š
    print("\nâœ‚ï¸ åˆ†å‰²æ•¸æ“š...")
    train_size = int(len(df) * 0.8)

    train_df = df.iloc[:train_size]
    test_df = df.iloc[train_size:]

    X_train = train_df[all_features]
    y_train = train_df['patient_count']
    X_test = test_df[all_features]
    y_test = test_df['patient_count']

    print(f"   è¨“ç·´é›†: {len(X_train)} ç­†")
    print(f"   æ¸¬è©¦é›†: {len(X_test)} ç­†")

    # æ¸¬è©¦ 1: éå¢ç‰¹å¾µ
    forward_results, best_forward = test_feature_counts(X_train, y_train, X_test, y_test, all_features)

    # æ¸¬è©¦ 2: ç‰¹å¾µé‡è¦æ€§æ’å
    ranking, importances, indices = get_feature_importance_ranking(X_train, y_train, all_features)

    # æ¸¬è©¦ 3: RFECV
    rfecv_result = test_rfecv(X_train, y_train, X_test, y_test, all_features)

    # ç¸½çµ
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬è©¦ç¸½çµ")
    print("=" * 80)

    print(f"\nåŸºæº– (èˆŠæ¨¡å‹): MAE = 15.73")
    print(f"éå¢ç‰¹å¾µæœ€ä½³: {best_forward['n_features']} å€‹ç‰¹å¾µ, MAE = {best_forward['mae']:.2f}")
    print(f"RFECV æœ€ä½³: {rfecv_result['n_features']} å€‹ç‰¹å¾µ, MAE = {rfecv_result['mae']:.2f}")

    # æ‰¾å‡ºæ•´é«”æœ€ä½³
    best_overall = min([
        ('Forward', best_forward),
        ('RFECV', rfecv_result)
    ], key=lambda x: x[1]['mae'])

    print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_overall[0]}")
    print(f"   ç‰¹å¾µæ•¸é‡: {best_overall[1]['n_features']}")
    print(f"   MAE: {best_overall[1]['mae']:.2f}")
    print(f"   æ”¹å–„: {((15.73 - best_overall[1]['mae']) / 15.73 * 100):.1f}%")

    # ä¿å­˜çµæœ
    results = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'baseline_mae': 15.73,
        'total_features': len(all_features),
        'forward_selection': {
            'best_n_features': best_forward['n_features'],
            'best_mae': best_forward['mae'],
            'all_results': [{'n': r['n_features'], 'mae': r['mae']} for r in forward_results]
        },
        'rfecv': {
            'best_n_features': rfecv_result['n_features'],
            'best_mae': rfecv_result['mae'],
            'selected_features': rfecv_result['features']
        },
        'best_overall': {
            'method': best_overall[0],
            'n_features': best_overall[1]['n_features'],
            'mae': best_overall[1]['mae'],
            'improvement_pct': ((15.73 - best_overall[1]['mae']) / 15.73 * 100)
        }
    }

    os.makedirs('models', exist_ok=True)
    with open('models/feature_selection_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ° models/feature_selection_results.json")

    # æ¨è–¦
    print("\n" + "=" * 80)
    print("ğŸ’¡ æ¨è–¦")
    print("=" * 80)

    if best_overall[1]['n_features'] < len(all_features) * 0.5:
        print(f"\nâœ… ç‰¹å¾µæ•¸é‡å¯ä»¥æ¸›å°‘!")
        print(f"   å¾ {len(all_features)} å€‹æ¸›å°‘åˆ° {best_overall[1]['n_features']} å€‹")
        print(f"   æ¸›å°‘ {len(all_features) - best_overall[1]['n_features']} å€‹ç‰¹å¾µ ({(1 - best_overall[1]['n_features']/len(all_features))*100:.1f}%)")
        print(f"   åŒæ™‚æ”¹å–„æº–ç¢ºåº¦!")
    else:
        print(f"\nâš ï¸ å¤§éƒ¨åˆ†ç‰¹å¾µéƒ½æœ‰ç”¨")
        print(f"   æ¨è–¦ä½¿ç”¨ {best_overall[1]['n_features']} å€‹ç‰¹å¾µ")

    print(f"\nä½¿ç”¨é€™ {best_overall[1]['n_features']} å€‹ç‰¹å¾µé‡æ–°è¨“ç·´æ¨¡å‹ä»¥ç²å¾—æœ€ä½³çµæœã€‚")

    print("\n" + "=" * 80)
    print("âœ… æ¸¬è©¦å®Œæˆ")
    print("=" * 80)


if __name__ == '__main__':
    main()
