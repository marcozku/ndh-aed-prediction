# Quick Start Implementation Guide for Cursor
## NDH AED Attendance AI Prediction Algorithm

This guide provides a step-by-step copy-paste ready implementation.

---

## IMPLEMENTATION CHECKLIST

### ‚úÖ PHASE 1: Environment Setup (30 minutes)

```bash
# Create virtual environment
python -m venv aed_prediction_env
source aed_prediction_env/bin/activate  # On Windows: aed_prediction_env\Scripts\activate

# Install dependencies
pip install pandas numpy scikit-learn xgboost lightgbm
pip install tensorflow keras
pip install fbprophet  # Note: May require conda
pip install matplotlib seaborn plotly
pip install hyperopt
pip install jupyter notebook

# Verify installations
python -c "import xgboost, lightgbm, tensorflow, fbprophet; print('All libraries installed successfully')"
```

### ‚úÖ PHASE 2: Data Preparation (1 hour)

**Step 1: Load Data**
```python
import pandas as pd
import numpy as np
from datetime import datetime

# Load the attendance data
df = pd.read_csv('NDH_AED_Attendance_2014-2025.csv')
print(f"Dataset shape: {df.shape}")
print(f"Date range: {df['Date'].min()} to {df['Date'].max()}")
print(f"Attendance range: {df['Attendance'].min()} to {df['Attendance'].max()}")
```

**Step 2: Create All Features** (Copy from Section 8.3 of main spec)
```python
def create_comprehensive_features(df):
    """Create 50+ features for model training"""
    df['Date'] = pd.to_datetime(df['Date'])
    df = df.sort_values('Date').reset_index(drop=True)
    
    # Temporal
    df['Year'] = df['Date'].dt.year
    df['Month'] = df['Date'].dt.month
    df['Day_of_Week'] = df['Date'].dt.dayofweek
    df['Day_of_Month'] = df['Date'].dt.day
    df['Week_of_Year'] = df['Date'].dt.isocalendar().week
    df['Quarter'] = df['Date'].dt.quarter
    df['Days_Since_Start'] = (df['Date'] - df['Date'].min()).dt.days
    
    # Cyclical
    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)
    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)
    df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7)
    df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7)
    
    # Lagged features
    for lag in [1, 7, 14, 30, 365]:
        df[f'Attendance_Lag{lag}'] = df['Attendance'].shift(lag)
    
    # Fill NaN lags
    lag_cols = [col for col in df.columns if 'Attendance_Lag' in col]
    df[lag_cols] = df[lag_cols].fillna(df['Attendance'].mean())
    
    # Rolling statistics
    for window in [7, 30]:
        df[f'Attendance_Rolling{window}'] = df['Attendance'].rolling(window).mean()
        df[f'Attendance_Std{window}'] = df['Attendance'].rolling(window).std()
        df[f'Attendance_Max{window}'] = df['Attendance'].rolling(window).max()
        df[f'Attendance_Min{window}'] = df['Attendance'].rolling(window).min()
    
    # Fill NaN rolling
    rolling_cols = [col for col in df.columns if 'Attendance_Rolling' in col or 'Attendance_Std' in col]
    df[rolling_cols] = df[rolling_cols].fillna(method='bfill')
    
    # Event indicators
    df['Is_COVID_Period'] = ((df['Year'] >= 2020) & (df['Year'] <= 2022)).astype(int)
    df['Is_Omicron_Wave'] = ((df['Year'] == 2022) & (df['Month'] <= 5)).astype(int)
    df['Is_Winter_Flu_Season'] = df['Month'].isin([12, 1, 2, 3]).astype(int)
    df['Is_Summer_Period'] = df['Month'].isin([6, 7, 8]).astype(int)
    df['Is_Weekend'] = (df['Date'].dt.dayofweek >= 5).astype(int)
    df['Is_Monday'] = (df['Date'].dt.dayofweek == 0).astype(int)
    df['Is_Holiday'] = 0  # Implement with actual HK holidays
    
    # Rate of change
    df['Daily_Change'] = df['Attendance'].diff()
    df['Weekly_Change'] = df['Attendance'].diff(7)
    
    return df

# Apply feature engineering
df = create_comprehensive_features(df)
print(f"Features created: {len(df.columns)}")
print(f"Feature columns: {df.columns.tolist()}")

# Save engineered features
df.to_csv('features_engineered.csv', index=False)
print("‚úÖ Features saved to features_engineered.csv")
```

### ‚úÖ PHASE 3: Train-Test Split (15 minutes)

```python
from sklearn.model_selection import TimeSeriesSplit

# Method 1: Simple temporal split (80-20)
split_idx = int(len(df) * 0.8)
df_train = df[:split_idx].copy()
df_test = df[split_idx:].copy()

# Prepare X, y
feature_cols = [col for col in df.columns if col not in ['Date', 'Attendance']]
X_train = df_train[feature_cols].copy()
y_train = df_train['Attendance'].copy()
X_test = df_test[feature_cols].copy()
y_test = df_test['Attendance'].copy()

print(f"Train set: {X_train.shape[0]} samples ({X_train['Days_Since_Start'].min()}-{X_train['Days_Since_Start'].max()})")
print(f"Test set: {X_test.shape[0]} samples")
print(f"Features: {X_train.shape[1]}")

# Method 2: Time series cross-validation (for robust evaluation)
tscv = TimeSeriesSplit(n_splits=5)
for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):
    print(f"Fold {fold}: Train={len(train_idx)}, Val={len(val_idx)}")
```

### ‚úÖ PHASE 4: XGBoost Model (1 hour)

```python
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Initialize model
xgb_model = xgb.XGBRegressor(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    alpha=1.0,
    lambda=1.0,
    eval_metric='mae',
    random_state=42,
    n_jobs=-1
)

# Train
print("Training XGBoost...")
xgb_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=50,
    verbose=10
)

# Evaluate
y_pred_xgb = xgb_model.predict(X_test)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))

print(f"‚úÖ XGBoost Results:")
print(f"   MAE: {mae_xgb:.2f} patients")
print(f"   RMSE: {rmse_xgb:.2f} patients")
print(f"   MAPE: {np.mean(np.abs((y_test - y_pred_xgb) / y_test)) * 100:.2f}%")

# Feature importance
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)
print("\nTop 10 Features:")
print(importance_df.head(10))

# Save model
import pickle
with open('xgb_model.pkl', 'wb') as f:
    pickle.dump(xgb_model, f)
print("‚úÖ XGBoost model saved")
```

### ‚úÖ PHASE 5: LSTM Neural Network (1.5 hours)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Prepare sequences for LSTM
def create_sequences(X, y, seq_length=60):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_length):
        X_seq.append(X[i:i+seq_length])
        y_seq.append(y[i+seq_length])
    return np.array(X_seq), np.array(y_seq)

# Normalize features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create sequences
X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, seq_length=60)
X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, seq_length=60)

print(f"LSTM Input shape: {X_train_seq.shape}")

# Build model
lstm_model = Sequential([
    Bidirectional(LSTM(128, return_sequences=True, input_shape=(60, X_train_scaled.shape[1]))),
    Dropout(0.2),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.2),
    LSTM(32, return_sequences=False),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.1),
    Dense(32, activation='relu'),
    Dense(1)
])

lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mae', metrics=['rmse'])

# Train
print("Training LSTM...")
history = lstm_model.fit(
    X_train_seq, y_train_seq,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)],
    verbose=1
)

# Evaluate
y_pred_lstm = lstm_model.predict(X_test_seq)
mae_lstm = mean_absolute_error(y_test_seq, y_pred_lstm)
rmse_lstm = np.sqrt(mean_squared_error(y_test_seq, y_pred_lstm))

print(f"‚úÖ LSTM Results:")
print(f"   MAE: {mae_lstm:.2f} patients")
print(f"   RMSE: {rmse_lstm:.2f} patients")

# Save model
lstm_model.save('lstm_model.h5')
print("‚úÖ LSTM model saved")
```

### ‚úÖ PHASE 6: Prophet Model (45 minutes)

```python
from fbprophet import Prophet

# Prepare data
prophet_df = df[['Date', 'Attendance']].copy()
prophet_df.columns = ['ds', 'y']

# Add regressors
prophet_df['covid_period'] = df['Is_COVID_Period']
prophet_df['winter_flu'] = df['Is_Winter_Flu_Season']
prophet_df['is_monday'] = df['Is_Monday']

# Train-test split for Prophet
prophet_train = prophet_df[:split_idx].copy()
prophet_test = prophet_df[split_idx:].copy()

# Initialize and fit
print("Training Prophet...")
prophet_model = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=True,
    fourier_order_yearly=10,
    fourier_order_weekly=5,
    interval_width=0.95
)

prophet_model.add_regressor('covid_period')
prophet_model.add_regressor('winter_flu')
prophet_model.add_regressor('is_monday')

prophet_model.fit(prophet_train)

# Forecast
future = prophet_test[['ds', 'covid_period', 'winter_flu', 'is_monday']].copy()
forecast = prophet_model.predict(future)

y_pred_prophet = forecast['yhat'].values
mae_prophet = mean_absolute_error(prophet_test['y'].values, y_pred_prophet)
rmse_prophet = np.sqrt(mean_squared_error(prophet_test['y'].values, y_pred_prophet))

print(f"‚úÖ Prophet Results:")
print(f"   MAE: {mae_prophet:.2f} patients")
print(f"   RMSE: {rmse_prophet:.2f} patients")

# Save model
import pickle
with open('prophet_model.pkl', 'wb') as f:
    pickle.dump(prophet_model, f)
print("‚úÖ Prophet model saved")
```

### ‚úÖ PHASE 7: Ensemble Hybrid Model (30 minutes)

```python
class EnsemblePredictor:
    def __init__(self, xgb_model, lstm_model, prophet_model):
        self.xgb = xgb_model
        self.lstm = lstm_model
        self.prophet = prophet_model
        
    def predict(self, X_test, future_dates=None):
        # XGBoost prediction
        xgb_pred = self.xgb.predict(X_test)
        
        # LSTM prediction (requires sequence preparation)
        # For simplicity, use last prediction repeated
        lstm_pred = np.array([self.lstm.predict(X_test_seq[-1:], verbose=0)[0,0]] * len(X_test))
        
        # Prophet prediction (if future dates provided)
        if future_dates is not None:
            prophet_future = pd.DataFrame({'ds': future_dates})
            prophet_forecast = self.prophet.predict(prophet_future)
            prophet_pred = prophet_forecast['yhat'].values
        else:
            prophet_pred = np.zeros(len(X_test))
        
        # Weighted ensemble
        weights = np.array([0.40, 0.35, 0.25])
        ensemble_pred = (weights[0] * xgb_pred + 
                        weights[1] * lstm_pred + 
                        weights[2] * prophet_pred)
        
        # Confidence intervals
        std_preds = np.std([xgb_pred, lstm_pred, prophet_pred], axis=0)
        ci_lower = ensemble_pred - 1.96 * std_preds
        ci_upper = ensemble_pred + 1.96 * std_preds
        
        return ensemble_pred, ci_lower, ci_upper
    
    def evaluate(self, X_test, y_test):
        pred, ci_lower, ci_upper = self.predict(X_test)
        mae = mean_absolute_error(y_test, pred)
        rmse = np.sqrt(mean_squared_error(y_test, pred))
        mape = np.mean(np.abs((y_test - pred) / y_test)) * 100
        
        print(f"‚úÖ Ensemble Results:")
        print(f"   MAE: {mae:.2f} patients")
        print(f"   RMSE: {rmse:.2f} patients")
        print(f"   MAPE: {mape:.2f}%")
        
        return {'mae': mae, 'rmse': rmse, 'mape': mape}

# Create ensemble
ensemble = EnsemblePredictor(xgb_model, lstm_model, prophet_model)

# Evaluate
ensemble.evaluate(X_test, y_test)
```

---

## PRODUCTION PREDICTION PIPELINE

### Making Predictions for Future Dates

```python
def predict_next_7_days(ensemble_model, df_latest, scaler, lstm_model):
    """Predict attendance for next 7 days"""
    
    from datetime import timedelta
    
    # Get last 60 days for context
    X_latest = df_latest[feature_cols].tail(60).values
    X_latest_scaled = scaler.transform(X_latest)
    
    predictions = []
    dates = []
    ci_lower_list = []
    ci_upper_list = []
    
    current_date = df_latest['Date'].max() + timedelta(days=1)
    
    for day_ahead in range(7):
        prediction_date = current_date + timedelta(days=day_ahead)
        dates.append(prediction_date)
        
        # Create features for prediction date
        # (In production, use actual weather, flu data, etc.)
        X_pred = create_prediction_features(prediction_date, df_latest, X_latest_scaled)
        
        pred, ci_lower, ci_upper = ensemble_model.predict(X_pred.reshape(1, -1))
        
        predictions.append(pred[0])
        ci_lower_list.append(ci_lower[0])
        ci_upper_list.append(ci_upper[0])
    
    results_df = pd.DataFrame({
        'Date': dates,
        'Predicted_Attendance': predictions,
        'CI_Lower': ci_lower_list,
        'CI_Upper': ci_upper_list,
        'Confidence': 'High' if max(ci_upper_list) - min(ci_lower_list) < 40 else 'Medium'
    })
    
    return results_df

# Usage
future_predictions = predict_next_7_days(ensemble, df, scaler, lstm_model)
print("\n7-Day Forecast:")
print(future_predictions)
```

---

## MONITORING & RETRAINING

```python
import json
from datetime import datetime

def log_prediction_performance(actual, predicted, date):
    """Log daily prediction accuracy for monitoring"""
    error = abs(actual - predicted)
    mape = abs((actual - predicted) / actual) * 100
    
    log_entry = {
        'date': str(date),
        'actual': actual,
        'predicted': predicted,
        'error': error,
        'mape': mape,
        'timestamp': datetime.now().isoformat()
    }
    
    # Append to log file
    with open('prediction_log.jsonl', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')
    
    # Alert if error too large
    if error > 30:
        print(f"‚ö†Ô∏è  ALERT: Large error on {date}: {error:.1f} patients")
    
    return error < 15  # Target MAE < 15

def check_model_drift():
    """Check if model performance is degrading"""
    
    # Read recent logs
    recent_errors = []
    with open('prediction_log.jsonl', 'r') as f:
        lines = f.readlines()[-30:]  # Last 30 days
        for line in lines:
            entry = json.loads(line)
            recent_errors.append(entry['error'])
    
    avg_recent_error = np.mean(recent_errors)
    
    if avg_recent_error > 18:  # Threshold
        print(f"‚ö†Ô∏è  MODEL DRIFT DETECTED: Average error {avg_recent_error:.1f} > 18")
        print("    Recommend retraining")
        return True
    
    return False

# Monthly retraining
def monthly_retrain(df_new):
    """Retrain model with new data"""
    
    print("Starting monthly retraining...")
    
    df_updated = pd.concat([df, df_new], ignore_index=True)
    df_updated = create_comprehensive_features(df_updated)
    
    # Retrain all three models
    X = df_updated[feature_cols]
    y = df_updated['Attendance']
    
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # Retrain each model...
    print("‚úÖ Models retrained successfully")
```

---

## PERFORMANCE TARGETS (Check These!)

```
Target Metrics:
‚îú‚îÄ MAE: < 13 patients
‚îú‚îÄ RMSE: < 18 patients
‚îú‚îÄ MAPE: < 5.2%
‚îú‚îÄ Directional Accuracy: > 91%
‚îú‚îÄ Inference Time: < 100ms
‚îî‚îÄ Stratified Performance:
   ‚îú‚îÄ Pre-COVID (2014-2019): MAE < 15
   ‚îú‚îÄ COVID (2020-2022): MAE < 12
   ‚îú‚îÄ Post-COVID (2023-2025): MAE < 14
   ‚îú‚îÄ Winter: MAE < 16
   ‚îî‚îÄ Summer: MAE < 13

Alert Thresholds:
‚îú‚îÄ Daily Prediction Error > 30: WARN
‚îú‚îÄ Weekly Average MAE > 18: ALERT
‚îú‚îÄ Model Drift Detected: RETRAIN
‚îî‚îÄ Confidence Interval Width > 50: INCREASE UNCERTAINTY
```

---

## FILES YOU NOW HAVE

1. **AI-AED-Algorithm-Specification.txt** - Comprehensive 11-section guide
2. **Feature_Specifications_for_Algorithm.csv** - 33 features with importance
3. **Model_Parameters_Reference.csv** - 19 hyperparameters optimized
4. **NDH_AED_Attendance_2014-2025.csv** - Historical data (3,431 records)
5. **This file** - Quick start implementation guide

---

## NEXT STEPS

1. ‚úÖ Copy all code above into Jupyter notebook
2. ‚úÖ Run Phase 1-7 sequentially (4-5 hours total)
3. ‚úÖ Verify all models meet target MAE < 13
4. ‚úÖ Deploy ensemble model to production
5. ‚úÖ Monitor predictions daily
6. ‚úÖ Retrain monthly with new data

---

## TROUBLESHOOTING

**Q: LSTM model training is slow**
A: Use GPU acceleration or reduce sequence length from 60 to 30 days

**Q: Prophet gives NaN predictions**
A: Add more regressors or increase fourier_order parameters

**Q: Ensemble predictions are off**
A: Check if models are trained on same train-test split (temporal!)

**Q: Memory error on large dataset**
A: Use subsample=0.5 in XGBoost or split into batches

---

**Good luck building the unbeatable algorithm! üöÄ**
